{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"study.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1NNDWa6L4g-P501BW_I3rqUx1eCOnan2q","authorship_tag":"ABX9TyNotMGt0m3yUSkFdJOUsoSK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 16. 행동 스티커 만들기"],"metadata":{"id":"JwpfCsFJqHic"}},{"cell_type":"markdown","source":["# 16-1. 데이터셋을 어디에서 구할까?"],"metadata":{"id":"LUedTJuIqKQq"}},{"cell_type":"markdown","source":["### MPII 데이터셋 다운로드하기\n","MPII Human Pose Dataset을 사용해서 Human Pose Estimation task를 위한 모델을 훈련시켜보자.\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/images/GC-10-P-1.max-800x600.png)"],"metadata":{"id":"IQ8AalTWCblb"}},{"cell_type":"markdown","source":["미리 준비된 데이터셋에 다음과 같은 과정이 필요하다.\n","\n","```\n","# ray 라이브러리 설치가 필요합니다.\n","pip install ray\n","\n","# mpii 폴더로 이동\n","cd YOUR_PATH/mpii\n","\n","# 이미 파일들이 다 올라가 있으니 따로 업로드 하실 건 없습니다.\n","# zip파일을 한꺼번에 압축 풀기\n","!unzip '*.zip'\n","\n","# tar 파일 압축 풀기\n","!tar -xvf mpii_human_pose_v1.tar.gz -C .\n","\n","# mpii_human_pose_v1_u12_2 폴더로 이동\n","cd mpii_human_pose_v1_u12_2\n","\n","# json 파일 다운로드\n","!wget https://d3s0tskafalll9.cloudfront.net/media/documents/train.json\n","!wget https://d3s0tskafalll9.cloudfront.net/media/documents/validation.json\n","\n","# 작업 폴더로 이동\n","cd ..\n","```"],"metadata":{"id":"QD3b0uGhJP7L"}},{"cell_type":"code","source":["!pip install ray"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JyHrAaIMIi8v","executionInfo":{"status":"ok","timestamp":1650725110669,"user_tz":-540,"elapsed":3230,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"0c03cf4c-fab5-4f71-e51f-700f79b3ad76"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: ray in /usr/local/lib/python3.7/dist-packages (1.12.0)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray) (7.1.2)\n","Requirement already satisfied: virtualenv in /usr/local/lib/python3.7/dist-packages (from ray) (20.14.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray) (3.6.0)\n","Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray) (1.0.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray) (2.23.0)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray) (4.3.3)\n","Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray) (1.21.6)\n","Requirement already satisfied: frozenlist in /usr/local/lib/python3.7/dist-packages (from ray) (1.3.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray) (3.13)\n","Requirement already satisfied: aiosignal in /usr/local/lib/python3.7/dist-packages (from ray) (1.2.0)\n","Requirement already satisfied: grpcio<=1.43.0,>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray) (1.43.0)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray) (21.4.0)\n","Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray) (3.17.3)\n","Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<=1.43.0,>=1.28.1->ray) (1.15.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (4.11.3)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (5.7.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (0.18.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (4.1.1)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->ray) (3.8.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (2021.10.8)\n","Requirement already satisfied: distlib<1,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from virtualenv->ray) (0.3.4)\n","Requirement already satisfied: platformdirs<3,>=2 in /usr/local/lib/python3.7/dist-packages (from virtualenv->ray) (2.5.2)\n"]}]},{"cell_type":"markdown","source":["# 16-2. 데이터 전처리하기"],"metadata":{"id":"3ORkSrxrqKTg"}},{"cell_type":"code","source":["# 주의! ray를 tensorflow보다 먼저 import하면 오류가 발생할 수 있습니다\n","import io, json, os, math\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Add, Concatenate, Lambda\n","from tensorflow.keras.layers import Input, Conv2D, ReLU, MaxPool2D\n","from tensorflow.keras.layers import UpSampling2D, ZeroPadding2D\n","from tensorflow.keras.layers import BatchNormalization\n","import ray\n","\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","PROJECT_PATH = '/content/drive/MyDrive/aiffel_dataset/GD16_human_pose_sticker/mpii/'\n","IMAGE_PATH = os.path.join(PROJECT_PATH, 'images')\n","MODEL_PATH = os.path.join(PROJECT_PATH, 'models')\n","TFRECORD_PATH = os.path.join(PROJECT_PATH, 'tfrecords_mpii')\n","TRAIN_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'train.json')\n","VALID_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'validation.json')\n","\n","print('슝=3')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SOhsXPktOGsD","executionInfo":{"status":"ok","timestamp":1650725113677,"user_tz":-540,"elapsed":3013,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"2b41bcf4-aa07-4336-80aa-561e641ceea7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}]},{"cell_type":"markdown","source":["### json 파싱하기\n","`train.json`, `validation.json` 파일을 다운받았다. 이 파일들은 이미지에 담겨 있는 사람들의 pose keypoint 정보들을 갖고 있어 Pose Estimation을 위한 label로 삼을 수 있따.\n","\n","먼저 json이 어떻게 구성되어있는지 샘플로 annotation 정보를 1개만 출력해보자.   \n","`json.dumps()`를 활용해서 좀 더 명확하게 하면 더 좋다."],"metadata":{"id":"ZI4VbHOyk2aV"}},{"cell_type":"code","source":["with open(TRAIN_JSON) as train_json:\n","    train_annos = json.load(train_json)\n","    json_formatted_str = json.dumps(train_annos[0], indent=2)\n","    print(json_formatted_str)"],"metadata":{"id":"LEhy-CSwlR4j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650725114585,"user_tz":-540,"elapsed":911,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"b918ee1d-50a4-47e8-afa7-0cb936549210"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","  \"joints_vis\": [\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1\n","  ],\n","  \"joints\": [\n","    [\n","      620.0,\n","      394.0\n","    ],\n","    [\n","      616.0,\n","      269.0\n","    ],\n","    [\n","      573.0,\n","      185.0\n","    ],\n","    [\n","      647.0,\n","      188.0\n","    ],\n","    [\n","      661.0,\n","      221.0\n","    ],\n","    [\n","      656.0,\n","      231.0\n","    ],\n","    [\n","      610.0,\n","      187.0\n","    ],\n","    [\n","      647.0,\n","      176.0\n","    ],\n","    [\n","      637.0201,\n","      189.8183\n","    ],\n","    [\n","      695.9799,\n","      108.1817\n","    ],\n","    [\n","      606.0,\n","      217.0\n","    ],\n","    [\n","      553.0,\n","      161.0\n","    ],\n","    [\n","      601.0,\n","      167.0\n","    ],\n","    [\n","      692.0,\n","      185.0\n","    ],\n","    [\n","      693.0,\n","      240.0\n","    ],\n","    [\n","      688.0,\n","      313.0\n","    ]\n","  ],\n","  \"image\": \"015601864.jpg\",\n","  \"scale\": 3.021046,\n","  \"center\": [\n","    594.0,\n","    257.0\n","  ]\n","}\n"]}]},{"cell_type":"markdown","source":["`joints`가 우리가 label로 사용할 keypoint의 label이다.   \n","이미지 형상과 사람의 포즈에 따라 모든 label이 이미지에 나타나지 않기 때문에 `joints_vis`를 이용해서 실제 사용할 수 있는 keypoint 인지 나타낸다.   \n","MPII의 경우 1(visible), 0(non)으로만 나누어지기 때문에 조금 더 쉽게 사용할 수 있다. coco의 경우 2, 1, 0으로 표현해서 occlusion 상황까지 label화 되어 있다.\n","\n","`joint` 순서는 아래와 같은 순서로 배치되어 저장했다.   \n","- 0- 오른쪽 발목, 1- 오른쪽 무릎, 2- 오른쪽 엉덩이, 3- 왼쪽 엉덩이, 4- 왼쪽 무릎, 5- 왼쪽 발목\n","- 6- 골반, 7- 가슴(흉부), 8- 목, 9- 머리 위\n","- 10- 오른쪽 손목, 11- 오른쪽 팔꿈치, 12- 오른쪽 어깨, 13- 왼쪽 어깨, 14- 왼쪽 팔꿈치, 15- 왼쪽 손목\n","\n","`scale`, `center`는 사람 몸의 크기와 중심점이다. `scale`은 200을 곱해야 온전한 크기가 된다.\n","\n","json annotation을 파싱하는 함수를 만들어보자."],"metadata":{"id":"K3WXcMLll-nE"}},{"cell_type":"code","source":["def parse_one_annotation(anno, image_dir):\n","    filename = anno['image']\n","    joints = anno['joints']\n","    joints_visibility = anno['joints_vis']\n","    annotation = {\n","        'filename': filename,\n","        'filepath': os.path.join(image_dir, filename),\n","        'joints_visibility': joints_visibility,\n","        'joints': joints,\n","        'center': anno['center'],\n","        'scale' : anno['scale']\n","    }\n","    return annotation\n","\n","print('슝=3')"],"metadata":{"id":"j7jppVjRoZtM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650725114586,"user_tz":-540,"elapsed":11,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"08942131-1dfe-449e-db0b-85b5f5fb62c9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}]},{"cell_type":"markdown","source":["위 함수를 테스트해보자."],"metadata":{"id":"lT4n3DcF9I7e"}},{"cell_type":"code","source":["with open(TRAIN_JSON) as train_json:\n","    train_annos = json.load(train_json)\n","    test = parse_one_annotation(train_annos[0], IMAGE_PATH)\n","    print(test)"],"metadata":{"id":"TKfxabca9IN3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650725115304,"user_tz":-540,"elapsed":724,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"7554e2d5-6936-4c07-bcdb-420804e6a924"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["{'filename': '015601864.jpg', 'filepath': '/content/drive/MyDrive/aiffel_dataset/GD16_human_pose_sticker/mpii/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n"]}]},{"cell_type":"markdown","source":["# 16-3. TFRecord 파일 만들기"],"metadata":{"id":"o_Ez_umdqKWS"}},{"cell_type":"markdown","source":["### TFRecord 파일 만들기\n","이전 프로젝트에서는 tf.keras의 `ImageDataGenerator`를 이용해서 주로 학습 데이터를 읽었다. 하지만 실제 프로젝트는 튜토리얼 데이터셋보다 훨씬 큰 크기의 데이터를 다뤄야 한다.\n","\n","학습을 많이 해볼수록 학습 속도에 관심을 갖게 되는데, tensorflow 튜토리얼 문서에는 다음과 같은 표현으로 나타나 있다.   \n","> unless you are using tf.data and reading data is still the bottleneck to training\n","\n","일반적으로 학습 과정에서 gpu의 연산 속도보다 HDD I/O 속도가 느리기 때문에 병목 현상이 발생하고 대단위 프로젝트 실험에서 효율성이 떨어지는 것을 관찰할 수 있다.   \n","따라서 학습 데이터를 어떻게 빠르가 읽는가? 에 대한 고민을 반드시 수행해야 더 많은 실험을 할 수 있다."],"metadata":{"id":"4PE6xZ_aCc8S"}},{"cell_type":"markdown","source":["- [tf.data API로 성능 향상하기](https://www.tensorflow.org/guide/data_performance?hl=ko)\n","\n","학습 속도를 향상시키기 위해 데이터 변환이 병렬화되어야 한다. 그래서 데이터를 읽거나 가져올때(prefetch) 또는 데이터 변환 단계에서 gpu 학습과 병렬적으로 수행되도도록 prefetch를 적용해야 한다.   \n","수행하기 위해 `tf.data.Dataset.map` 변환을 제공하는 함수를 이용하고 이를 캐시에 저장하여 학습이 끝나면 바로 변환된 데이터를 제공해야한다."],"metadata":{"id":"mXjAbvRKv3m2"}},{"cell_type":"markdown","source":["tf에서 위 변환을 자동화해주는 도구를 제공한다. 데이터셋을 TFRecord 형태로 표현하는 것인데, 이는 binary record sequence를 저장하기 위한 형식이다. 그리고 내부적으로 protocol buffer를 이용한다.   \n","- [https://developers.google.com/protocol-buffers/?hl=ko](https://developers.google.com/protocol-buffers/?hl=ko)\n","\n","protocol buffer는 크로스 플랫폼에서 사용할 수 있는 직렬화 데이터 라이브러리라 생각하면 된다."],"metadata":{"id":"Z9jqkmk2zH0K"}},{"cell_type":"markdown","source":["데이터를 직렬화 한다는 것은 의미가 있는 정보만 추출해 나열하는 것을 말한다.   \n","예를들어 \"서울 강동구에 사는 30살 손흥민\"을 나타내기 위해 \"서울강동30손흥민\"으로 쓴다면 데이터만 남을 수 있다.   \n","데이터 형식을 고정해야 하기 때문에 첫 두 글자는 시를 뜻하고, 다음 두 글자는 구를, 다음 두 자리는 숫자가 나오고, 이름은 세 글자여야 하는 제한이 생긴다.   \n","100살이 넘는 사람이나 이름이 세 글자가 아니라면 형식에서 벗어날 수 있다. 그래서 매우 정형화된 데이터를 직렬화 하면 데이터 크기와 처리 속도 측면에서 유리하지만 사용하는데 제한이 있다고 기억해두자. 어떤 데이터에서 어떠한 방법으로 직렬화를 수행할지에 따라 발생되는 제한이 달라질 수도 있다."],"metadata":{"id":"1lBRpnUR1fdx"}},{"cell_type":"markdown","source":["구현을 시작해보자.   \n","앞서 추출한 annotation을 TFRecord로 변환하는 함수를 만들자. TFRecord는 `tf.train.Example`들의 합으로 이루어지므로 하나의 annotation을 하나의 `tf.train.Example`로 만들어주는 함수부터 작성해야한다."],"metadata":{"id":"VzaVVpQR2jDI"}},{"cell_type":"code","source":["def generate_tfexample(anno):\n","\n","    # byte 인코딩을 위한 함수\n","    def _bytes_feature(value):\n","        if isinstance(value, type(tf.constant(0))):\n","            value = value.numpy()\n","        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n","\n","    filename = anno['filename']\n","    filepath = anno['filepath']\n","    with open(filepath, 'rb') as image_file:\n","        content = image_file.read()\n","\n","    image = Image.open(filepath)\n","    if image.format != 'JPEG' or image.mode != 'RGB':\n","        image_rgb = image.convert('RGB')\n","        with io.BytesIO() as output:\n","            image_rgb.save(output, format=\"JPEG\", quality=95)\n","            content = output.getvalue()\n","\n","    width, height = image.size\n","    depth = 3\n","\n","    c_x = int(anno['center'][0])\n","    c_y = int(anno['center'][1])\n","    scale = anno['scale']\n","\n","    x = [\n","        int(joint[0]) if joint[0] >= 0 else int(joint[0]) \n","        for joint in anno['joints']\n","    ]\n","    y = [\n","        int(joint[1]) if joint[1] >= 0 else int(joint[0]) \n","        for joint in anno['joints']\n","    ]\n","\n","    v = [0 if joint_v == 0 else 2 for joint_v in anno['joints_visibility']]\n","\n","    feature = {\n","        'image/height':\n","        tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n","        'image/width':\n","        tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n","        'image/depth':\n","        tf.train.Feature(int64_list=tf.train.Int64List(value=[depth])),\n","        'image/object/parts/x':\n","        tf.train.Feature(int64_list=tf.train.Int64List(value=x)),\n","        'image/object/parts/y':\n","        tf.train.Feature(int64_list=tf.train.Int64List(value=y)),\n","        'image/object/center/x': \n","        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_x])),\n","        'image/object/center/y': \n","        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_y])),\n","        'image/object/scale':\n","        tf.train.Feature(float_list=tf.train.FloatList(value=[scale])),\n","        'image/object/parts/v':\n","        tf.train.Feature(int64_list=tf.train.Int64List(value=v)),\n","        'image/encoded':\n","        _bytes_feature(content),\n","        'image/filename':\n","        _bytes_feature(filename.encode())\n","    }\n","\n","    return tf.train.Example(features=tf.train.Features(feature=feature))\n","\n","print('슝=3')"],"metadata":{"id":"iJu_mmz7464k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650725115304,"user_tz":-540,"elapsed":11,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"4ce4a759-50db-4778-9a65-e8256bfbfc4b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}]},{"cell_type":"markdown","source":["하나의 annotation이 `tf.train.Example`이 되었다면 이제 여러 annotation에 대해 작업할 수 있도록 함수를 만들어보자.\n","\n","여기서 하나의 TFRecord를 만들지 않고 여러 TFRecord를 만들어보자. 먼저 얼마나 많은 TFRecord를 만들지 결정할 함수를 만들자."],"metadata":{"id":"gng5Lnvr47sY"}},{"cell_type":"code","source":["def chunkify(l, n):\n","    size = len(l) // n\n","    start = 0\n","    results = []\n","    for i in range(n):\n","        results.append(l[start:start + size])\n","        start += size\n","    return results\n","\n","print('슝=3')"],"metadata":{"id":"gzNMY9DI47Xo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650725115305,"user_tz":-540,"elapsed":9,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"996968dc-2364-4ea8-ff48-6ae9c2d8504b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}]},{"cell_type":"markdown","source":["이 함수는 전체 데이터를 몇 개의 그룹으로 나눌지 결정해준다. 전체 데이터 `l`을 `n`그룹으로 나눈다. 결과적으로 `n`개의 TFRecord 파일을 만들겠다는 이야기이다.\n","\n","더 전문적으로 `n`개로 shard 했다고 말한다.   \n","기업 단위의 데이터는 매우 크고 여러 장비에 나누어져 있기 때문에 sharding은 자주 이뤄진다. 하나의 큰 데이터를 여러 개의 파일로 쪼개고 여러 컴퓨터에 나누어 담는 것이라고 여기면 된다. 이렇게 하면 데이터 저장, 병렬 처리하는데 이점이 있다.\n","\n","데이터는 원래 하나의 큰 데이터였기 때문에 저장 후 어떻게 사용할 것인가에 따라 sharding 전략이 달라진다. 이번에는 간단히 다뤄보자.   \n","모델을 학습하는데 각 파일의 크기와 개수만 고려하면, 너무 작은 파일로 많이 나누는 것도, 너무 큰 파일로 적게 나누는 것도 좋지 않다. 너무 작은 파일로 많이 나누면 학습 중간에 잦은 입출력이 요구되고, 너무 큰 파일로 적게 나누면 입출려감다 걸리는 시간이 길어진다. 입출력에 걸리는 시간이 GPU 계산 시간보다 길어지면 그만큼 손해가 된다.   \n","적절한 파일 크기와 개수는 케바케라고 할 수 있다.\n","\n","- [TFRecord 및 tf.train.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord?hl=ko)\n","\n","위 링크를 참고하면 annotation을 shard로 왜 나눠야하는지 알 수 있다.   \n","I/O 병목현상을 피하기 위해 입력 파일을 여러개로 나눈 뒤 병렬적으로 prefetch 하는 것이 학습 속도를 빠르게 한다.   \n","참고를 보면 데이터를 읽는 호스트보다 최소 10배 많은 파일을 보유하는게 좋다고 한다. 각 파일은 동시에 I/O prefetch의 이점을 누릴 수 있도록 충분히 커야한다고 나와있다.(최소 10MB 이상, 이상적으로 100MB 이상)\n","\n","설명은 어렵지만 실행하면 단순하다. `chunkify` 함수를 테스트해보자."],"metadata":{"id":"JCkYOUT65JG0"}},{"cell_type":"code","source":["test_chunks = chunkify([0] * 1000, 64)\n","print(test_chunks)\n","print(len(test_chunks))\n","print(len(test_chunks[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vVnEy5jepxkk","executionInfo":{"status":"ok","timestamp":1650725115305,"user_tz":-540,"elapsed":8,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"b2a9ebbd-efa8-496f-9833-6ca605efd6ae"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n","64\n","15\n"]}]},{"cell_type":"markdown","source":["0이 1000개가 들어있는 리스트를 64개 만들었다.   \n","하나의 chunk를 TFRecord로 만들어 줄 함수를 만들자."],"metadata":{"id":"qtsMfFU5pz55"}},{"cell_type":"code","source":["@ray.remote\n","def build_single_tfrecord(chunk, path):\n","    print('start to build tf records for ' + path)\n","\n","    with tf.io.TFRecordWriter(path) as writer:\n","        for anno in chunk:\n","            tf_example = generate_tfexample(anno)\n","            writer.write(tf_example.SerializeToString())\n","\n","    print('finished building tf records for ' + path)\n","\n","print('슝=3')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wf1tvMAgp-8o","executionInfo":{"status":"ok","timestamp":1650725115306,"user_tz":-540,"elapsed":8,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"8fec8ba3-05a5-4d35-e07d-1f409feefdad"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}]},{"cell_type":"markdown","source":["`chunk`안에 여러 annotation이 있고, 이들은 `tf.train.Example`로 변환된 후에 문자열로 직렬화되어 TFRecord에 담긴다.\n","\n","한 가지 주의할 점은 함수 정의 위에 `@ray.remote`가 있다는 점이다.\n","\n","- [RAY](https://www.ray.io/)\n","\n","Ray는 병렬 처리를 위한 라이브러리로, 파이썬에서 기본적으로 제공하는 multiprocession 패키지보다 편하게 다양한 환경에서 사용할 수 있다."],"metadata":{"id":"R-C7PaRuq9N2"}},{"cell_type":"markdown","source":["준비가 끝났으니 전체 데이터를 적당한 수의 TFRecord 파일로 만들어주는 함수를 만들어보자. ray를 사용하기 때문에 함수를 호출하는 문법이 약간 다르다."],"metadata":{"id":"i1QGwuVfr26w"}},{"cell_type":"code","source":["def build_tf_records(annotations, total_shards, split):\n","    chunks = chunkify(annotations, total_shards)\n","    futures = [\n","        build_single_tfrecord.remote(\n","            chunk, '{}/{}_{}_of_{}.tfrecords'.format(\n","                TFRECORD_PATH,\n","                split,\n","                str(i + 1).zfill(4),\n","                str(total_shards).zfill(4),\n","            )) for i, chunk in enumerate(chunks)\n","    ]\n","    ray.get(futures)\n","\n","print('슝=3')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1NG0BRaur_de","executionInfo":{"status":"ok","timestamp":1650725115306,"user_tz":-540,"elapsed":6,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"a5afee13-0670-4a99-b598-ac915ba6a460"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}]},{"cell_type":"markdown","source":["# 16-4. Ray"],"metadata":{"id":"c4TEst-qqKY7"}},{"cell_type":"markdown","source":["위에서 작성한 함수를 사용해 데이터를 TFRecord로 만들어주자.   \n","train 데이터는 64개, val 데이터는 8개의 파일로 만들자."],"metadata":{"id":"ekaOdw77Hxyd"}},{"cell_type":"code","source":["num_train_shards = 64\n","num_val_shards = 8\n","\n","ray.init()\n","\n","print('Start to parse annotations.')\n","if not os.path.exists(TFRECORD_PATH):\n","    os.makedirs(TFRECORD_PATH)\n","\n","with open(TRAIN_JSON) as train_json:\n","    train_annos = json.load(train_json)\n","    train_annotations = [\n","        parse_one_annotation(anno, IMAGE_PATH)\n","        for anno in train_annos\n","    ]\n","    print('First train annotation: ', train_annotations[0])\n","\n","with open(VALID_JSON) as val_json:\n","    val_annos = json.load(val_json)\n","    val_annotations = [\n","        parse_one_annotation(anno, IMAGE_PATH) \n","        for anno in val_annos\n","    ]\n","    print('First val annotation: ', val_annotations[0])\n","    \n","print('Start to build TF Records.')\n","build_tf_records(train_annotations, num_train_shards, 'train')\n","build_tf_records(val_annotations, num_val_shards, 'val')\n","\n","print('Successfully wrote {} annotations to TF Records.'.format(\n","    len(train_annotations) + len(val_annotations)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":381},"id":"b5iOkIR5H6GK","executionInfo":{"status":"error","timestamp":1650725118281,"user_tz":-540,"elapsed":2980,"user":{"displayName":"노현호","userId":"15410213599666758892"}},"outputId":"da3fd1c3-cde4-4607-d27d-19b4d0f2dac1"},"execution_count":11,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-4c5c044fc182>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_val_shards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Start to parse annotations.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/worker.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(address, num_cpus, num_gpus, resources, object_store_memory, local_mode, ignore_reinit_error, include_dashboard, dashboard_host, dashboard_port, job_config, configure_logging, logging_level, logging_format, log_to_driver, namespace, runtime_env, storage, _enable_object_reconstruction, _redis_max_memory, _plasma_directory, _node_ip_address, _driver_object_store_memory, _memory, _redis_password, _temp_dir, _metrics_export_port, _system_config, _tracing_startup_hook, **kwargs)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0;31m# isn't called.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m         _global_node = ray.node.Node(\n\u001b[0;32m-> 1023\u001b[0;31m             \u001b[0mhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshutdown_at_exit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspawn_reaper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mray_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mray_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m         )\n\u001b[1;32m   1025\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/node.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ray_params, head, shutdown_at_exit, spawn_reaper, connect_only)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconnect_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_ray_processes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m             \u001b[0;31m# we should update the address info after the node has been started\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/node.py\u001b[0m in \u001b[0;36mstart_ray_processes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;31m# Make sure we don't call `determine_plasma_store_config` multiple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;31m# times to avoid printing multiple warnings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m         \u001b[0mresource_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_resource_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         (\n\u001b[1;32m   1132\u001b[0m             \u001b[0mplasma_directory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/node.py\u001b[0m in \u001b[0;36mget_resource_spec\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0mresources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ray_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mredis_max_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m             ).resolve(is_head=self.head, node_ip_address=self.node_ip_address)\n\u001b[0m\u001b[1;32m    482\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/_private/resource_spec.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self, is_head, node_ip_address)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;31m# Choose a default object store size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0msystem_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_private\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_system_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mavail_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_private\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_available_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mobject_store_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_store_memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/_private/utils.py\u001b[0m in \u001b[0;36mget_system_memory\u001b[0;34m()\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_limit_filename_v2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_limit_filename_v2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m             \u001b[0mdocker_limit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;31m# Use psutil if it is available.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'max\\n'"]}]},{"cell_type":"markdown","source":["Ray에 대해 더 알아보자\n","- [What is Ray?](https://docs.ray.io/en/latest/)\n","\n","위 자료에 주어진 예시 코드를 보면 ray를 사용하는 방법이 있다.   \n","```\n","import ray\n","ray.init()\n","\n","@ray.remote\n","def f(x):\n","    return x * x\n","\n","futures = [f.remote(i) for i in range(4)]\n","print(ray.get(futures)) # [0, 1, 4, 9]\n","\n","@ray.remote\n","class Counter(object):\n","    def __init__(self):\n","        self.n = 0\n","\n","    def increment(self):\n","        self.n += 1\n","\n","    def read(self):\n","        return self.n\n","\n","counters = [Counter.remote() for i in range(4)]\n","[c.increment.remote() for c in counters]\n","futures = [c.read.remote() for c in counters]\n","print(ray.get(futures)) # [1, 1, 1, 1]\n","```"],"metadata":{"id":"IOlMdKHDH70Z"}},{"cell_type":"markdown","source":["함수나 클래스에 `@ray.remote` 데코레이터를 붙이고 `some_function.remote()` 형식으로 함수를 만든다.   \n","클래스의 경우 메서드를 호출할 때 `remote()`를 이용한다. 함수나 메서드는 이 시점에 실행이 아닌 생성만 된다. 그리고 `ray.get()`을 통해 실행이 되는 구조이다.   \n","함수를 바로 호출하는게 아닌 작업 생성만 해놓고 나중에 실행한다는 점을 유의하자.\n","\n","- [10x Faster Parallel Python Without Python Multiprocessing(https://towardsdatascience.com/10x-faster-parallel-python-without-python-multiprocessing-e5017c93cce1)\n","\n","multiprocessing과 ray의 사용상 큰 차이점은 multiprocessing은 병렬화를 위해 추상적 구조를 새로 설계해야 하지만 ray는 쓰던 코드에서 거의 수정 없이 병렬화 할 수 있는 장점이 있다."],"metadata":{"id":"P5XTlVukIQPZ"}},{"cell_type":"markdown","source":["# 16-5. data label로 만들기"],"metadata":{"id":"a1ejRx6cqKcK"}},{"cell_type":"markdown","source":["TFRecord로 저장된 데이터를 모델의 학습에 필요한 데이터로 바꿔줄 함수가 필요하다. tensorflow에서 이미 제공해주는 함수를 사용하면 되기 때문에 간단하다.   \n","주의할 점은 TFRecord가 직렬화된 데이터이기 때문에 만들 때 데이터 순서와 읽어올 때 데이터 순서가 같아야 한다는 점이다. 데이터의 형식도 동일하게 맞춰야 한다."],"metadata":{"id":"uyXwt4-kyzDn"}},{"cell_type":"code","source":["def parse_tfexample(example):\n","    image_feature_description = {\n","        'image/height': tf.io.FixedLenFeature([], tf.int64),\n","        'image/width': tf.io.FixedLenFeature([], tf.int64),\n","        'image/depth': tf.io.FixedLenFeature([], tf.int64),\n","        'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n","        'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n","        'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n","        'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n","        'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n","        'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n","        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n","        'image/filename': tf.io.FixedLenFeature([], tf.string),\n","    }\n","    return tf.io.parse_single_example(example, image_feature_description)\n","\n","print('슝=3')"],"metadata":{"id":"zPIbJT_M1TzF","executionInfo":{"status":"aborted","timestamp":1650725118277,"user_tz":-540,"elapsed":5,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이렇게 얻은 image와 label을 이용해서 적절한 학습 형태로 변환한다. 이미지를 그대로 사용하지 않고 적당히 정사각형으로 crop하여 사용한다.\n","\n","우리가 알고 있는 정보는 joints의 위치, center의 좌표, body height값이다. 균일하게 학습하기 위해 body width를 적절히 정하는 것도 중요하다. 이와 관련해서 여러 방법이 있을 수 있겠지만 배우는 단계에서 더 중요하게 봐야 할 부분은 우리가 임의로 조정한 crop box가 이미지 바깥으로 나가지 않는지 예외 처리를 잘 해주어야 한다는 점이다."],"metadata":{"id":"3dWRWEOm1njB"}},{"cell_type":"code","source":["def crop_roi(image, features, margin=0.2):\n","    img_shape = tf.shape(image)\n","    img_height = img_shape[0]\n","    img_width = img_shape[1]\n","    img_depth = img_shape[2]\n","\n","    keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n","    keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n","    center_x = features['image/object/center/x']\n","    center_y = features['image/object/center/y']\n","    body_height = features['image/object/scale'] * 200.0\n","\n","    # keypoint 중 유효한값(visible = 1) 만 사용합니다.\n","    masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n","    masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n","\n","    # min, max 값을 찾습니다.\n","    keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n","    keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n","    keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n","    keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n","\n","    # 높이 값을 이용해서 x, y 위치를 재조정 합니다. 박스를 정사각형으로 사용하기 위해 아래와 같이 사용합니다.\n","    xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n","    xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n","    ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n","    ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n","\n","    # 이미지 크기를 벗어나는 점을 재조정 해줍니다.\n","    effective_xmin = xmin if xmin > 0 else 0\n","    effective_ymin = ymin if ymin > 0 else 0\n","    effective_xmax = xmax if xmax < img_width else img_width\n","    effective_ymax = ymax if ymax < img_height else img_height\n","    effective_height = effective_ymax - effective_ymin\n","    effective_width = effective_xmax - effective_xmin\n","\n","    image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n","    new_shape = tf.shape(image)\n","    new_height = new_shape[0]\n","    new_width = new_shape[1]\n","\n","    effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n","    effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n","\n","    return image, effective_keypoint_x, effective_keypoint_y\n","\n","print('슝=3')"],"metadata":{"id":"4_p9_lhA17Jq","executionInfo":{"status":"aborted","timestamp":1650725118278,"user_tz":-540,"elapsed":6,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["![](https://d3s0tskafalll9.cloudfront.net/media/images/GC-10-P-2.max-800x600.png)\n","\n","(x, y) 좌표로 되어있는 keypoint를 heatmap으로 변경한다. 하나의 점에만 표시 되어있는 정보를 좌표 근처 여러 지점에 확률 분포 형태로 학습시키면 결과가 더 좋았던 케이스가 있다. 이러한 확률 분포 형태의 정보를 heatmap이라고 부른다.   \n","확률 분포로는 2차원 가우시안 분포를 사용한다.\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/original_images/GC-10-P-3.png)"],"metadata":{"id":"pBg2NLfr178l"}},{"cell_type":"code","source":["def generate_2d_guassian(height, width, y0, x0, visibility=2, sigma=1, scale=12):\n","    heatmap = tf.zeros((height, width))\n","\n","    xmin = x0 - 3 * sigma\n","    ymin = y0 - 3 * sigma\n","    xmax = x0 + 3 * sigma\n","    ymax = y0 + 3 * sigma\n","    \n","    if xmin >= width or ymin >= height or xmax < 0 or ymax < 0 or visibility == 0:\n","        return heatmap\n","\n","    size = 6 * sigma + 1\n","    x, y = tf.meshgrid(tf.range(0, 6 * sigma + 1, 1), tf.range(0, 6 * sigma + 1, 1), indexing='xy')\n","\n","    center_x = size // 2\n","    center_y = size // 2\n","\n","    gaussian_patch = tf.cast(tf.math.exp(\n","        -(tf.math.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale,\n","                             dtype=tf.float32)\n","\n","    patch_xmin = tf.math.maximum(0, -xmin)\n","    patch_ymin = tf.math.maximum(0, -ymin)\n","    patch_xmax = tf.math.minimum(xmax, width) - xmin\n","    patch_ymax = tf.math.minimum(ymax, height) - ymin\n","\n","    heatmap_xmin = tf.math.maximum(0, xmin)\n","    heatmap_ymin = tf.math.maximum(0, ymin)\n","    heatmap_xmax = tf.math.minimum(xmax, width)\n","    heatmap_ymax = tf.math.minimum(ymax, height)\n","\n","    indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n","    updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n","\n","    count = 0\n","\n","    for j in tf.range(patch_ymin, patch_ymax):\n","        for i in tf.range(patch_xmin, patch_xmax):\n","            indices = indices.write(count, [heatmap_ymin + j, heatmap_xmin + i])\n","            updates = updates.write(count, gaussian_patch[j][i])\n","            count += 1\n","\n","    heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n","\n","    return heatmap\n","\n","def make_heatmaps(features, keypoint_x, keypoint_y, heatmap_shape):\n","    v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n","    x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n","    y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n","\n","    num_heatmap = heatmap_shape[2]\n","    heatmap_array = tf.TensorArray(tf.float32, 16)\n","\n","    for i in range(num_heatmap):\n","        gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n","        heatmap_array = heatmap_array.write(i, gaussian)\n","\n","    heatmaps = heatmap_array.stack()\n","    heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0])  # change to (64, 64, 16)\n","\n","    return heatmaps\n","\n","print('슝=3')"],"metadata":{"id":"GmC57ZCm2WKK","executionInfo":{"status":"aborted","timestamp":1650725118278,"user_tz":-540,"elapsed":5,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["지금까지 만든 함수들을 객체 형태로 조합해보자. 객체 형태로 만들면 선언부는 복잡해 보여도 훨씬 장점이 많다.   \n","함수에서 객체의 메서드로 수정할 때는 `self`를 추가해야 한다."],"metadata":{"id":"UnrTik9j2em5"}},{"cell_type":"code","source":["class Preprocessor(object):\n","    def __init__(self,\n","                 image_shape=(256, 256, 3),\n","                 heatmap_shape=(64, 64, 16),\n","                 is_train=False):\n","        self.is_train = is_train\n","        self.image_shape = image_shape\n","        self.heatmap_shape = heatmap_shape\n","\n","    def __call__(self, example):\n","        features = self.parse_tfexample(example)\n","        image = tf.io.decode_jpeg(features['image/encoded'])\n","\n","        if self.is_train:\n","            random_margin = tf.random.uniform([1], 0.1, 0.3)[0]\n","            image, keypoint_x, keypoint_y = self.crop_roi(image, features, margin=random_margin)\n","            image = tf.image.resize(image, self.image_shape[0:2])\n","        else:\n","            image, keypoint_x, keypoint_y = self.crop_roi(image, features)\n","            image = tf.image.resize(image, self.image_shape[0:2])\n","\n","        image = tf.cast(image, tf.float32) / 127.5 - 1\n","        heatmaps = self.make_heatmaps(features, keypoint_x, keypoint_y, self.heatmap_shape)\n","\n","        return image, heatmaps\n","\n","        \n","    def crop_roi(self, image, features, margin=0.2):\n","        img_shape = tf.shape(image)\n","        img_height = img_shape[0]\n","        img_width = img_shape[1]\n","        img_depth = img_shape[2]\n","\n","        keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n","        keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n","        center_x = features['image/object/center/x']\n","        center_y = features['image/object/center/y']\n","        body_height = features['image/object/scale'] * 200.0\n","        \n","        masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n","        masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n","        \n","        keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n","        keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n","        keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n","        keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n","        \n","        xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n","        xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n","        ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n","        ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n","        \n","        effective_xmin = xmin if xmin > 0 else 0\n","        effective_ymin = ymin if ymin > 0 else 0\n","        effective_xmax = xmax if xmax < img_width else img_width\n","        effective_ymax = ymax if ymax < img_height else img_height\n","        effective_height = effective_ymax - effective_ymin\n","        effective_width = effective_xmax - effective_xmin\n","\n","        image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n","        new_shape = tf.shape(image)\n","        new_height = new_shape[0]\n","        new_width = new_shape[1]\n","        \n","        effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n","        effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n","        \n","        return image, effective_keypoint_x, effective_keypoint_y\n","        \n","    \n","    def generate_2d_guassian(self, height, width, y0, x0, visibility=2, sigma=1, scale=12):\n","        \n","        heatmap = tf.zeros((height, width))\n","\n","        xmin = x0 - 3 * sigma\n","        ymin = y0 - 3 * sigma\n","        xmax = x0 + 3 * sigma\n","        ymax = y0 + 3 * sigma\n","\n","        if xmin >= width or ymin >= height or xmax < 0 or ymax <0 or visibility == 0:\n","            return heatmap\n","\n","        size = 6 * sigma + 1\n","        x, y = tf.meshgrid(tf.range(0, 6*sigma+1, 1), tf.range(0, 6*sigma+1, 1), indexing='xy')\n","\n","        center_x = size // 2\n","        center_y = size // 2\n","\n","        gaussian_patch = tf.cast(tf.math.exp(-(tf.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale, dtype=tf.float32)\n","\n","        patch_xmin = tf.math.maximum(0, -xmin)\n","        patch_ymin = tf.math.maximum(0, -ymin)\n","        patch_xmax = tf.math.minimum(xmax, width) - xmin\n","        patch_ymax = tf.math.minimum(ymax, height) - ymin\n","\n","        heatmap_xmin = tf.math.maximum(0, xmin)\n","        heatmap_ymin = tf.math.maximum(0, ymin)\n","        heatmap_xmax = tf.math.minimum(xmax, width)\n","        heatmap_ymax = tf.math.minimum(ymax, height)\n","\n","        indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n","        updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n","\n","        count = 0\n","\n","        for j in tf.range(patch_ymin, patch_ymax):\n","            for i in tf.range(patch_xmin, patch_xmax):\n","                indices = indices.write(count, [heatmap_ymin+j, heatmap_xmin+i])\n","                updates = updates.write(count, gaussian_patch[j][i])\n","                count += 1\n","                \n","        heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n","\n","        return heatmap\n","\n","\n","    def make_heatmaps(self, features, keypoint_x, keypoint_y, heatmap_shape):\n","        v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n","        x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n","        y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n","        \n","        num_heatmap = heatmap_shape[2]\n","        heatmap_array = tf.TensorArray(tf.float32, 16)\n","\n","        for i in range(num_heatmap):\n","            gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n","            heatmap_array = heatmap_array.write(i, gaussian)\n","        \n","        heatmaps = heatmap_array.stack()\n","        heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0]) # change to (64, 64, 16)\n","        \n","        return heatmaps\n","\n","    def parse_tfexample(self, example):\n","        image_feature_description = {\n","            'image/height': tf.io.FixedLenFeature([], tf.int64),\n","            'image/width': tf.io.FixedLenFeature([], tf.int64),\n","            'image/depth': tf.io.FixedLenFeature([], tf.int64),\n","            'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n","            'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n","            'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n","            'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n","            'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n","            'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n","            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n","            'image/filename': tf.io.FixedLenFeature([], tf.string),\n","        }\n","        return tf.io.parse_single_example(example,\n","                                          image_feature_description)\n","\n","print('슝=3')"],"metadata":{"id":"vqHPxLtc2pPR","executionInfo":{"status":"aborted","timestamp":1650725118278,"user_tz":-540,"elapsed":5,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["데이터 전처리는 끝이고 모델을 만들어보자."],"metadata":{"id":"cr9a-7A62pgm"}},{"cell_type":"markdown","source":["# 16-6. 모델을 학습해보자"],"metadata":{"id":"JRTMvFF_qKeG"}},{"cell_type":"markdown","source":["### Hourglass 모델 만들기\n","![](https://d3s0tskafalll9.cloudfront.net/media/images/GC-10-P-4.max-800x600.png)\n","\n","이미지처럼 구조를 가진 Hourglass 모델을 구현해보자.   \n","직육면체 박스는 residual block이다.\n","\n","hourglass 모델을 잘 생각해보면 마치 양파처럼 가장 바깥 layer를 제거하면 똑같은 구조가 나타나는 것을 알 수 있다. 이 점을 이용하여 재귀 함수를 이용한다면 간단하게 모델을 표현할 수 있다.   \n","바깥부터 5개의 layer를 만들고 싶다면 order를 이용해서 5, 4... 1이 될 때까지 HourglassModule을 반복하면서 order가 1이 될 때 BottleneckBlock으로 대체하면 아주 간결하게 만들 수 있다."],"metadata":{"id":"jBWNIdrr5BZ6"}},{"cell_type":"code","source":["def BottleneckBlock(inputs, filters, strides=1, downsample=False, name=None):\n","    identity = inputs\n","    if downsample:\n","        identity = Conv2D(\n","            filters=filters,\n","            kernel_size=1,\n","            strides=strides,\n","            padding='same',\n","            kernel_initializer='he_normal')(inputs)\n","\n","    x = BatchNormalization(momentum=0.9)(inputs)\n","    x = ReLU()(x)\n","    x = Conv2D(\n","        filters=filters // 2,\n","        kernel_size=1,\n","        strides=1,\n","        padding='same',\n","        kernel_initializer='he_normal')(x)\n","\n","    x = BatchNormalization(momentum=0.9)(x)\n","    x = ReLU()(x)\n","    x = Conv2D(\n","        filters=filters // 2,\n","        kernel_size=3,\n","        strides=strides,\n","        padding='same',\n","        kernel_initializer='he_normal')(x)\n","\n","    x = BatchNormalization(momentum=0.9)(x)\n","    x = ReLU()(x)\n","    x = Conv2D(\n","        filters=filters,\n","        kernel_size=1,\n","        strides=1,\n","        padding='same',\n","        kernel_initializer='he_normal')(x)\n","\n","    x = Add()([identity, x])\n","    return x\n","\n","print('슝=3')"],"metadata":{"id":"NDuxZvSP8yfg","executionInfo":{"status":"aborted","timestamp":1650725118278,"user_tz":-540,"elapsed":5,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def HourglassModule(inputs, order, filters, num_residual):\n","    \n","    up1 = BottleneckBlock(inputs, filters, downsample=False)\n","    for i in range(num_residual):\n","        up1 = BottleneckBlock(up1, filters, downsample=False)\n","\n","    low1 = MaxPool2D(pool_size=2, strides=2)(inputs)\n","    for i in range(num_residual):\n","        low1 = BottleneckBlock(low1, filters, downsample=False)\n","\n","    low2 = low1\n","    if order > 1:\n","        low2 = HourglassModule(low1, order - 1, filters, num_residual)\n","    else:\n","        for i in range(num_residual):\n","            low2 = BottleneckBlock(low2, filters, downsample=False)\n","\n","    low3 = low2\n","    for i in range(num_residual):\n","        low3 = BottleneckBlock(low3, filters, downsample=False)\n","\n","    up2 = UpSampling2D(size=2)(low3)\n","\n","    return up2 + up1\n","\n","print('슝=3')"],"metadata":{"id":"WY6WwEjp8veb","executionInfo":{"status":"aborted","timestamp":1650725118279,"user_tz":-540,"elapsed":6,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### intermediate output을 위한 linear layer\n","hourglass 모듈을 여러 층으로 쌓은 것이 stacked hourglass network이다. 모델이 깊어지는 만큼 학습이 어려워 intermediate loss(auxilary loss)를 추가해야 하는것을 논문에서 언급했다.\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/original_images/GC-10-P-5.png)"],"metadata":{"id":"klKSUMv685qZ"}},{"cell_type":"code","source":["def LinearLayer(inputs, filters):\n","    x = Conv2D(\n","        filters=filters,\n","        kernel_size=1,\n","        strides=1,\n","        padding='same',\n","        kernel_initializer='he_normal')(inputs)\n","    x = BatchNormalization(momentum=0.9)(x)\n","    x = ReLU()(x)\n","    return x\n","\n","print('슝=3')"],"metadata":{"id":"457sDPYK9GJP","executionInfo":{"status":"aborted","timestamp":1650725118279,"user_tz":-540,"elapsed":6,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["따라서 stacked되는 hourglass층 사이사이에 LinearLayer를 삽입하고 중간 loss를 계산해준다.\n","\n","지금까지 만든 hourglass를 여러 층으로 쌓으면 stacked hourglass가 된다.\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/images/GC-10-P-6.max-800x600.png)"],"metadata":{"id":"P2d61-YY9GU3"}},{"cell_type":"code","source":["def StackedHourglassNetwork(\n","        input_shape=(256, 256, 3), \n","        num_stack=4, \n","        num_residual=1,\n","        num_heatmap=16):\n","    \n","    inputs = Input(shape=input_shape)\n","\n","    x = Conv2D(\n","        filters=64,\n","        kernel_size=7,\n","        strides=2,\n","        padding='same',\n","        kernel_initializer='he_normal')(inputs)\n","    x = BatchNormalization(momentum=0.9)(x)\n","    x = ReLU()(x)\n","    x = BottleneckBlock(x, 128, downsample=True)\n","    x = MaxPool2D(pool_size=2, strides=2)(x)\n","    x = BottleneckBlock(x, 128, downsample=False)\n","    x = BottleneckBlock(x, 256, downsample=True)\n","\n","    ys = []\n","    for i in range(num_stack):\n","        x = HourglassModule(x, order=4, filters=256, num_residual=num_residual)\n","        for i in range(num_residual):\n","            x = BottleneckBlock(x, 256, downsample=False)\n","\n","        x = LinearLayer(x, 256)\n","\n","        y = Conv2D(\n","            filters=num_heatmap,\n","            kernel_size=1,\n","            strides=1,\n","            padding='same',\n","            kernel_initializer='he_normal')(x)\n","        ys.append(y)\n","\n","        if i < num_stack - 1:\n","            y_intermediate_1 = Conv2D(filters=256, kernel_size=1, strides=1)(x)\n","            y_intermediate_2 = Conv2D(filters=256, kernel_size=1, strides=1)(y)\n","            x = Add()([y_intermediate_1, y_intermediate_2])\n","\n","    return tf.keras.Model(inputs, ys, name='stacked_hourglass')\n","\n","print('슝=3')"],"metadata":{"id":"4i2G71Aq9UzX","executionInfo":{"status":"aborted","timestamp":1650725118279,"user_tz":-540,"elapsed":6,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이제 `StackedHourglassNetwork`만 이용하면 모델을 쉽게 만들 수 있다."],"metadata":{"id":"F3wGs75E9U_5"}},{"cell_type":"markdown","source":["# 16-7. 학습 엔진 만들기"],"metadata":{"id":"CdD47kMiqKhV"}},{"cell_type":"markdown","source":["### GPU가 여러 개인 환경\n","모델 학습을 진행할 차례이다.\n","\n","학습할 수 있는 GPU가 여러 개이고 데이터를 병렬로 학습시키려면 어떻게 해야할까? 여러 GPU를 사용하기 위해 약간의 코드를 추가해야한다. 현재 학습 환경은 아니지만 살짝 공부해보자.\n","- [Distributed training with Keras](https://www.tensorflow.org/tutorials/distribute/keras)\n","\n","핵심 키워드는 `tf.distribute.MirroredStrategy`이다.   \n","한 컴퓨터에 GPU가 여러 개인 경우 사용할 수 있는 방법으로 여러 GPU가 모델을 학습한 후 각각의 Loss를 계산하면 CPU가 전체 Loss를 종합한다. 그런 후 모델의 가중치를 업데이트 하도록 하는 것이다.\n","\n","각 GPU에서 계산한 Loss를 토대로 전체 Loss를 종합해주는 역할은 `strategy.reduce` 함수가 담당한다.\n","\n","이번에도 각 함수를 별개로 만들지 않고 하나의 객체로 만들어보자."],"metadata":{"id":"jA1bnHDN9ZTn"}},{"cell_type":"code","source":["class Trainer(object):\n","    def __init__(self,\n","                 model,\n","                 epochs,\n","                 global_batch_size,\n","                 strategy,\n","                 initial_learning_rate):\n","        self.model = model\n","        self.epochs = epochs\n","        self.strategy = strategy\n","        self.global_batch_size = global_batch_size\n","        self.loss_object = tf.keras.losses.MeanSquaredError(\n","            reduction=tf.keras.losses.Reduction.NONE)\n","        self.optimizer = tf.keras.optimizers.Adam(\n","            learning_rate=initial_learning_rate)\n","        self.model = model\n","\n","        self.current_learning_rate = initial_learning_rate\n","        self.last_val_loss = math.inf\n","        self.lowest_val_loss = math.inf\n","        self.patience_count = 0\n","        self.max_patience = 10\n","        self.best_model = None\n","\n","    def lr_decay(self):\n","        if self.patience_count >= self.max_patience:\n","            self.current_learning_rate /= 10.0\n","            self.patience_count = 0\n","        elif self.last_val_loss == self.lowest_val_loss:\n","            self.patience_count = 0\n","        self.patience_count += 1\n","\n","        self.optimizer.learning_rate = self.current_learning_rate\n","\n","    def lr_decay_step(self, epoch):\n","        if epoch == 25 or epoch == 50 or epoch == 75:\n","            self.current_learning_rate /= 10.0\n","        self.optimizer.learning_rate = self.current_learning_rate\n","\n","    def compute_loss(self, labels, outputs):\n","        loss = 0\n","        for output in outputs:\n","            weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n","            loss += tf.math.reduce_mean(\n","                tf.math.square(labels - output) * weights) * (\n","                    1. / self.global_batch_size)\n","        return loss\n","\n","    def train_step(self, inputs):\n","        images, labels = inputs\n","        with tf.GradientTape() as tape:\n","            outputs = self.model(images, training=True)\n","            loss = self.compute_loss(labels, outputs)\n","\n","        grads = tape.gradient(\n","            target=loss, sources=self.model.trainable_variables)\n","        self.optimizer.apply_gradients(\n","            zip(grads, self.model.trainable_variables))\n","\n","        return loss\n","\n","    def val_step(self, inputs):\n","        images, labels = inputs\n","        outputs = self.model(images, training=False)\n","        loss = self.compute_loss(labels, outputs)\n","        return loss\n","\n","    def run(self, train_dist_dataset, val_dist_dataset):\n","        @tf.function\n","        def distributed_train_epoch(dataset):\n","            tf.print('Start distributed traininng...')\n","            total_loss = 0.0\n","            num_train_batches = 0.0\n","            for one_batch in dataset:\n","                per_replica_loss = self.strategy.run(\n","                    self.train_step, args=(one_batch, ))\n","                batch_loss = self.strategy.reduce(\n","                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n","                total_loss += batch_loss\n","                num_train_batches += 1\n","                tf.print('Trained batch', num_train_batches, 'batch loss',\n","                         batch_loss, 'epoch total loss', total_loss / num_train_batches)\n","            return total_loss, num_train_batches\n","\n","        @tf.function\n","        def distributed_val_epoch(dataset):\n","            total_loss = 0.0\n","            num_val_batches = 0.0\n","            for one_batch in dataset:\n","                per_replica_loss = self.strategy.run(\n","                    self.val_step, args=(one_batch, ))\n","                num_val_batches += 1\n","                batch_loss = self.strategy.reduce(\n","                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n","                tf.print('Validated batch', num_val_batches, 'batch loss',\n","                         batch_loss)\n","                if not tf.math.is_nan(batch_loss):\n","                    # TODO: Find out why the last validation batch loss become NaN\n","                    total_loss += batch_loss\n","                else:\n","                    num_val_batches -= 1\n","\n","            return total_loss, num_val_batches\n","\n","        for epoch in range(1, self.epochs + 1):\n","            self.lr_decay()\n","            print('Start epoch {} with learning rate {}'.format(\n","                epoch, self.current_learning_rate))\n","\n","            train_total_loss, num_train_batches = distributed_train_epoch(\n","                train_dist_dataset)\n","            train_loss = train_total_loss / num_train_batches\n","            print('Epoch {} train loss {}'.format(epoch, train_loss))\n","\n","            val_total_loss, num_val_batches = distributed_val_epoch(\n","                val_dist_dataset)\n","            val_loss = val_total_loss / num_val_batches\n","            print('Epoch {} val loss {}'.format(epoch, val_loss))\n","\n","            # save model when reach a new lowest validation loss\n","            if val_loss < self.lowest_val_loss:\n","                self.save_model(epoch, val_loss)\n","                self.lowest_val_loss = val_loss\n","            self.last_val_loss = val_loss\n","\n","        return self.best_model\n","\n","    def save_model(self, epoch, loss):\n","        model_name = MODEL_PATH + '/model-epoch-{}-loss-{:.4f}.h5'.format(epoch, loss)\n","        self.model.save_weights(model_name)\n","        self.best_model = model_name\n","        print(\"Model {} saved.\".format(model_name))\n","\n","print('슝=3')"],"metadata":{"id":"LNQhKmLuAOGj","executionInfo":{"status":"aborted","timestamp":1650725118279,"user_tz":-540,"elapsed":6,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["데이터셋을 만드는 함수를 작성하자. TFRecord 파일이 여러개이므로 `tf.data.Dataset.list_files`를 통해 불러오자."],"metadata":{"id":"MeYcFgEmAO_b"}},{"cell_type":"code","source":["IMAGE_SHAPE = (256, 256, 3)\n","HEATMAP_SIZE = (64, 64)\n","\n","def create_dataset(tfrecords, batch_size, num_heatmap, is_train):\n","    preprocess = Preprocessor(\n","        IMAGE_SHAPE, (HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap), is_train)\n","\n","    dataset = tf.data.Dataset.list_files(tfrecords)\n","    dataset = tf.data.TFRecordDataset(dataset)\n","    dataset = dataset.map(\n","        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","\n","    if is_train:\n","        dataset = dataset.shuffle(batch_size)\n","\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","\n","    return dataset\n","\n","print('슝=3')"],"metadata":{"id":"TaORXqnEAU14","executionInfo":{"status":"aborted","timestamp":1650725118280,"user_tz":-540,"elapsed":7,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["데이터셋과 모델, 훈련용 객체를 조립하면된다. 하나의 함수로 만들어줄탠데 주의할 점은 `with strategy.scope():` 부분이 반드시 필요하다.\n","- [Custom training with tf.distribute.Strategy\n","](https://www.tensorflow.org/tutorials/distribute/custom_training)\n","\n","데이터셋도 `experimental_distribute_dataset`을 통해 연결해줘야 하는 점도 중요하다."],"metadata":{"id":"DWDWnZTmAWDA"}},{"cell_type":"code","source":["def train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords):\n","    strategy = tf.distribute.MirroredStrategy()\n","    global_batch_size = strategy.num_replicas_in_sync * batch_size\n","    train_dataset = create_dataset(\n","        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n","    val_dataset = create_dataset(\n","        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n","\n","    if not os.path.exists(MODEL_PATH):\n","        os.makedirs(MODEL_PATH)\n","\n","    with strategy.scope():\n","        train_dist_dataset = strategy.experimental_distribute_dataset(\n","            train_dataset)\n","        val_dist_dataset = strategy.experimental_distribute_dataset(\n","            val_dataset)\n","\n","        model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap)\n","\n","        trainer = Trainer(\n","            model,\n","            epochs,\n","            global_batch_size,\n","            strategy,\n","            initial_learning_rate=learning_rate)\n","\n","        print('Start training...')\n","        return trainer.run(train_dist_dataset, val_dist_dataset)\n","\n","print('슝=3')"],"metadata":{"id":"E1OU0MAnAodK","executionInfo":{"status":"aborted","timestamp":1650725118280,"user_tz":-540,"elapsed":7,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["모델을 학습시킬 차례이다. 1Epoch를 학습하는데 1시간 가까이 소요되므로 미리 저장해둔 결과를 load해서 사용하자.\n","```\n","train_tfrecords = os.path.join(TFRECORD_PATH, 'train*')\n","val_tfrecords = os.path.join(TFRECORD_PATH, 'val*')\n","epochs = 2\n","batch_size = 16\n","num_heatmap = 16\n","learning_rate = 0.0007\n","\n","best_model_file = train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords)\n","```"],"metadata":{"id":"N2TxcT3oAqD0"}},{"cell_type":"markdown","source":["# 16-8. 둠칫둠칫 댄스타임"],"metadata":{"id":"T7It9a5QqKkZ"}},{"cell_type":"markdown","source":["### 예측 엔진 만들기\n","학습이 끝난 모델이 얼마나 잘 예측하는지 확인해보자. 미리 학습된 모델을 불러오자."],"metadata":{"id":"YFqfmNAhBCIa"}},{"cell_type":"code","source":["WEIGHTS_PATH = os.path.join(PROJECT_PATH, 'models', 'model-v0.0.1-epoch-2-loss-1.3072.h5')\n","\n","model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1)\n","model.load_weights(WEIGHTS_PATH)\n","\n","# 이전의 학습하는 코드 블럭을 통해 학습하고 그 모델을 사용할 경우 아래 주석 처리된 코드를 사용하면 됩니다\n","# model.load_weights(best_model_file)"],"metadata":{"id":"3g3uVEdBBU1Q","executionInfo":{"status":"aborted","timestamp":1650725118280,"user_tz":-540,"elapsed":7,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["학습에 사용했던 keypoint들을 사용해야 하기 때문에 필요한 변수를 지정해주자. 변수에 저장되는 것은 해당 부위를 나타내는 인덱스이다."],"metadata":{"id":"vnEKSS1lBblW"}},{"cell_type":"code","source":["R_ANKLE = 0\n","R_KNEE = 1\n","R_HIP = 2\n","L_HIP = 3\n","L_KNEE = 4\n","L_ANKLE = 5\n","PELVIS = 6\n","THORAX = 7\n","UPPER_NECK = 8\n","HEAD_TOP = 9\n","R_WRIST = 10\n","R_ELBOW = 11\n","R_SHOULDER = 12\n","L_SHOULDER = 13\n","L_ELBOW = 14\n","L_WRIST = 15\n","\n","MPII_BONES = [\n","    [R_ANKLE, R_KNEE],\n","    [R_KNEE, R_HIP],\n","    [R_HIP, PELVIS],\n","    [L_HIP, PELVIS],\n","    [L_HIP, L_KNEE],\n","    [L_KNEE, L_ANKLE],\n","    [PELVIS, THORAX],\n","    [THORAX, UPPER_NECK],\n","    [UPPER_NECK, HEAD_TOP],\n","    [R_WRIST, R_ELBOW],\n","    [R_ELBOW, R_SHOULDER],\n","    [THORAX, R_SHOULDER],\n","    [THORAX, L_SHOULDER],\n","    [L_SHOULDER, L_ELBOW],\n","    [L_ELBOW, L_WRIST]\n","]\n","\n","print('슝=3')"],"metadata":{"id":"QBFDhlDjBhLh","executionInfo":{"status":"aborted","timestamp":1650725118280,"user_tz":-540,"elapsed":7,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["모델을 학습할 때 라벨이 되는 좌표를 heatmap으로 바꿨다. 학습을 heatmap으로 해서 모델의 추론 결과도 heatmap이다. 그래서 heatmap으로부터 좌표를 추출해야한다.   \n","heatmap중 최대값\b을 갖는 지점을 찾아내자."],"metadata":{"id":"Lm8bSV1SBjDd"}},{"cell_type":"code","source":["def find_max_coordinates(heatmaps):\n","    flatten_heatmaps = tf.reshape(heatmaps, (-1, 16))\n","    indices = tf.math.argmax(flatten_heatmaps, axis=0)\n","    y = tf.cast(indices / 64, dtype=tf.int64)\n","    x = indices - 64 * y\n","    return tf.stack([x, y], axis=1).numpy()\n","\n","print('슝=3')"],"metadata":{"id":"0kM8f6TIBzW4","executionInfo":{"status":"aborted","timestamp":1650725118280,"user_tz":-540,"elapsed":7,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["위 함수만으로 256x256 이미지에 64x64 heatmap max값을 표현할 때 quantization 오차가 발생하여 실제 계산에서는 3x3 필터를 이용해서 근사치를 구해준다."],"metadata":{"id":"cwAhczvPBz4v"}},{"cell_type":"code","source":["def extract_keypoints_from_heatmap(heatmaps):\n","    max_keypoints = find_max_coordinates(heatmaps)\n","\n","    padded_heatmap = np.pad(heatmaps, [[1,1],[1,1],[0,0]], mode='constant')\n","    adjusted_keypoints = []\n","    for i, keypoint in enumerate(max_keypoints):\n","        max_y = keypoint[1]+1\n","        max_x = keypoint[0]+1\n","        \n","        patch = padded_heatmap[max_y-1:max_y+2, max_x-1:max_x+2, i]\n","        patch[1][1] = 0\n","        \n","        index = np.argmax(patch)\n","        \n","        next_y = index // 3\n","        next_x = index - next_y * 3\n","        delta_y = (next_y - 1) / 4\n","        delta_x = (next_x - 1) / 4\n","        \n","        adjusted_keypoint_x = keypoint[0] + delta_x\n","        adjusted_keypoint_y = keypoint[1] + delta_y\n","        adjusted_keypoints.append((adjusted_keypoint_x, adjusted_keypoint_y))\n","        \n","    adjusted_keypoints = np.clip(adjusted_keypoints, 0, 64)\n","    normalized_keypoints = adjusted_keypoints / 64\n","    return normalized_keypoints\n","\n","print('슝=3')"],"metadata":{"id":"MMlDppRTB9Sl","executionInfo":{"status":"aborted","timestamp":1650725118280,"user_tz":-540,"elapsed":7,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["모델과 이미지 경로를 입력하면 이미지와 keypoint를 출력하는 함수를 만들어준다."],"metadata":{"id":"_1IJZIEQB_WN"}},{"cell_type":"code","source":["def predict(model, image_path):\n","    encoded = tf.io.read_file(image_path)\n","    image = tf.io.decode_jpeg(encoded)\n","    inputs = tf.image.resize(image, (256, 256))\n","    inputs = tf.cast(inputs, tf.float32) / 127.5 - 1\n","    inputs = tf.expand_dims(inputs, 0)\n","    outputs = model(inputs, training=False)\n","    if type(outputs) != list:\n","        outputs = [outputs]\n","    heatmap = tf.squeeze(outputs[-1], axis=0).numpy()\n","    kp = extract_keypoints_from_heatmap(heatmap)\n","    return image, kp\n","\n","print('슝=3')"],"metadata":{"id":"GFl7fcV8CEMn","executionInfo":{"status":"aborted","timestamp":1650725118280,"user_tz":-540,"elapsed":7,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이제 그림만 그려주면 완성이다. 그림을 그릴 때는 두 가지 그림을 그릴 것이다. keypoint들과 뼈대이다.   \n","keypoint는 관절 역할, 그리고 이 keypoint들을 연결시킨 것이 뼈대가 될것이다."],"metadata":{"id":"gB9J4jNaCEd-"}},{"cell_type":"code","source":["def draw_keypoints_on_image(image, keypoints, index=None):\n","    fig,ax = plt.subplots(1)\n","    ax.imshow(image)\n","    joints = []\n","    for i, joint in enumerate(keypoints):\n","        joint_x = joint[0] * image.shape[1]\n","        joint_y = joint[1] * image.shape[0]\n","        if index is not None and index != i:\n","            continue\n","        plt.scatter(joint_x, joint_y, s=10, c='red', marker='o')\n","    plt.show()\n","\n","def draw_skeleton_on_image(image, keypoints, index=None):\n","    fig,ax = plt.subplots(1)\n","    ax.imshow(image)\n","    joints = []\n","    for i, joint in enumerate(keypoints):\n","        joint_x = joint[0] * image.shape[1]\n","        joint_y = joint[1] * image.shape[0]\n","        joints.append((joint_x, joint_y))\n","    \n","    for bone in MPII_BONES:\n","        joint_1 = joints[bone[0]]\n","        joint_2 = joints[bone[1]]\n","        plt.plot([joint_1[0], joint_2[0]], [joint_1[1], joint_2[1]], linewidth=5, alpha=0.7)\n","    plt.show()\n","\n","print('슝=3')"],"metadata":{"id":"uauBiEo8CO4W","executionInfo":{"status":"aborted","timestamp":1650725118281,"user_tz":-540,"elapsed":7,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["테스트하여 모델의 성능을 확인해보자."],"metadata":{"id":"7-h7p6J9CQMV"}},{"cell_type":"code","source":["test_image = os.path.join(PROJECT_PATH, 'test_image.jpg')\n","\n","image, keypoints = predict(model, test_image)\n","draw_keypoints_on_image(image, keypoints)\n","draw_skeleton_on_image(image, keypoints)"],"metadata":{"id":"YfXU6RcGCScn","executionInfo":{"status":"aborted","timestamp":1650725118281,"user_tz":-540,"elapsed":7,"user":{"displayName":"노현호","userId":"15410213599666758892"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 16-9. Project: 모델 바꿔보기"],"metadata":{"id":"_pf0s4DjqKnX"}},{"cell_type":"markdown","source":["지금까지 StackedHourglass Network를 기반으로 학습을 진행해왔다. 하지만 이전 챕터에서 살펴본 것처럼 simplebaseline 모델이 이보다 훨씬 간단한 구조임에도 더욱 좋은 성능을 보여주었다.   \n","실제로 그런 성능을 얻을 수 있는지 확인해보자.\n","\n","1. simplebaseline 모델 완성하기\n","`simplebaseline.py`파일과 이전 챕터 simplebaseline 내용을 참고하여 모델을 완성해보자.\n","2. simplebaseline 모델로 변경하여 훈련하기\n","stackedhourglass를 학습시킨 코드의 모델 선언 부분을 simplebaseline 모델로 변경한 후 학습을 다시 진행해보자.\n","3. 두 모델의 비교\n","두 모델 동일한 Epoch 수 만큼 학습하여 Pose Estimation 결과와 학습 진행 결과를 비교해보자.   \n","가급적 두 모델 모두 3epoch 이상(5epoch 이상 권장) 학습하자."],"metadata":{"id":"3BYP0qXOCdy8"}}]}