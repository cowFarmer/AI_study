{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0bf9e61",
   "metadata": {},
   "source": [
    "# 27. Likelihood(MLE와 MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09681df5",
   "metadata": {},
   "source": [
    "데이터의 집합이 있고, 데이터가 따르는 확률 분포를 통해 적절한 입력과 적절한 출력값을 추정해 머신러닝 데이터를 추출한다. 하지만 데이터셋의 크기는 유한하여 데이터가 따르는 정확한 확률 분포를 구하는 것은 불가능하다. 그래서 파라미터에 의해 결정되는 모델을 만든 다음, 파라미터 값을 조절하여 데이터의 분포를 간접적으로 표현한다. 즉 머신러닝의 목표를 모델이 표현하는 __확률 분포를 데이터의 실제 분포에 가깝게 최적의 파라미터 값을 찾는 것__ 이라고 할 수 있다.   \n",
    "해당 내용은 [Mathematics for Machine Learning](https://mml-book.github.io/)을 참고했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82d5b54",
   "metadata": {},
   "source": [
    "# 27-2. 확률 변수로서의 모델 파라미터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d4b642",
   "metadata": {},
   "source": [
    "간단한 일차함수 모델을 생각해보자. $y=f(x)=ax+b$ 위 식에서 $a, b$는 $f$라는 함수로 표현되는 모델의 형태를 결정하는 파라미터로 작용한다. 그리고 $(a, b)$가 위치하는 $R^2$ 공간을 파라미터 공간이라고한다.   \n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/math02_1-1-edited.max-800x600.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6b6f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "parameter_points = []\n",
    "fig1, axes1 = plt.subplots(2, 5, figsize=(10, 4))\n",
    "\n",
    "for ax in axes1.flatten():\n",
    "    # np.random.uniform: 정해진 구간에서 수를 무작위로 추출하여 반환합니다.\n",
    "    a, b = np.random.uniform(-10, 10, size=2)\n",
    "    a = round(a, 3)\n",
    "    b = round(b, 3)\n",
    "    parameter_points.append((a, b))\n",
    "\n",
    "    x = np.linspace(-10, 10, 50)\n",
    "    y = a*x + b\n",
    "    ax.plot(x, y)\n",
    "\n",
    "    ax.set_title('y='+str(a)+'x'+'{0:+.03f}'.format(b))\n",
    "    ax.set_xlim(-10, 10)\n",
    "    ax.set_ylim(-10, 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "px, py = np.split(np.array(parameter_points), 2, axis=1)\n",
    "fig2 = plt.figure()\n",
    "axes2 = plt.gca()\n",
    "\n",
    "axes2.set_title('samples from parameter space')\n",
    "axes2.set_xlim(-10, 10)\n",
    "axes2.set_ylim(-10, 10)\n",
    "\n",
    "plt.scatter(px, py)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aea02d6",
   "metadata": {},
   "source": [
    "$y = ax + b$의 그래프를 그리는 코드인데 간단한 시각화를 위해 (-10, 10) 구간에서 샘플링을 진행했다. 파라미터 $a, b$를 공간 $R^2$의 원소로 생각하면, 파라미터 공간에 주어진 확률 분포를 생각할 수 있다.   \n",
    "아래의 코드는 평균이 (1, 0)인 종규분포로 $a, b$의 값이 각각 1, 0에 가까울 확률이 크다고 본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cd9341",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_points = []\n",
    "fig, axes1 = plt.subplots(2, 5, figsize=(10, 4))\n",
    "\n",
    "for ax in axes1.flatten():\n",
    "    # np.random.normal: 정규분포를 따르는 확률 변수의 랜덤한 값을 반환합니다.\n",
    "    a, b = np.random.normal(loc=[1, 0], scale=0.5)\n",
    "    a = round(a, 3)\n",
    "    b = round(b, 3)\n",
    "    parameter_points.append((a, b))\n",
    "\n",
    "    x = np.linspace(-10, 10, 50)\n",
    "    y = a*x + b\n",
    "    ax.plot(x, y)\n",
    "\n",
    "    ax.set_title('y='+str(a)+'x'+'{0:+.03f}'.format(b))\n",
    "    ax.set_xlim(-10, 10)\n",
    "    ax.set_ylim(-10, 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "px, py = np.split(np.array(parameter_points), 2, axis=1)\n",
    "fig2 = plt.figure()\n",
    "axes2 = plt.gca()\n",
    "\n",
    "axes2.set_title('samples from parameter space')\n",
    "axes2.set_xlim(-10, 10)\n",
    "axes2.set_ylim(-10, 10)\n",
    "\n",
    "plt.scatter(px, py)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e499fc",
   "metadata": {},
   "source": [
    "# 27-3. posterior, prior, likelihood 사이의 관계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f64bdf9",
   "metadata": {},
   "source": [
    "베이지안 머신러닝 모델은 데이터를 통해 파라미터 공간의 확률 분포를 학습한다. 중요한 점은 파라미터를 고정된 값이 아닌 불확실성(uncertainty)을 가진 확률 변수로 본다.   \n",
    "\n",
    "__prior; 사전확률, likelihood; 가능도, posterior; 사후확률__   \n",
    "데이터의 집합 $X$가 주어졌을 때, 데이터가 따르는 확률 분포 $p(X)$를 잘 나타내는 일차함수 모델을 찾는것이 목표이다. 데이터를 관찰하기 전 파라미터 공간에 주어진 확률 분포 $p(\\theta)$를 __prior probability__ 라고 한다. 이는 일반적인 정규분포가 되거나, 데이터의 특성이 반영된 특정 확률 분포가 될 수도 있다.   \n",
    "\n",
    "prior 분포를 고정시킨다면 주어진 파라미터 분포에 대해 갖고 있는 데이터가 얼마나 __likelihood__ 를 계산할 수 있다. $p(X=_x|\\theta)$ 파라미터 분포 $p(\\theta)$가 정해졌을때 $x$ 데이터가 관찰될 확률을 말한다. 결국 데이터는 고정된 값이고 구하고 싶은건 모델의 파라미터 $\\theta$값이다. likelihood가 높으면 지정한 파라미터 조건에서 데이터가 관찰될 확률이 높다는 것이다. 이는 데이터의 분포를 모델이 잘 표현한다고 말할 수 있다.   \n",
    "\n",
    "likelihood 값을 최대화 하는 방향으로 모델을 학습시키는 방법을 __최대 가능도 추정(maximum likelihood estimation, MLE)__ 이라고 한다. 반대로 데이터의 집합이 주어졌을 때 분포를 구해야하면 이 값을 '데이터 관찰 후 계산되는 확률' 이라는 뜻에 __posterior__ 이라고 한다.   \n",
    "\n",
    "생각해보면 posterior가 더 의미 있는게, 데이터의 개수는 유한하기 때문에 데이터가 따르는 머신러닝의 목표가 $p(X)$를 직접 구할 수 없으니 모델 파라미터 $\\theta$를 조절해가며 간접적으로 근사치를 찾아야한다. 그래서 직접 최적의 파라미터 $\\theta$값을 찾는게 아닌 prior와 likelihood에 관한 식으로 변형하고, 그 식을 최대화하는 파라미터 값을 찾는 방식으로 한다.   \n",
    "이렇게 posterior를 최대화하는 방향으로 모델 학습 방법을 __최대 사후 확률 추정(maximum a posteriori estimation, MAP)__ 이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e815ec",
   "metadata": {},
   "source": [
    "# 27-4. likelihood와 머신러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f99df21",
   "metadata": {},
   "source": [
    "머신러닝 모델은 한정된 파라미터로 데이터의 실제 분포를 근사하게 만드는 역할을 하기 때문에, 100%의 정확도를 만들기는 불가능하다. 그래서 모델의 입력 데이터로부터 예측한 출력 데이터(prediction)와 실제 값(label) 사이에 오차가 생기는데, 우리는 관찰 데이터에 노이즈가 섞여있어서 이런 오차가 발생한다고 생각한다.   \n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/original_images/math02_2-1-edited.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff893ac",
   "metadata": {},
   "source": [
    "지도 학습(supervised learning)을 생각해보자. 데이터셋의 입력 $x$와 라벨 $y$가 짝지어진 형태다. likelihood는 파라미터 분포가 주어졌을 때 특정 데이터가 관찰될 확률을 나타낸다.   \n",
    "지도학습에서는 파라미터 분포 $\\theta$와 입력 데이터 $X_n$이 주어졌을 때 라벨 $y_n$을 예측하는 문제다. 입력 데이터의 집합을 $X$, 라벨들의 집합을 $Y$라고 할때, likelihood는 파라미터와 입력 데이터가 주어졌을 때 출력값(라벨)의 확률 분포, $p(Y|\\theta, X)$가 된다. 선형 모델의 출력값의 분포는 예측값에 노이즈 분포를 더한 값이 된다.   \n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/math02_2-2_Na6BA2w.max-800x600.png)\n",
    "$xy$ 평면 위에 모델에 해당하는 빨간색 직선이 있다. 출력값의 분포를 나타내기 위해 $p(y)$ 좌표축을 추가했다. 입력 데이터가 $X_n$일 때 모델의 예측값은 $\\theta^TX_n$이고, 출력값의 분포는 $p(y_n|\\theta, X_n)$으로 노란색으로 표시한 정규분포 그래프가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7af3176",
   "metadata": {},
   "source": [
    "# 27-5. likelihood 감잡기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec3730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(321)\n",
    "\n",
    "input_data = np.linspace(-2, 2, 5)\n",
    "label = input_data + 1 + np.random.normal(0, 1, size=5)\n",
    "\n",
    "plt.scatter(input_data, label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62ae86c",
   "metadata": {},
   "source": [
    "5개의 랜덤한 데이터 포인트를 생성하고 좌표평면 위에 표지해주는 코드이다. 아래의 코드는 일차함수 모델의 식을 바꾸면 데이터 포인트와 likelihood의 값이 바뀌는걸 볼 수 있다.   \n",
    "데이터 포인트 옆에 있는 숫자는 likelihood의 값이고, 직선은 모델이 표현하는 함수의 그래프이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e375c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: y = ax + b\n",
    "# a, b 값을 바꾸면서 실행해보세요\n",
    "#-------------------------------#\n",
    "a = 1\n",
    "b = 1\n",
    "#-------------------------------#\n",
    "\n",
    "# 모델 예측값\n",
    "model_output = a*input_data + b\n",
    "likelihood = []\n",
    "\n",
    "# x: 입력데이터, y: 데이터라벨\n",
    "# 예측값과 라벨의 차이를 제곱해 exp에 사용\n",
    "for x, y, output in zip(input_data, label, model_output):\n",
    "    likelihood.append(1/(math.sqrt(2*math.pi*0.1*0.1))*math.exp(-pow(y-output,2)/(2*0.1*0.1)))\n",
    "\n",
    "model_x = np.linspace(-2, 2, 50)\n",
    "model_y = a*model_x + b\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(input_data, label)\n",
    "ax.plot(model_x, model_y)\n",
    "\n",
    "for i, text in enumerate(likelihood):\n",
    "    ax.annotate('%.3e'%text, (input_data[i], label[i]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da935ea9",
   "metadata": {},
   "source": [
    "likelihood가 중요한 이유는 데이터 포인트가 모델 함수에서 멀어질수록 likelihood가 기하급수적으로 감소하는걸 볼 수 있다. likelihood를 구하는 식이 모델 예측값과 데이터 라벨의 차이를 제곱해서 exponential 위에 올려 놓은것이다. 그래서 이 차이가 조금만 벌어져도 민감하게 반응한다.   \n",
    "머신러닝의 목표가 데이터들의 포인트를 잘 표현하는 모델을 찾는 것을 생각하면, 데이터 포인터들의 likelihood 값을 크게 하는 모델을 찾는것이 된다. 그래서 이 값을 최대하는 모델 파라미터를 찾는 방법이 __최대 가능도 추론(MLE)__ 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be746b11",
   "metadata": {},
   "source": [
    "# 27-6. MLE: 최대 가능도 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffca13d",
   "metadata": {},
   "source": [
    "좋은 머신러닝 모델은 데이터셋에 있는 모든 데이터 포인트의 likelihood 값을 크게 만드는 모델이다. 어떻게 구하면 좋을까?   \n",
    "이 전에 다룬 데이터셋의 입력과 라벨값이 짝지어져 있는 상태, 즉 데이터 포인트는 서로 독립이고 같은 확률 분포를 따른다고 가정해보자. 이를 독립 데이터 데이터포인트(independent and identically distributed), i.i.d라고 부르고 머신러닝 문제에 꼭 필요한 전제조건이다. 데이터 포인트들이 독립이므로, 데이터셋 전체의 likelihood는 데이터 포인트 각각의 likelihood를 모두 곱한 값과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d3bca",
   "metadata": {},
   "source": [
    "MLE를 실제로 적용할 때 likelihood 대신 log likelihood를 최대화하는 파라미터를 구한다. 데이터셋의 likelihood가 데이터 포인트 각각의 likelihood를 곱한 형태인데, 로그를 씌우면 곱셈 연산이 덧셈 연산으로 바뀌면서 미분 계산이 편리해지는 장점이 있다.   \n",
    "\n",
    "로그 함수는 단조 증가(monotonically increasing)하므로 likelihood를 최대화하는 파라미터와 log likelihood를 최대화하는 파라미터 값이 같아 학습 결과에 영향을 주지 않는다.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47bbb31",
   "metadata": {},
   "source": [
    "# 27-7. MLE 최적해 구하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cffea4e",
   "metadata": {},
   "source": [
    "MLE를 이용해 최적의 파라미터를 찾아보고 데이터셋의 likelihood도 계산해보자. $y = x + 1$ 함수를 기준으로 랜덤한 노이즈를 섞어 데이터 포인트 20개를 생성하고 시각화하는 코드이다. 노이즈의 분포는 평균이 0이고 표준편차가 0.5인 정규분포이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38bee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "num_samples = 20\n",
    "\n",
    "input_data = np.linspace(-2, 2, num_samples)\n",
    "labels = input_data + 1 + np.random.normal(0, 0.5, size=num_samples)\n",
    "\n",
    "plt.scatter(input_data, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6bdecf",
   "metadata": {},
   "source": [
    "데이터를 생성할 때는 노이즈 분포의 표준편차를 정했지만 데이터를 관찰하고 모델을 설계하는 입장에서는 노이즈의 원래 표준편차를 알 수 없다고한다. 구현에서 주의해야 할 부분은 입력 데이터가 변수 $x$와 상수항이 묶인 2차원 열벡터로 표현된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e3235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(labels, preds):\n",
    "    result = 1/(np.sqrt(2*math.pi*0.1*0.1))*np.exp(-np.power(labels-preds,2)/(2*0.1*0.1))\n",
    "    \n",
    "    return np.prod(result)\n",
    "\n",
    "def neg_log_likelihood(labels, preds):\n",
    "    const_term = len(labels)*math.log(1/math.sqrt(2*math.pi*0.1*0.1))\n",
    "\n",
    "    return (-1)*(const_term + 1/(2*0.1*0.1)*np.sum(-np.power(labels-preds,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0afdee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: 20x2 matrix, y: 20x1 matrix\n",
    "# input_data 리스트를 column vector로 바꾼 다음 np.append 함수로 상수항을 추가합니다.\n",
    "X = np.append(input_data.reshape((-1, 1)), np.ones((num_samples, 1)), axis=1)\n",
    "y = labels\n",
    "\n",
    "theta_1, theta_0 = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)\n",
    "\n",
    "print('slope: '+'%.4f'%theta_1+' bias: '+'%.4f'%theta_0)\n",
    "\n",
    "predictions = theta_1 * input_data + theta_0\n",
    "print('likelihood: '+'%.4e'%likelihood(labels, predictions))\n",
    "print('negative log likelihood: '+'%.4e'%neg_log_likelihood(labels, predictions))\n",
    "\n",
    "model_x = np.linspace(-2, 2, 50)\n",
    "model_y = theta_1 * model_x + theta_0\n",
    "\n",
    "plt.scatter(input_data, labels)\n",
    "plt.plot(model_x, model_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36f88d5",
   "metadata": {},
   "source": [
    "코드로 구한 최적의 모델은 $y=0.8578x+1.2847$ 이 됐다. 데이터 포인트들이 $y=x+1$ 함수로 생성된 것을 감안하면 꽤 가까운 결과가 나왔다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055eabfd",
   "metadata": {},
   "source": [
    "# 27-8. MAP: 최대 사후 확률 추정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e896d",
   "metadata": {},
   "source": [
    "선형 회귀 문제에서 MLE로 구한 최적 파라미터 식을 살펴보자. $\\theta_{ML}=(X^TX)^{-1}X^Ty$ 이를 살펴보면 $X$는 데이터셋 행렬이고, $y$는 라벨들을 모아놓은 백터다. 이러한 접근법은 계산이 비교적 간단하다는 장점이 있지만, __MLE의 최적해는 관측된 데이터 값에만 의존__ 하고, 데이터에 노이즈가 많이 섞여 있는 경우, 이상치 데이터가 존재하는 경우에는 모델의 안정성이 떨어진다는 단점이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37562911",
   "metadata": {},
   "source": [
    "머신러닝 모델의 최적 파라미터를 찾는 방법으로 likelihood를 최대화하는 MLE를 찾는 것 이었고, 또 하나는 __최대 사후 확률 추정(maximum a posteriori estimation, MAP)__ 이 있다. MAP는 데이터셋이 주어졌을 때 파라미터의 분포에서 확률 값을 최대화하는 파라미터 $\\theta$를 찾는다. MLE에서 negative log likelihood를 최소화했던 것과 같이, MAP에서도 posterior를 최대화하는 파라미터 대신 negative log posterior를 최소화하는 파라미터 값을 구한다.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c8297b",
   "metadata": {},
   "source": [
    "# 27-9. MLE와 MAP의 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea5f101",
   "metadata": {},
   "source": [
    "MAP는 MLE와 비슷하지만 정규화 항에 해당하는 negative log prior 부분이 존재 유무의 차이가 있다. 그래서 MLE 보다 MAP 모델이 더 안정적이다. 이상치가 있는 데이터셋을 이용해서 이를 비교해보자. $y = x + 1$ 함수값에 랜덤한 노이즈를 더해 데이터 포인트를 생성하고 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4438e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "num_samples = 10\n",
    "\n",
    "input_data = np.linspace(-2, 2, num_samples)\n",
    "labels = input_data + 1 + np.random.normal(0, 0.5, size=num_samples)\n",
    "\n",
    "input_data = np.append(input_data, [0.5, 1.5])\n",
    "labels = np.append(labels, [9.0, 10.0])\n",
    "\n",
    "plt.scatter(input_data, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2901d8",
   "metadata": {},
   "source": [
    "$\\theta_{MLE} = (X^T X)^{-1}X^Ty$   \n",
    "$\\theta_{MAP} = (X^T X + \\frac{\\sigma^2}{\\alpha^2}I)^{-1} X^Ty$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621cfc79",
   "metadata": {},
   "source": [
    "노이즈 분포의 표준편차 $\\sigma$는 0.1로 가정하고, 파라미터 분포의 표준편차 $\\alpha$는 0.04로 지정했다. 정규화 상수 $\\lambda$가 $\\alpha^2$에 반비례하는 값이다. $\\alpha$가 작을수록, 즉 파라미터 분포의 표준편차를 작게 잡을수록 파라미터 값에 대한 제약 조건을 강하게 걸어주는 것 과 같다. 정규화 측면에서도 $\\lambda$값이 클수록 모델의 유연성은 감소한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a4284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(labels, preds):\n",
    "    result = 1/(np.sqrt(2*math.pi*0.1*0.1))*np.exp(-np.power(labels-preds,2)/(2*0.1*0.1))\n",
    "    \n",
    "    return np.prod(result)\n",
    "\n",
    "def neg_log_likelihood(labels, preds):\n",
    "    const_term = len(labels)*math.log(1/math.sqrt(2*math.pi*0.1*0.1))\n",
    "\n",
    "    return (-1)*(const_term + 1/(2*0.1*0.1)*np.sum(-np.power(labels-preds,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b556c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: 21x2 matrix, y: 21x1 matrix\n",
    "# input_data 리스트를 column vector로 바꾼 다음 np.append 함수로 상수항을 추가합니다.\n",
    "X = np.append(input_data.reshape((-1, 1)), np.ones((num_samples+2, 1)), axis=1)\n",
    "y = labels\n",
    "\n",
    "# MLE 파라미터 계산식\n",
    "mle_theta_1, mle_theta_0 = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)\n",
    "# MAP 파라미터 계산식\n",
    "map_theta_1, map_theta_0 = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)+(0.1*0.1)/(0.04*0.04)*np.eye(2)), X.T), y)\n",
    "\n",
    "print('[MLE result] (blue)')\n",
    "print('slope: '+'%.4f'%mle_theta_1+' bias: '+'%.4f'%mle_theta_0)\n",
    "\n",
    "mle_preds = mle_theta_1 * input_data + mle_theta_0\n",
    "print('likelihood: '+'%.4e'%likelihood(labels, mle_preds))\n",
    "print('negative log likelihood: '+'%.4e\\n'%neg_log_likelihood(labels, mle_preds))\n",
    "\n",
    "print('[MAP result] (orange)')\n",
    "print('slope: '+'%.4f'%map_theta_1+' bias: '+'%.4f'%map_theta_0)\n",
    "\n",
    "map_preds = map_theta_1 * input_data + map_theta_0\n",
    "print('likelihood: '+'%.4e'%likelihood(labels, map_preds))\n",
    "print('negative log likelihood: '+'%.4e'%neg_log_likelihood(labels, map_preds))\n",
    "\n",
    "model_x = np.linspace(-2, 2, 50)\n",
    "mle_model_y = mle_theta_1 * model_x + mle_theta_0\n",
    "map_model_y = map_theta_1 * model_x + map_theta_0\n",
    "\n",
    "plt.scatter(input_data, labels)\n",
    "plt.plot(model_x, mle_model_y)\n",
    "plt.plot(model_x, map_model_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ce85fa",
   "metadata": {},
   "source": [
    "결과물의 파란색 직선은 MLE, 주황색 직선은 MAP를 이용해 찾은 모델이다. 파란색 직선은 이상치 데이터까지 포함한 negative log likelihood를 감소시키기 위해 직선이 위로 치우쳐 아래쪽 데이터 범위의 경향성에서 벗어났다. 하지만 주황색은 데이터가 추가되어도 크게 벗어나지 않고 있다.   \n",
    "\n",
    "데이터 분포에서 멀리 떨어진 이상치가 추가되었기 때문에, likelihood의 값은 언더플로우가 발생해서 0으로 표시된다. negative log likelihood의 값으로 MLE 결과와 MAP 결과를 비교했다. MAP가 MLE에 비해 이상치 데이터가 추가되었을 때 모델 파라미터의 변화는 MLE보다 작다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6096161",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
