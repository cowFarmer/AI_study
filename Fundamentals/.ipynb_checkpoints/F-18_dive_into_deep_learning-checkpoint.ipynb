{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da65474b",
   "metadata": {},
   "source": [
    "# Numpy를 이용해서 딥러닝의 간단한 신경망과 훈련 과정되짚어보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea0c783",
   "metadata": {},
   "source": [
    "## MNIST 데이터셋 되짚어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99079d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MNIST 데이터 로드 혹은 다운로드\n",
    "mnist = keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()   \n",
    "\n",
    "# 데이터 가공\n",
    "x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0\n",
    "x_train_reshaped = x_train_norm.reshape(-1, x_train_norm.shape[1]*x_train_norm.shape[2])\n",
    "x_test_reshaped = x_test_norm.reshape(-1, x_test_norm.shape[1]*x_test_norm.shape[2])\n",
    "\n",
    "# 딥러닝 모델 구성 2개의 다층 퍼셉트론(MLP)으로 이루어져 있음\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(50, activation='sigmoid', input_shape=(784,)))  # 입력층 d=784, 은닉층 레이어 H=50\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))   # 출력층 레이어 K=10\n",
    "model.summary()\n",
    "\n",
    "# 모델 구성, 학습\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.fit(x_train_reshaped, y_train, epochs=10)\n",
    "\n",
    "# 모델 테스트 결과\n",
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped,y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2662504c",
   "metadata": {},
   "source": [
    "### 다층 퍼셉트론(Multi-Layer Perceptron; MLP)\n",
    "![](https://ichi.pro/assets/images/max/724/0*ZRqDRiz_jGI76NJv.png)\n",
    "\n",
    "입력층은 $x_d$ , 은닉층은 $Z_H$ , 출력층은 $S_K$로 구성되어 있으며   \n",
    "입력-은닉 가중치는 $W^d*H$, 은닉-출력 가중치는 $W^H*K$로 되어있다.   \n",
    "위의 모델은 입력층 노드`d=784`개, 은닉층 `H=50`개, 출력층 `K=10`개로 되어있다.   \n",
    "\n",
    "입력층 -> 은닉층, 은닉층 -> 출력층 사이에 행렬(Matrix)가 존재한다.   \n",
    "위 모델은 입력층-은닉층 사이에 `784x50`, 은닉층-출력층 사이에 `50x10`의 행렬이 존재한다.   \n",
    "이 행렬은 파라미터, Weight라고 부른다 그리고 이런 식의 관계가 성립한다. $y = W * X + b$   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c27d5d5",
   "metadata": {},
   "source": [
    "## 활성화 함수, 손실 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062ec64c",
   "metadata": {},
   "source": [
    "### 활성화 함수(Activation Functions)\n",
    "딥러닝에서 활성화 함수가 필수적이다. 보통 비선형 함수에서 활성화 함수를 사용하는데 MLP에서 사용하면 모델의 표현력이 좋아진다. 자주 쓰이는 활성화 함수를 알아보자\n",
    "1. Sigmoid\n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/f-14-2.max-800x600.png)\n",
    "위 MLP의 은닉층에 활성화 함수로 사용했다.   \n",
    "`model.add(keras.layers.Dense(50, activation='sigmoid', input_shape=(784,)))`   \n",
    "vanishing gradient 문제 발생, exp 함수 사용시 비용이 큼   \n",
    "\n",
    "2. Tanh\n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/f-14-3.max-800x600.png)   \n",
    "함수의 중심을 0으로 옮겨 sigmoid의 느린 최적화 문제를 해결, vanishing gradient 문제 발생   \n",
    "\n",
    "3. ReLU\n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/f-14-4.max-800x600.png)   \n",
    "위의 Sigmoid나 Tanh보다 학습이 빠름, 연산 비용이 크지 않고 구현이 간단한 장점 갖고 있어서 요즘 많이 사용함   \n",
    "### 손실 함수(Loss Function)\n",
    "비선형 활성화 함수를 갖고 여러 개의 은닉층을 거친 다음 신호 정보들을 출력층으로 전달한다. 이때 우리가 원하는 정답과 전달된 신호 정보들의 차이를 계산하고, 차이를 줄이기 위해 파라미터들을 조정하는 것이 딥러닝의 학습 흐름이다.   \n",
    "이 차이를 구할때 사용되는 함수가 손실함수(Loss function), 비용함수(Cost function)이라고 한다.\n",
    "- 평균제곱오차(MSE: Mean Square Error)   \n",
    "![](https://user-images.githubusercontent.com/10937193/58108494-e7bc7580-7c26-11e9-90a1-b988522a0b64.png)\n",
    "- 교차 엔트로피(Cross Entropy)   \n",
    "두 확률분포 사이의 유사도가 클수록 작아진다. \n",
    "![](https://mblogthumb-phinf.pstatic.net/MjAxODA2MTVfMjYg/MDAxNTI5MDQ4Mzc3MzI2.r0u34AMec-6ZLnAIxbfD-G0cfTfOz2cja8TTfBkUUbEg.oiIeDj9lu6Mtp780pgiNZDBGYn5thLSq1vpQPDpZshog.PNG.ssdyka/e_4.2.png?type=w2)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a179bca",
   "metadata": {},
   "source": [
    "## 경사하강법(Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e0a6eb",
   "metadata": {},
   "source": [
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/f-14v3-3-1.max-800x600.png)   \n",
    "경사하강법은 손실 함수의 값이 최소가 되게끔 찾아주는 역할을 한다. 즉 손실 함수를 통해 구한 오차를 갖고 각 파라미터를 조정하는 것이 경사하강법이다.   \n",
    "학습률(learning rate)라는 개념을 도입해서 기울기 값과 학습률을 곱한 만큼 하강한다. [learning rate 관련 글](https://aileen93.tistory.com/71)   \n",
    "딥러닝 학습에 있어서 초기 가중치(파라미터) 설정은 엄청 중요하다 [가중치 관련 글](https://reniew.github.io/13/)   \n",
    "파라미너 W의 변화에 맞춰 오차(Loss) L의 변화량을 구해야한다. 그리고 오차 기울기가 커지는 반대 방향으로 파라미터를 조정해주는데 적절한 step size 역할을 하는 learning rate가 필수적이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fb9253",
   "metadata": {},
   "source": [
    "## 오차역전파법(Backpropagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05c3e30",
   "metadata": {},
   "source": [
    "경사하강법의 기울기를 갖고 어떻게 입력층까지 전달할까? 이 때 오차역전파법 개념이 사용된다.   \n",
    "![](https://www.researchgate.net/profile/Rozaida_Ghazali/publication/234005707/figure/fig2/AS:667830315917314@1536234563135/The-structure-of-single-hidden-layer-MLP-with-Backpropagation-algorithm.png)\n",
    "MLP를 학습시키기 위한 알고리즘중 하나로 Output 결과와 내가 원하는 target값의 차이를 구한 뒤 그 오차 값을 각 레이어들을 되돌아가면서 역전파하고 기존에 갖고 있는 변수들을 갱신해 나간다.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ba4918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
