{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f7869a",
   "metadata": {},
   "source": [
    "__딥러닝 모델을 구성하는 레이어 특히 Linear, Convolution의 동작 방식에 대해서 알아보자.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141dce68",
   "metadata": {},
   "source": [
    "딥러닝을 공부할 때 통계나 수학 그리고 인공 신경망 네트워크(ANN)을 설계하면서 감을 익힌다. 하지만 수학과 설계 사이의 미싱 링크를 느낄 수 있었을것이다. 예를들어 $y=Wx+b$에서 최적의 $W$(weight)와 $b$를 찾는 과정이다.   \n",
    "\n",
    "딥러닝의 수많은 신경망들은 각각 다른 Weight와 독특한 특성을 갖고 있다. 데이터에서 원하는 특징을 효과적으로 추출하기 위해 올바른 Weight를 정의하는 과정이 중요하다. 이 과정은 신경망을 분석하거나 설계하는 데에 큰 도움이 될 수 있으므로 잘 알아두자.   \n",
    "\n",
    "이번 노드에서는 `Linear`와 `Convolution`레이어에 대해 집중적으로 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ac0f43",
   "metadata": {},
   "source": [
    "# 1. 데이터의 형태"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672fc50d",
   "metadata": {},
   "source": [
    "데이터의 형태 변화를 보고 이해해보자. 예를들어 10개의 단어로 이루어진 문장을 5개의 단어로 요약했으면 정보를 집약시킨것이고, 20개의 단어로 확장했다면 정보를 세밀하게 표현했음을 짐작할 수 있다.   \n",
    "\n",
    "이미지 데이터는 채널이라는 데이터를 갖고 있다. 많이 쓰는 RGB 이미지를 보면 R, G, B로 나눠져있다. 그래서 매트릭스로 (Channel, Width, Height) 이렇게 표현한다.   \n",
    "\n",
    "해상도가 1280x1024(30fps) 러닝타임(T)가 90분인 흑백 영화는 어떻게 표기할까?   \n",
    "(T, C, W, H)로 봤을때 (90x60x30=162000, 1, 1280, 1024)로 표기한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a14419a",
   "metadata": {},
   "source": [
    "레이어란?\n",
    "> 하나의 물체가 여러 개의 논리적인 객체들로 구성되어 있는 경우, 각각의 객체를 하나의 레이어라 한다.   \n",
    "\n",
    "신경망을 구성하는 여러 개의 레이어들을 이해하면 신경망을 이해하는 것과 같다. 그리고 이러한 레이어에 Weight가 있어서 어떠한 물체를 인식할 때 신경망을 써서 인식한다.   \n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/F-24-5.max-800x600.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aadb943",
   "metadata": {},
   "source": [
    "# 2. Linear 레이어"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789d87df",
   "metadata": {},
   "source": [
    "선형대수학에서 쓰이는 용어 중에 선형 변환(Linear Transform)이 있는데 이것과 동일한 기능을 하는 레이어이다. 관련 영상은 [Linear transformations and matrices | Chapter 3, Essence of linear algebra\n",
    "](https://youtu.be/kYB8IZa5AuE) 여기서 자세하게 설명해준다.   \n",
    "\n",
    "Linear 레이어는 선형 변환을 활용해서 데이터를 특정 차원으로 변환하는 기능을 한다고 한다. 위의 영상에서는 i hat과 j hat으로 표현한다. 100차원의 데이터를 300차원으로 변환하면 데이터를 풍부하게 표현하는 효과가 있고, 10차원의 데이터로 변환하면 집약시키는 효과가 있다.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3722c3",
   "metadata": {},
   "source": [
    "\n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/F-24-6.max-800x600.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c1d073",
   "metadata": {},
   "source": [
    "위의 사각형 그림을 보면 모두 2차원에 점 4개로 표현 가능하므로 (4, 2) 행렬 형태의 데이터로 표현할 수 있다. 두 사각형을 각각 하나의 정수로 표현하고자 할 때 어떻게 집약시킬지 구상해보자.   \n",
    "> 1. (4,2)x(2,1) 행렬= (4,)\n",
    "2. (4,)x(4,1) 행렬= (1,)\n",
    "\n",
    "2차원을 1차원으로 변환하는데 (2,1) 행렬이 선언되고, 4차원을 1차원으로 변환하는데 (4,1)행렬이 선언 되었다. 각각의 행렬들이 Weight로 Linear 레이어는 (입력의 차원, 출력의 차원)의 Wieght를 가지는 특성이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c7a8be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1단계 연산 준비: (64, 4, 2)\n",
      "1단계 연산 결과: (64, 4)\n",
      "1단계 Linear Layer의 Weight 형태: (2, 1)\n",
      "\n",
      "2단계 연산 준비: (64, 4)\n",
      "2단계 연산 결과: (64,)\n",
      "2단계 Linear Layer의 Weight 형태: (4, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 64\n",
    "boxes = tf.zeros((batch_size, 4, 2))     # Tensorflow는 Batch를 기반으로 동작하기에,\n",
    "                                         # 우리는 사각형 2개 세트를 batch_size개만큼\n",
    "                                         # 만든 후 처리를 하게 됩니다.\n",
    "print(\"1단계 연산 준비:\", boxes.shape)\n",
    "\n",
    "first_linear = tf.keras.layers.Dense(units=1, use_bias=False) \n",
    "# units은 출력 차원 수를 의미합니다.\n",
    "# Weight 행렬 속 실수를 인간의 뇌 속 하나의 뉴런 '유닛' 취급을 하는 거죠!\n",
    "\n",
    "first_out = first_linear(boxes)\n",
    "first_out = tf.squeeze(first_out, axis=-1) # (4, 1)을 (4,)로 변환해줍니다.\n",
    "                                           # (불필요한 차원 축소)\n",
    "\n",
    "print(\"1단계 연산 결과:\", first_out.shape)\n",
    "print(\"1단계 Linear Layer의 Weight 형태:\", first_linear.weights[0].shape)\n",
    "\n",
    "print(\"\\n2단계 연산 준비:\", first_out.shape)\n",
    "\n",
    "second_linear = tf.keras.layers.Dense(units=1, use_bias=False)\n",
    "second_out = second_linear(first_out)\n",
    "second_out = tf.squeeze(second_out, axis=-1)\n",
    "\n",
    "print(\"2단계 연산 결과:\", second_out.shape)\n",
    "print(\"2단계 Linear Layer의 Weight 형태:\", second_linear.weights[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8234dbc7",
   "metadata": {},
   "source": [
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/F-24-7.max-800x600.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d9e762",
   "metadata": {},
   "source": [
    "하지만 사각형 각각 1차원으로 바뀌는 (2,1) Weight를 줬을 때 결과값이 같아진다. 여기서 Weight의 모든 요소를 Parameter라고 한다. 이렇게 데이터를 집약했을 때 데이터 손실이 일어날 수 있음을 볼 수 있다. 그래서 이번엔 데이터를 풍부하게 만들어보자.   \n",
    "> 1. (4,2)x(2x3) 행렬 = (4,3)\n",
    "2. (4,3)x(3x1) 행렬 = (4,)\n",
    "3. (4,)x(4x1) 행렬 = (1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89528b89",
   "metadata": {},
   "source": [
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/F-24-8.max-800x600.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a528fa08",
   "metadata": {},
   "source": [
    "1에서 2x3=6개, 2에서 3x1=3개, 3에서 4x1=4개 총 13개의 파라미터가 Weight 가중치로 쓰였다. 이를 `tf.keras.layers.Layer.count_params()` 함수로 구현하면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f7033ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1단계 연산 준비: (64, 4, 2)\n",
      "1단계 연산 결과: (64, 4, 3)\n",
      "1단계 Linear Layer의 Weight 형태: (2, 3)\n",
      "\n",
      "2단계 연산 준비: (64, 4, 3)\n",
      "2단계 연산 결과: (64, 4)\n",
      "2단계 Linear Layer의 Weight 형태: (3, 1)\n",
      "\n",
      "3단계 연산 준비: (64, 4)\n",
      "3단계 연산 결과: (64,)\n",
      "3단계 Linear Layer의 Weight 형태: (4, 1)\n",
      "총 Parameters: 13\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 64\n",
    "boxes = tf.zeros((batch_size, 4, 2))\n",
    "\n",
    "print(\"1단계 연산 준비:\", boxes.shape)\n",
    "\n",
    "first_linear = tf.keras.layers.Dense(units=3, use_bias=False)\n",
    "first_out = first_linear(boxes)\n",
    "\n",
    "print(\"1단계 연산 결과:\", first_out.shape)\n",
    "print(\"1단계 Linear Layer의 Weight 형태:\", first_linear.weights[0].shape)\n",
    "\n",
    "print(\"\\n2단계 연산 준비:\", first_out.shape)\n",
    "\n",
    "second_linear = tf.keras.layers.Dense(units=1, use_bias=False)\n",
    "second_out = second_linear(first_out)\n",
    "second_out = tf.squeeze(second_out, axis=-1)\n",
    "\n",
    "print(\"2단계 연산 결과:\", second_out.shape)\n",
    "print(\"2단계 Linear Layer의 Weight 형태:\", second_linear.weights[0].shape)\n",
    "\n",
    "print(\"\\n3단계 연산 준비:\", second_out.shape)\n",
    "\n",
    "third_linear = tf.keras.layers.Dense(units=1, use_bias=False)\n",
    "third_out = third_linear(second_out)\n",
    "third_out = tf.squeeze(third_out, axis=-1)\n",
    "\n",
    "print(\"3단계 연산 결과:\", third_out.shape)\n",
    "print(\"3단계 Linear Layer의 Weight 형태:\", third_linear.weights[0].shape)\n",
    "\n",
    "total_params = \\\n",
    "first_linear.count_params() + \\\n",
    "second_linear.count_params() + \\\n",
    "third_linear.count_params()\n",
    "\n",
    "print(\"총 Parameters:\", total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2490fc4",
   "metadata": {},
   "source": [
    "그러면 Parameter가 많은게 좋다고 생각할 수도있는데 이는 Overfitting을 가져올 수 있다. 그래서 Weight의 형태만 선언해주고 데이터가 가진 분포에 따라 적합한 Weight를 찾아가는 과정을 훈련(training)이라고 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee9cfd5",
   "metadata": {},
   "source": [
    "__편향(Bias)__   \n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/F-24_bias_graph.max-800x600.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e2a854",
   "metadata": {},
   "source": [
    "위 그림을 보면 두 데이터가 비슷한 형태로 분포되어 있지만 파라미터의 변화(선형변환)로 둘을 일치시키기 어려워 보인다. 원점을 평행이동하는 것으로 해결할 수 있는데 선형변환된 값에 편향 파라미터 b를 더해주는 것으로 표현한다. $y=Wx+b$에 있는 $b$가 편향 값이다.   \n",
    "코드 예제에 있는 Dense 클래스의 `use_bias`를 True로 바꾸면 실험해볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afab515c",
   "metadata": {},
   "source": [
    "# 3. 정보 집약의 Convolution 레이어"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b8fa83",
   "metadata": {},
   "source": [
    "(1920x1080x3) 이미지를 살펴보려면 다음과 같은 과정을 거쳐야한다.   \n",
    "(1920x1080x3)->(6220800) x [6220800 x 1 Weight] = (1,) 아무리 적어도 620만개의 파라미터가 생성된다. 그리고 레이어들을 펼쳐 픽셀로 만들고 한줄씩 살펴야 해서 비효율적이다. 그래서 Convolution 레이어가 고안됐다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e004c7",
   "metadata": {},
   "source": [
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/F-24_conv.max-800x600.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b58cc97",
   "metadata": {},
   "source": [
    "위는 3x3 사이즈의 필터 K를 선언하고 슬라이딩하며 각 픽셀을 곱하며 더하는 Convolution 연산을 표현하고 있다. 몇 칸씩 옮기면서 보는지 설정하는 값은 Stride이다. 그리고 입력 데이터의 테두리에 0을 추가하는 Padding도 있다. 자세한 설명은 [padding은 왜 할까?](https://brunch.co.kr/@coolmindory/37)을 살펴보자.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d2b3ee",
   "metadata": {},
   "source": [
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/F-24-11.max-800x600.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c4645b",
   "metadata": {},
   "source": [
    "그림처럼 convolution filter를 여러개 중첩하면 [필터의 개수 x 필터의 가로 x 필터의 세로]로 이루거진 Weight를 가진다는 사실을 알 수 있다.   \n",
    "> 1. (1920, 1080, 3) x [16 x 5 x 5 x 3, Weight&Stride 5] = (384, 216, 16)\n",
    "2. (384, 216, 16) -> (1327104) x [1327104 x 1 Weight] = (1,)\n",
    "\n",
    "16개의 5x5 필터를 가진 Convolution 레이어를 Stride 5로 이미지 슬라이딩하고, 이렇게 생성된 출력을 1차원으로 펼쳐 Linear 레이어로 정보를 집약하는 과정이다. 필터들은 입력의 3채널에 적용되므로 3 x 16 x 5 x 5 = 1200개다.   \n",
    "한개의 Convolution 레이어로 파라미터 수를 620만개에서 130만개로 크게 줄일 수 있었다. Convolution 레이어는 여러 겹의 레이어를 중첩하는 것이 일반적이고, 중첩할수록 최종 Linear 레이어는 작아진다. 입력에서 중요한 부분을 뽑아내는데 최적화이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6a9aec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 이미지 데이터: (64, 1920, 1080, 3)\n",
      "\n",
      "Convolution 결과: (64, 384, 216, 16)\n",
      "Convolution Layer의 Parameter 수: 1200\n",
      "\n",
      "1차원으로 펼친 데이터: (64, 1327104)\n",
      "\n",
      "Linear 결과: (64, 1)\n",
      "Linear Layer의 Parameter 수: 1327104\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 64\n",
    "pic = tf.zeros((batch_size, 1920, 1080, 3))\n",
    "\n",
    "print(\"입력 이미지 데이터:\", pic.shape)\n",
    "conv_layer = tf.keras.layers.Conv2D(filters=16,\n",
    "                                    kernel_size=(5, 5),\n",
    "                                    strides=5,\n",
    "                                    use_bias=False)\n",
    "conv_out = conv_layer(pic)\n",
    "\n",
    "print(\"\\nConvolution 결과:\", conv_out.shape)\n",
    "print(\"Convolution Layer의 Parameter 수:\", conv_layer.count_params())\n",
    "\n",
    "flatten_out = tf.keras.layers.Flatten()(conv_out)\n",
    "print(\"\\n1차원으로 펼친 데이터:\", flatten_out.shape)\n",
    "\n",
    "linear_layer = tf.keras.layers.Dense(units=1, use_bias=False)\n",
    "linear_out = linear_layer(flatten_out)\n",
    "\n",
    "print(\"\\nLinear 결과:\", linear_out.shape)\n",
    "print(\"Linear Layer의 Parameter 수:\", linear_layer.count_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6805d6cb",
   "metadata": {},
   "source": [
    "Convolution 레이어가 이미지 입력을 처리할 때 Linear 레이어 대비 적은 파라미터로 중요한 이미지 피처를 뽑아 효과적임을 알 수 있었다. 그리고 이미지는 지역성(locality)이 중요한 정보가 되는 경우가 있는데, Linear는 중요한 정보가 소실되고 큰 파라미터 속에서 입출력 사이의 관계 가중치를 찾아야하는 어려운 문제를 풀어야 한다. Convolution 레이어는 필터의 구조 안에 지역 정보가 보존된다.   \n",
    "이렇게 인접한 픽셀들 사이에서 패턴을 추출할 수 있는게 불필요한 파라미터 및 연산량을 제거하고 효율적이게 정보를 집약시킬 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9585d4a6",
   "metadata": {},
   "source": [
    "# 4. 필터로 핵심을 추리는 Pooling 레이어"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78968db",
   "metadata": {},
   "source": [
    "Convolution 레이어만으로 객체를 판별하는데 문제가 있을 수 있다.   \n",
    "- 5x5 필터 사이즈는 object detection을 하기 위한 유의미한 정보를 담아내기에 작은 사이즈다.\n",
    "- Stride 5를 줬는데 이 사이에 object가 경계선에 있으면 인식하지 못할 수 있다.\n",
    "\n",
    "그렇다고 필터 사이즈를 키우면 파라미터와 연산량의 사이즈가 커지고 Accuracy도 떨어질 가능성이 높다. 그렇다면 어떻게 해결해야할까?   \n",
    "\n",
    "__Receptive Field__(수용영역)을 크게 해야한다. "
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "1f96220c",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38b6e5f",
   "metadata": {},
   "source": [
    "위의 이미지를 보면 7X7이미지에 3X3필터로 Convolution한 결과를 보여준다. 우측 빨간색 부분은 원본 이미지의 좌상단 3x3을 수용한 것인데, Receptive Field 크기가 Filter size와 같다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabc7f87",
   "metadata": {},
   "source": [
    "![](https://blog.kakaocdn.net/dn/biczZ1/btqED1eyDLf/c5cBddY5vw8DRo0K0S49L1/img.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990a8a41",
   "metadata": {},
   "source": [
    "Max Pooling의 장점으로 몇 가지가 있는데 알아보자.   \n",
    "- translational invariance 효과   \n",
    "Max Pooling을 통해 영역 중 가장 두드러진 특징 하나를 뽑는 것은 이미지가 약간의 시프트 효과에도 동일한 특징을 안정적으로 잡아낼 수 있다. 그리고 object 위치에 대한 오버피팅을 방지하고 안정적인 특징 추출 효과가 있다.   \n",
    "- Non-linear 함수와 동일한 피처 추출 효과   \n",
    "Relu같은 Non-linear 함수도 하위 많은 레이어의 연산 결과를 무시하는 효과를 발생하지만 그 결과 중요한 피처를 상위 레이어로 추출한다. 결과적으로 분류기의 성능을 증진시키는 효과를 가진다. Min, Max Pooling도 이와 동일하다.   \n",
    "- Receptive Field 극대화 효과   \n",
    "Max Pooling 없이 Receptive Field를 크게 하려면 Convolutional 레이어를 아주 많이 쌓아야한다. 그 결과 큰 파라미터로 인한 오버피팅, 연산량 증가, gradient vanishing 등 많은 문제를 감안해야한다. 해결하는 방법으로 Max Pooling이랑 Dilated Convolution이 있는데 [링크](https://m.blog.naver.com/sogangori/220952339643)를 참고하자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8e4890",
   "metadata": {},
   "source": [
    "# 5. 집약 정보 복원하는 Deconvolution 레이어"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954a2db5",
   "metadata": {},
   "source": [
    "Convolution 레이어를 통해 집약된 정보는 원본 데이터를 너무 많이 손실한건 아닐까? 손실된 정보에 의존한 이미지 분류와 오브젝트 디텍션 등의 결과를 신뢰할 수 있을까? 와 같이 의구심을 품을 수 있다. 그래서 Convolution 결과를 역재생 해서 원본과 유사한 정보를 복원하는 오토 인코더에 알아보자.   \n",
    "\n",
    "MNIST 데이터셋을 입력받아 그대로 복원하는 Auto Encoder를 만들어보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4feb39",
   "metadata": {},
   "source": [
    "### MNIST 데이터셋 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc711cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import json\n",
    "import matplotlib.pyplot as plt #for plotting\n",
    "\n",
    "# MNIST 데이터 로딩\n",
    "(x_train, _), (x_test, _) = mnist.load_data()    # y_train, y_test는 사용하지 않습니다.\n",
    "\n",
    "x_train = np.expand_dims(x_train, axis=3)\n",
    "x_test = np.expand_dims(x_test, axis=3)\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef7775f",
   "metadata": {},
   "source": [
    "AutoEncoder가 수행하는 Image Reconstrction Task는 x_train 라벨이 x_train 자신이기 때문에 y_train, y_test를 사용하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a693fef",
   "metadata": {},
   "source": [
    "### AutoEncoder 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be5178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoEncoder 모델 구성 - Input 부분\n",
    "input_shape = x_train.shape[1:]\n",
    "input_img = Input(shape=input_shape)\n",
    "\n",
    "# AutoEncoder 모델 구성 - Encoder 부분\n",
    "encode_conv_layer_1 = Conv2D(16, (3, 3), activation='relu', padding='same')\n",
    "encode_pool_layer_1 = MaxPooling2D((2, 2), padding='same')\n",
    "encode_conv_layer_2 = Conv2D(8, (3, 3), activation='relu', padding='same')\n",
    "encode_pool_layer_2 = MaxPooling2D((2, 2), padding='same')\n",
    "encode_conv_layer_3 = Conv2D(4, (3, 3), activation='relu', padding='same')\n",
    "encode_pool_layer_3 = MaxPooling2D((2, 2), padding='same')\n",
    "\n",
    "encoded = encode_conv_layer_1(input_img)\n",
    "encoded = encode_pool_layer_1(encoded)\n",
    "encoded = encode_conv_layer_2(encoded)\n",
    "encoded = encode_pool_layer_2(encoded)\n",
    "encoded = encode_conv_layer_3(encoded)\n",
    "encoded = encode_pool_layer_3(encoded)\n",
    "\n",
    "# AutoEncoder 모델 구성 - Decoder 부분\n",
    "decode_conv_layer_1 = Conv2D(4, (3, 3), activation='relu', padding='same')\n",
    "decode_upsample_layer_1 = UpSampling2D((2, 2))\n",
    "decode_conv_layer_2 = Conv2D(8, (3, 3), activation='relu', padding='same')\n",
    "decode_upsample_layer_2 = UpSampling2D((2, 2))\n",
    "decode_conv_layer_3 = Conv2D(16, (3, 3), activation='relu')\n",
    "decode_upsample_layer_3 = UpSampling2D((2, 2))\n",
    "decode_conv_layer_4 = Conv2D(1, (3, 3), activation='sigmoid', padding='same')\n",
    "\n",
    "decoded = decode_conv_layer_1(encoded)   # Decoder는 Encoder의 출력을 입력으로 받습니다.\n",
    "decoded = decode_upsample_layer_1(decoded)\n",
    "decoded = decode_conv_layer_2(decoded)\n",
    "decoded = decode_upsample_layer_2(decoded)\n",
    "decoded = decode_conv_layer_3(decoded)\n",
    "decoded = decode_upsample_layer_3(decoded)\n",
    "decoded = decode_conv_layer_4(decoded)\n",
    "\n",
    "# AutoEncoder 모델 정의\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2663f4",
   "metadata": {},
   "source": [
    "Output shape를 변화시키는건 `MaxPooling2D`레이어만 한다. 그래서 Encoder를 통과한 직후에 Output은 4x4가 되어있다. 우리가 하려는 것은 Decoder 레이어인데 Con2D와 UpSampling2D 레이어를 거쳐 28x28이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e2623",
   "metadata": {},
   "source": [
    "### AutoEncoder 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda2042",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d7b4ff",
   "metadata": {},
   "source": [
    "loss로 분류 모델에서 사용되는 `binary_crossentropy`를 사용했다. 모든 dim에서 0~1사이의 값을 가진 입력 데이터와 출력 데이터(마지막 sigmoid) 사이의 분포가 최대한 유사하는 바람에 loss는 타당하다고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c429dd7",
   "metadata": {},
   "source": [
    "### AutoEncoder Reconstruction Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babc78cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_10 = x_test[:10]       # 테스트 데이터셋에서 10개만 골라서\n",
    "x_test_hat = autoencoder.predict(x_test_10)    # AutoEncoder 모델의 이미지 복원생성\n",
    "x_test_imgs = x_test_10.reshape(-1, 28, 28)\n",
    "x_test_hat_imgs = x_test_hat.reshape(-1, 28, 28)\n",
    "\n",
    "plt.figure(figsize=(12,5))  # 이미지 사이즈 지정\n",
    "for i in range(10):  \n",
    "    # 원본이미지 출력\n",
    "    plt.subplot(2, 10, i+1)\n",
    "    plt.imshow(x_test_imgs[i])\n",
    "    # 생성된 이미지 출력\n",
    "    plt.subplot(2, 10, i+11)\n",
    "    plt.imshow(x_test_hat_imgs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a22ae8",
   "metadata": {},
   "source": [
    "이미지를 제대로 생성하는지 확인해봤다. 하지만 동일한 Decoder 네트워크 구조를 갖고 Variational Autoencoder나 DCGAN 등은 훌륭한 이미지를 생성하기 때문에, Decoder 네트워크 구조의 문제만은 아닐것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91dc3b3",
   "metadata": {},
   "source": [
    "### Decoder Layers for Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e1ed0",
   "metadata": {},
   "source": [
    "위의 Decoder에서 이미지 복원을 위해 사용한 Convolution 레이어는 Encoder에서 사용한 것과 동일하다. 그래서 크기도 변하지 않은 채 채널 개수만 2배로 늘리고 있어 Convolution 레이어를 거쳐 정보가 집약되는 것이 아닌 많아지고 있다.   \n",
    "\n",
    "Convolution의 수학적 역연산으로 Deconvolution과는 다르다. 오히려 정방향 Convolution 연산을 통해 원본 이미지와 가까운 이미지를 재생하는 효과를 구현한 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feecc0f",
   "metadata": {},
   "source": [
    "### Upsampling 레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90e4bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
