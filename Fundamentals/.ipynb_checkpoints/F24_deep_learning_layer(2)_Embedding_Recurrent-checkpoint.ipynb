{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff7409db",
   "metadata": {},
   "source": [
    "### 딥러닝 모델을 구성하는 Embedding 레이어, RNN 레이어에 대해 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6a5b94",
   "metadata": {},
   "source": [
    "# 분포 가설과 분산 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a44d764",
   "metadata": {},
   "source": [
    "컴퓨터에게 바나나와 사과를 가르치기 위해 어떻게 할까? 바나나는 `[1]`, 사과는 `[0]`이라고 순번을 매겨보자. 그러면 새롭게 추가하는 배는 `[2]`라고 표현하면 될까? 단어 순서대로 순번을 매기면 편하지만 컴퓨터한테 단어의 의미를 알려주긴 어렵다.   \n",
    "\n",
    "의미를 넣기 위해 2차원 이상의 벡터로 단어를 표현해보자. 사과: `[0,0]`, 바나나: `[1,1]`, 배: `[0,1]` 벡터의 첫 번째 요소는 `0: 둥글다, 1: 길쭉하다`를 나타내고, 두 번째 요소는 색상이 `0: 빨강, 1: 노랑`을 나타내면 어느정도 맞는다. 이렇게 벡터의 특정 차원에 단어, 의미를 매핑하는 방식을 __희소 표현(sparse representation)__ 이라고 한다. 하지만 세상의 모든 단어를 표현하기엔 벡터가 수 없이 많이 필요하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc8dcd1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f86177",
   "metadata": {},
   "source": [
    "방법을 바꿔 모든 단어들을 고정차원의 벡터로 표현해보자. (예를들어 256차원으로) 그리고 특정 차원이 의미를 갖지도 않을 것이다. 유사한 맥락에서 나타나는 단어는 의미도 비슷하다는 방식인데 이를 __분포 가설(distribution hypothesis)__ 이라고 한다.   \n",
    "\n",
    "- 나는 _을 먹는다.\n",
    "- 나는 물을 먹는다.\n",
    "- 나는 사과를 먹는다.\n",
    "\n",
    "`나는`과 `먹는다` 사이에 나타나는 것들이 의미적인 유사성을 갖고 있지만, 무엇인지는 알 수 없다. 유사한 맥락에서 나온 단어끼리 벡터 사이의 거리를 가깝게 하고, 그렇지 않으면 멀게 만들어보자. 이런 방식을 __분산 표현(distributed representation)__ 이라고 한다. 희소 표현과 달리 벡터의 차원이 특정한 의미를 담고 있는게 아니라 벡터가 여러 차원에 분산되어 있는 것이다. 이를 응용하면 단어간의 유사도도 계산으로 구할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9745f175",
   "metadata": {},
   "source": [
    "__Embedding 레이어__ 는 단어의 분산 표현을 구하기 위한 레이어이다. 단어 n개와 k차원으로 표현해달라고 컴퓨터에 입력하면 컴퓨터가 $n*k$ 형태의 분산 표현 사전을 만드는데 이것이 Weight이고 파라미터이다. 그리고 수많은 데이터를 통해 적합한 파라미터를 찾아가게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489265e5",
   "metadata": {},
   "source": [
    "# Embedding 레이어"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6752ac5",
   "metadata": {},
   "source": [
    "간단하게 컴퓨터용 단어 사전이라고 말할 수 있다. 단어 n개를 말해주면 컴퓨터는 사전을 만들고 데이터를 거쳐 단어의 분산 표현을 업데이트한다.   \n",
    "Embedding 사이즈를 정해주면서 단어를 깊게 표현하라고 하면 Weight는 단어의 수, Embedding은 사이즈로 정의되어 다음과 같은 모습을 갖게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb4babd",
   "metadata": {},
   "source": [
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/F-24-12.max-800x600.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7697ebf",
   "metadata": {},
   "source": [
    "Embedding 레이어는 입력으로 들어온 단어를 분산 표현으로 연결해주는데 Weight에서 특정 행을 읽어오는 것과 같아 __룩업 테이블(lookup table)__ 이라고 부르기도 한다. 단어가 룩업테이블에 매핑되는 부분은 어떤 원리로 동작하는 것일까?   \n",
    "\n",
    "[단어 간 유사도 파악 방법](https://www.kakaobrain.com/blog/6) \n",
    "__원-핫 인코딩(one-hot encoding)__ 이라고 한다. n개의 단어를 n차원의 벡터로 표현하고 특정 단어 하나를 1로 나머지는 0으로 담아내는 방식이다. 하지만 단어간 관계를 반영하지 못하고 독립적인 상태로 존재하고 특정 고차원 수준을 넘어서면 성능이 0으로 수렴한다는 단점이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f686fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "vocab = {      # 사용할 단어 사전 정의\n",
    "    \"i\": 0,\n",
    "    \"need\": 1,\n",
    "    \"some\": 2,\n",
    "    \"more\": 3,\n",
    "    \"coffee\": 4,\n",
    "    \"cake\": 5,\n",
    "    \"cat\": 6,\n",
    "    \"dog\": 7\n",
    "}\n",
    "\n",
    "sentence = \"i i i i need some more coffee coffee coffee\"\n",
    "# 위 sentence\n",
    "_input = [vocab[w] for w in sentence.split()]  # [0, 0, 0, 0, 1, 2, 3, 4, 4, 4]\n",
    "\n",
    "vocab_size = len(vocab)   # 8\n",
    "\n",
    "one_hot = tf.one_hot(_input, vocab_size)\n",
    "print(one_hot.numpy())    # 원-핫 인코딩 벡터를 출력해 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983a6e85",
   "metadata": {},
   "source": [
    "이렇게 생성된 원-핫 벡터를 linear 레이어에 넣어보면 어떤 결과가 나올까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9f6fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_size = 2   # 보기 좋게 2차원으로 분산 표현하도록 하죠!\n",
    "linear = tf.keras.layers.Dense(units=distribution_size, use_bias=False)\n",
    "one_hot_linear = linear(one_hot)\n",
    "\n",
    "print(\"Linear Weight\")\n",
    "print(linear.weights[0].numpy())\n",
    "\n",
    "print(\"\\nOne-Hot Linear Result\")\n",
    "print(one_hot_linear.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94146137",
   "metadata": {},
   "source": [
    "linear 레이어의 weight에서 단어 인덱스 배열에 해당하는 행만 읽어오는 효과가 있다.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0703c108",
   "metadata": {},
   "source": [
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/F-24-12.max-800x600.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba48e491",
   "metadata": {},
   "source": [
    "단어를 원-핫 인코딩해서 linear 연산을 하는게 파란 선이다. 원-핫 인코딩을 위해 단어 사전을 구축하고 단어를 사전의 인덱스로 변환해주면 Embedding 레이어를 사용할 수 있는것이다. 많은 자연어 처리 모델에서 문장 데이터 속 단어들을 단어 사전의 인덱스 숫자로 표혔했다가 모델에 입력하는데, 사실은 인덱스를 원-핫 임베딩으로 변환 후 Embedding 레이어의 입력으로 넣어주고 있었다.   \n",
    "\n",
    "Embedding 레이어를 선언하는 법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3d7d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_words = tf.constant([[3, 57, 35]])\n",
    "# 3번 단어 / 57번 단어 / 35번 단어로 이루어진 한 문장입니다.\n",
    "\n",
    "print(\"Embedding을 진행할 문장:\", some_words.shape)\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=64, output_dim=100)\n",
    "# 총 64개의 단어를 포함한 Embedding 레이어를 선언할 것이고,\n",
    "# 각 단어는 100차원으로 분산 표현 할 것입니다.\n",
    "\n",
    "print(\"Embedding된 문장:\", embedding_layer(some_words).shape)\n",
    "print(\"Embedding Layer의 Weight 형태:\", embedding_layer.weights[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9778d0",
   "metadata": {},
   "source": [
    "Embedding 레이어는 쉽지만 주의사항이 있다. 딥러닝은 미분을 기반으로 동작하는데 이는 단어를 대응하기만 해서 미분이 불가능하다. 그래서 신경망 설계를 할 때, 어떤 연산 결과를 Embedding 레이어에 연결시킬 수 없다. 그래서 Embedding 레이어는 입력에 직접 연결되게 사용해야 한다. 입력의 형태는 원-핫 인코딩 단어 벡터 형태가 이상적이다.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2055ecb8",
   "metadata": {},
   "source": [
    "# 순차적인 Recurrent 레이어 RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7543759",
   "metadata": {},
   "source": [
    "문장, 영상, 음성 등의 데이터는 이미지 데이터와 다르게 순차적인(sequntial) 특성을 갖고 있다. 이는 시간의 개념으로 생각하면 된다. 데이터의 나열 사이에 연관성이 없어도 시퀀셜 데이터는 아니라고 할 수 없다. `[1, 2, 3, 오리, baby, 0.8]` 이런 데이터도 연관성이 없지만 시퀀스 데이터라고 칭한다.   \n",
    "\n",
    "인공지능이 예측을 하기 위해 요소간 연관성이 있어야한다. 그래서 딥러닝이 말하는 시퀀스 데이터는 순차적인 특성을 필수로 갖는다. 그리고 이런 데이터를 처리하기 위해 Recurrent Neural Network 혹은 Recurrent 레이어(__RNN__)가 고안됐다. 반복되는 연속성의 성격을 갖고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8fee05",
   "metadata": {},
   "source": [
    "[Illustrated Guide to Recurrent Neural Networks](https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9)   \n",
    "![](https://miro.medium.com/max/960/1*TqcA9EIUF-DGGTBhIx_qbQ.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf011c8",
   "metadata": {},
   "source": [
    "RNN에 입력으로 들어간 모든 단어만큼 Weight를 만드는게 아니고 `(입력의 차원, 출력의 차원)`에 해당하는 하나의 Weight를 순차적으로 업데이트 하는 것이다. 그래서 여러 번의 연산이 필요해 다른 레이어에 비해 느리다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3125e2f",
   "metadata": {},
   "source": [
    "![](https://d3s0tskafalll9.cloudfront.net/media/original_images/F-24-13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61218831",
   "metadata": {},
   "source": [
    "위 그림을 보면 What에서 추출한 정보가 마지막의 ?에 도달했을땐 엄청 희석된 모습을 볼 수 있다. 이건 RNN의 고질적인 문제점이고, 이를 기울기 소실(vanishing gradient) 문제라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b4670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"What time is it ?\"\n",
    "dic = {\n",
    "    \"is\": 0,\n",
    "    \"it\": 1,\n",
    "    \"What\": 2,\n",
    "    \"time\": 3,\n",
    "    \"?\": 4\n",
    "}\n",
    "\n",
    "print(\"RNN에 입력할 문장:\", sentence)\n",
    "\n",
    "sentence_tensor = tf.constant([[dic[word] for word in sentence.split()]])\n",
    "\n",
    "print(\"Embedding을 위해 단어 매핑:\", sentence_tensor.numpy())\n",
    "print(\"입력 문장 데이터 형태:\", sentence_tensor.shape)\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=len(dic), output_dim=100)\n",
    "emb_out = embedding_layer(sentence_tensor)\n",
    "\n",
    "print(\"\\nEmbedding 결과:\", emb_out.shape)\n",
    "print(\"Embedding Layer의 Weight 형태:\", embedding_layer.weights[0].shape)\n",
    "\n",
    "rnn_seq_layer = \\\n",
    "tf.keras.layers.SimpleRNN(units=64, return_sequences=True, use_bias=False)\n",
    "rnn_seq_out = rnn_seq_layer(emb_out)\n",
    "\n",
    "print(\"\\nRNN 결과 (모든 Step Output):\", rnn_seq_out.shape)\n",
    "print(\"RNN Layer의 Weight 형태:\", rnn_seq_layer.weights[0].shape)\n",
    "\n",
    "rnn_fin_layer = tf.keras.layers.SimpleRNN(units=64, use_bias=False)\n",
    "rnn_fin_out = rnn_fin_layer(emb_out)\n",
    "\n",
    "print(\"\\nRNN 결과 (최종 Step Output):\", rnn_fin_out.shape)\n",
    "print(\"RNN Layer의 Weight 형태:\", rnn_fin_layer.weights[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ee31a6",
   "metadata": {},
   "source": [
    "# 순차적인 Recurrent 레이어 LSTM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
