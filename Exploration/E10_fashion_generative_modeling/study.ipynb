{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "895ebcf9",
   "metadata": {},
   "source": [
    "# 인공지능으로 세상에 없던 새로운 패션 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56682c0e",
   "metadata": {},
   "source": [
    "지금까지 많이 분류 모델을 많이 만들었다. 이들은 판별 모델링(Discriminative Modeling)이라고 부른다. 생성 모델링(Generative Modeling)은 학습한 데이터셋과 비슷하면서 기존에 없는 새로운 데이터셋을 생성하는 모델이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d46f6",
   "metadata": {},
   "source": [
    "[Announcing AWS DeepComposer with Dr. Matt Wood, feat. Jonathan Coulton\n",
    "](https://www.youtube.com/watch?v=XH2EbK9dQlg&t=650s)   \n",
    "AWS의 음악을 만드는 DeepComposer를 보면 생성자(generator), 판별자(discriminator) 두 가지 네트워크로 모델을 만들었다. 생성자는 오케스트라의 역할로 음악을 만드는 역할을 하고 판별자는 지휘자의 역할로 피드백을 하며 음악을 더 괜찮게 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db537a9c",
   "metadata": {},
   "source": [
    "# Pix2Pix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0861e65f",
   "metadata": {},
   "source": [
    "간단한 이미지를 입력하면 실제 사진처럼 바꿔줄 때 많이 사용하는 모델이다. input image를 받으면 내부 연산을 통해 실제 사진같은 predicted image를 출력해준다. 초기에는 ground truth와 비교하며 평가하고 점차 실제같은 결과물을 만들어낸다. [논문](https://arxiv.org/pdf/1611.07004.pdf)   \n",
    "![](https://cdn-ak.f.st-hatena.com/images/fotolife/l/lib-arts/20190602/20190602114525.png)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f577aa8b",
   "metadata": {},
   "source": [
    "# CycleGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3bec27",
   "metadata": {},
   "source": [
    "Pix2Pix에서 발전된 모델로 CycleGAN이 있다. 한 이미지와 다른 이미지를 번갈아가며 Cyclic하게 변환할 수 있다. 실사 이미지를 그림으로 바꾸는 것 과 그림을 실사 이미지로 바꾸는 두 가지가 모두 가능한 양방향으로 이미지 변환이 가능하다. 각각의 사진 데이터만 있으면 각 스타일을 학습해서 이미지에 학습한 스타일을 입힐 수 있다. 딥러닝에서 쌍으로 된 데이터가 필요 없다는 것은 데이터를 구하기 쉽고 라벨을 붙이는 주석 비용이 필요하지 않아 큰 장점이다.   \n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/CycleGAN.max-800x600.png)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e837dd8d",
   "metadata": {},
   "source": [
    "다른 유명한 활용 사례는 실제 사진을 가른 화가가 그린 그림처럼 바꾸는 것이다.   \n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/CycleGAN2.max-800x600.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9a1b1e",
   "metadata": {},
   "source": [
    "# Neural Style Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d7e021",
   "metadata": {},
   "source": [
    "이미지의 스타일을 변환시키는 방법이다. 전체 이미지를 구성하고 싶은 Base Image의 content(내용)만 추출하고, 입히고 싶은 스타일이 담긴 Style Image의 style을 추출한다. base와 style 두 장을 활용해서 새로운 이미지를 만들어 내는 것이다.   \n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/StyleTransfer.max-800x600.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3955df4f",
   "metadata": {},
   "source": [
    "# Fashion MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e972c2b",
   "metadata": {},
   "source": [
    "fashion MNIST 데이터셋으로 [fashion-mnist 깃허브](https://github.com/zalandoresearch/fashion-mnist)를 참고하자.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288fe683",
   "metadata": {},
   "source": [
    "28x28 흑백 사진의 train set 6만장, test set 1만장으로 이루어져있다. 0~9까지의 카테고리로 0-T-shirt/top, 1-Trouser, 2-Pullover, 3-Dress, 4-Coat, 5-Sandal, 6-Shirt, 7-Sneaker, 8-Bag, 9-Ankle boot으로 구성되어 있다. 기존 숫자 분류기 MNIST는 too easy, overused, noet represent modern CV tasks 의 이유로 새롭게 만들어졌다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c48d081",
   "metadata": {},
   "source": [
    "# 코드 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af918ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import PIL\n",
    "import imageio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"tensorflow\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfcc0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(train_x, _), (test_x, _) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1791658a",
   "metadata": {},
   "source": [
    "라벨이 필요 없기 때문에 `y_train`, `y_test`는 `_`로 데이터를 무시하고 불러오도록 진행했다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cb1071",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"max pixel:\", train_x.max())\n",
    "print(\"min pixel:\", train_x.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb4eede",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = (train_x - 127.5) / 127.5 # 이미지를 [-1, 1]로 정규화\n",
    "\n",
    "print(\"max pixel:\", train_x.max())\n",
    "print(\"min pixel:\", train_x.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d025241",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a771b171",
   "metadata": {},
   "source": [
    "딥러닝에서 이미지를 다루려면 채널 수에 대한 차원이 필요하다. 흑백 이미지이므로 채널값 1을 추가하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4554919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.reshape(train_x.shape[0], 28, 28, 1).astype('float32')\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a79c145",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_x[0].reshape(28, 28), cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cb8de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(train_x[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f'index: {i}')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99350dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 12))\n",
    "\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    random_index = np.random.randint(1, 60000) # 랜덤으로 정수를 만들어서 25개를 보여주는 함수\n",
    "    plt.imshow(train_x[random_index].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f'index: {random_index}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a56be8",
   "metadata": {},
   "source": [
    "### 데이터 학습 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0a22f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83914c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_x).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4697fa",
   "metadata": {},
   "source": [
    "BUFFER_SIZE가 데이터셋을 섞어서 모델에 넣어주고, BATCH_SIZE별로 잘라서 학습을 진행하는 __미니 배치 학습__이다. [텐서플로우 도큐먼트 tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c1a844",
   "metadata": {},
   "source": [
    "# 그림을 만드는 생성자, 평가하는 비평가 판별자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b79a86",
   "metadata": {},
   "source": [
    "가장 간단한 형태의 생성 모델중 GAN(Generative Adversarial Network)이 있다. 2014년 lan Goodfellow라는 연구자에서 나왔고 두 가지 네트워크가 있다. [논문 확인하기](https://arxiv.org/pdf/1406.2661.pdf)   \n",
    "- 생성자 generator: 의미 없는 랜덤 노이즈로부터 신경망에서 연산을 통해 이미지 형상의 벡터를 생성한다. \n",
    "- 판별자 discriminator: 기존에 있던 진짜 이미지와 생성자가 만들어낸 이미지를 입력받아 각 이미지가 real, fake에 대한 판단을 실숫값 정도로 출력한다. \n",
    "\n",
    "논문을 확인하면 두 네트워크에 생성자가 만든것을 판별자가 구별할 수 없을만큼 계속해서 경쟁하듯 서로의 개선을 유도하는데, 이 때문에 Adversarial(적대적인)으로 표현했다.   \n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/GAN.max-800x600.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c6ce49",
   "metadata": {},
   "source": [
    "## 생성자 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d58f58",
   "metadata": {},
   "source": [
    "DCGAN(Deep convolutional GAN)은 GAN이 발표되고 1년 반 후에 발표된 논문이다. GAN을 발전시켜 고화질 이미지 생성을 이루어낸 첫 번재 논문이다. 이후 GAN 기반 이미지 생성은 DCGAN 모델을 발전시킨 형태이므로 매우 의미 있으니 한번 구조를 살펴보자. [Tensorflow 2.0 DCGAN 구현](https://www.tensorflow.org/tutorials/generative/dcgan?hl=ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a7a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "\n",
    "    # Start\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # First: Dense layer\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Second: Reshape layer\n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "\n",
    "    # Third: Conv2DTranspose layer\n",
    "    model.add(layers.Conv2DTranspose(128, kernel_size=(5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Fourth: Conv2DTranspose layer\n",
    "    model.add(layers.Conv2DTranspose(64, kernel_size=(5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Fifth: Conv2DTranspose layer\n",
    "    model.add(layers.Conv2DTranspose(1, kernel_size=(5, 5), strides=(2, 2), padding='same', use_bias=False, \\\n",
    "                                     activation='tanh'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6483027a",
   "metadata": {},
   "source": [
    "중요한게 Conv2DTranspose 레이어로 Conv2D와 반대로 이미지 사이즈를 넓혀주는 층이다. 이를 세 번 사용해 (7, 7, 256) -> (14, 14, 64) -> (28, 28, 1) 으로 이미지를 키웠다.   \n",
    "BatchNormalization 레이어는 신경망의 가중치가 폭발하지 않도록 정규화 시켜주고 활성화 함수는 LeakyReLU를 사용했다. 마지막은 tanh를 사용했는데 -1~1로 정규화한 데이터셋과 동일하기위해 사용했다.\n",
    "- [Batch normalization](https://eehoeskrap.tistory.com/430)\n",
    "- [활성화 함수](https://newly0513.tistory.com/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d16cb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a16eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.random.normal([1, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d48f2d",
   "metadata": {},
   "source": [
    "`tf.random.normal`을 이용해서 가우시안 분포에서 뽑아낸 랜덤 벡터로 이루어진 노이즈 벡터를 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caab5537",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_image = generator(noise, training=False)\n",
    "generated_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57922409",
   "metadata": {},
   "source": [
    "학습하는중이 아니라 training은 False로 뒀다. batch normalization 레이어는 훈련 시기와 추론(infernce) 시기의 행동이 달라 이처럼 주어야 올바른 결과를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e674d420",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595be974",
   "metadata": {},
   "source": [
    "시각화해봤지만 모델이 학습되지 않은 상태라 의미가 없는 노이즈 이미지가 생성되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07717c4e",
   "metadata": {},
   "source": [
    "## 판별자 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0bb019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "\n",
    "    # Start\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # First: Conv2D Layer\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Second: Conv2D Layer\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Third: Flatten Layer\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Fourth: Dense Layer\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3dd81b",
   "metadata": {},
   "source": [
    "`Conv2DTranspose`층으로 이미지를 키운 생성자와 반대로 `Conv2D`층으로 이미지 크기를 줄여나간다. [28, 28, 1] 사이즈의 이미지를 (28, 28, 1) -> (14, 14, 64) -> (7, 7, 128)로 줄인다. `Flatten`으로 3차원 이미지를 1차원으로 쭉 펴 7x7x128=6272, (1, 6272) 형상의 벡터로 변환시킨다. 마지막 `Dense`로 단 하나의 값을 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570faf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c570159",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision = discriminator(generated_image, training=False)\n",
    "decision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cae0d17",
   "metadata": {},
   "source": [
    "텐서플로우의 텐서 형태로 출력이 됐다. 이를 갖고 모델 학습을 시키면 될 거같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd9f71b",
   "metadata": {},
   "source": [
    "# 모델 학습을 위한 손실함수와 최적화함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700ce223",
   "metadata": {},
   "source": [
    "## 손실함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d885f61c",
   "metadata": {},
   "source": [
    "GAN은 교차 엔트로피(Cross Entropy)를 사용한다. 분류 모델을 설계할때 많이 사용한 손실함수다. 점차 가까워지는걸 원하는 두 값이 얼마나 차이나는지 정량적으로 계산할 때 많이 쓰인다. 판별자는 하나의 이미지가 가짜인지 진짜인지 나타내는 이진 클래스 분류 문제를 풀어야 하므로 binary cross entropy를 사용하자.   \n",
    "\n",
    "real image의 라벨을 1, fake image 라벨을 0으로 두었을 때 모델마다 손실함수를 이용해 달성해야하는 목표는 다음과 같다.   \n",
    "- 생성자: 판별자가 fake image를 판별할 때 D(fake_image) 값이 1에 가까워지는것.\n",
    "- 판별자: real image 판별값, D(real_image)는 1에 fake image의 D는 0에 가까워지는것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c17f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068ffe7e",
   "metadata": {},
   "source": [
    "교차 엔트로피를 계산하기 위해 판별자가 판별한 값이 필요한데 출력하는 값의 범위가 정해지지 않아 모든 실숫값을 가질 수 있다. 그래서 다음처럼 `tf.keras.losses`의 `BinaryCrossentropy`를 사용해서 `from_logits=True`로 설정해야 sigmoid 함수를 활용해 0~1 사이의 값으로 정규화해 알맞게 계산할 수 있다.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2afcd3",
   "metadata": {},
   "source": [
    "cross_entropy는 face_output, real_output을 비교하는데 어떻게 해야할까?   \n",
    "`tf.ones_like()`, `tf.zeros_like()`를 활용하면 해당 벡터가 1, 0으로 가득 채워지는데 이를 활용하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632693e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c001b4",
   "metadata": {},
   "source": [
    "fake_output이 1에 가까워지기를 바래서 다음과 같이 구현했다. cross_entropy값은 fake_output이 1에 가까울수록 작은 값을 갖게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195e11be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d4254c",
   "metadata": {},
   "source": [
    "real, fake 두가지의 loss값을 계산하게 된다. real은 1에 가깝게, fake는 0에 가깝게 바라므로 두 가지 계산한다. 최종적으로 이 둘을 더한 값이 return 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46df0ce",
   "metadata": {},
   "source": [
    "### discriminator accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4465f83d",
   "metadata": {},
   "source": [
    "real, fake output의 accuracy를 따로 계산해서 비교해보자. 가장 이상적인 수치는 초반에 둘 다 1.0에 가깝게 나오다가 서서히 낮아져 0.5에 가까워지는 것이다. fake accuracy가 1.0에 가까우면 생성자가 판별자를 잘 속이지 못하고 있다는 뜻이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a61a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_accuracy(real_output, fake_output):\n",
    "    real_accuracy = tf.reduce_mean(tf.cast(tf.math.greater_equal(real_output, tf.constant([0.5])), tf.float32))\n",
    "    fake_accuracy = tf.reduce_mean(tf.cast(tf.math.less(fake_output, tf.constant([0.5])), tf.float32))\n",
    "    return real_accuracy, fake_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f9271d",
   "metadata": {},
   "source": [
    "위를 해석해보면   \n",
    "- `tf.math.greater_equal(real_output, tf.constant([0.5])`는 real_output의 값이 0.5 이상인지에 따라 True, False로 계산된다\n",
    "- `tf.cast(), tf.float32`로 true면 1.0 false면 0.0으로 변환한다. \n",
    "- `tf.reduce_mean()`으로 평균을 내어 배치의 정확도를 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e51bcc5",
   "metadata": {},
   "source": [
    "## 최적화 함수(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fb122c",
   "metadata": {},
   "source": [
    "Adam을 이용할 것인데 이는 `tf.keras.optimizers` 안에 있다. 중요한 점은 생성자와 구분자는 따로 학습을 진행하는 개별 네트워크여서 optimizer를 따로 선언해주어야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f2e54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4) # 1e-4는 learning rate를 말한다.\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e247b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim]) # 100차원의 노이즈를 16개 선언\n",
    "seed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d1bd95",
   "metadata": {},
   "source": [
    "# 훈련과정 설계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f47e4a7",
   "metadata": {},
   "source": [
    "하나의 미니 배치당 진행할 train_step 함수를 먼저 만들어야한다. 학습시킬 훈련 함수 위에 `@tf.funtion`이라는 데코레이터를 붙여 사용한다. 이는 직접 session을 열어 학습하고 완료되면 닫아주는 번거로운 과정을 내부적으로 처리해 편리하게 만들어준다. [파이썬-데코레이터](https://schoolofweb.net/blog/posts/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EB%8D%B0%EC%BD%94%EB%A0%88%EC%9D%B4%ED%84%B0-decorator/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec44dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def f(x, y):\n",
    "  print(type(x))\n",
    "  print(type(y))\n",
    "  return x ** 2 + y\n",
    "\n",
    "x = np.array([2, 3])\n",
    "y = np.array([3, -2])\n",
    "f(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e4e0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "@tf.function    # 위와 동일한 함수이지만 @tf.function 데코레이터가 적용되었습니다.\n",
    "def f(x, y):\n",
    "  print(type(x))\n",
    "  print(type(y))\n",
    "  return x ** 2 + y\n",
    "\n",
    "x = np.array([2, 3])\n",
    "y = np.array([3, -2])\n",
    "f(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb50b89",
   "metadata": {},
   "source": [
    "`@tf.fuction` 데코레이터가 사용된 함수에 입력된 값은 tensorflow의 graph 노드 타입으로 변환되는걸 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d136974",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):  #(1) 입력데이터\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])  #(2) 생성자 입력 노이즈\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:  #(3) tf.GradientTape() 오픈\n",
    "        generated_images = generator(noise, training=True)  #(4) generated_images 생성\n",
    "\n",
    "        #(5) discriminator 판별\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        #(6) loss 계산\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "        #(7) accuracy 계산\n",
    "        real_accuracy, fake_accuracy = discriminator_accuracy(real_output, fake_output) \n",
    "    \n",
    "    #(8) gradient 계산\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    #(9) 모델 학습\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss, real_accuracy, fake_accuracy  #(10) 리턴값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61a5282",
   "metadata": {},
   "source": [
    "- (1). real image 역할의 images 세트를 입력 받음\n",
    "- (2). FAKE IMAGE를 생성하기 위한 noise를 images와 같은 크기의 BATCH_SIZE만큼 생성함\n",
    "- (3). 가중치 갱신을 위한 gradient를 자동 미분으로 계산하기 위해 with로 열기\n",
    "- (4). generator가 noise를 입력받고 images를 생성하기\n",
    "- (5). real image인 images, fake image인 generated_images를 입력받아 각각 출력\n",
    "- (6). 이들을 각각의 loss값 계산\n",
    "- (7). 그리고 accuracy도 계산\n",
    "- (8). gen_tape, disc_tape로 gradient 계산\n",
    "- (9). 계산된 gradient를 optimizer에 입력해 가중치 갱신\n",
    "- (10). 이번 스텝에 계산된 loss, accuracy를 리턴\n",
    "\n",
    "이 과정으로 한번의 train_step이 끝난다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dacecc",
   "metadata": {},
   "source": [
    "train_step의 학습 현황을 볼 수 있는 함수를 생성하고 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9b52ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, it, sample_seeds):\n",
    "\n",
    "    predictions = model(sample_seeds, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "   \n",
    "    plt.savefig('{}/aiffel/dcgan_newimage/fashion/generated_samples/sample_epoch_{:04d}_iter_{:03d}.png'\n",
    "                    .format(os.getenv('HOME'), epoch, it))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d564c82",
   "metadata": {},
   "source": [
    "loss와 accuracy 그래프도 시각화해보자. 두 모델이 서로의 학습 과정에 영향을 주고받기 때문에 까다롭다. train_step() 함수가 리턴하는 gen_loss, disc_loss, real_accuracy, fake_accuracy 4가지의 값을 history라는 dict구조에 리스트로 저장하다 매 epoch마다 시각화 하도록 해보자. history는 `history['gen_loss']`로 접근할 수 있게 list로 관리했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29415f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 6    # matlab 차트의 기본 크기를 15,6으로 지정해 줍니다.\n",
    "\n",
    "def draw_train_history(history, epoch):\n",
    "    # summarize history for loss  \n",
    "    plt.subplot(211)  \n",
    "    plt.plot(history['gen_loss'])  \n",
    "    plt.plot(history['disc_loss'])  \n",
    "    plt.title('model loss')  \n",
    "    plt.ylabel('loss')  \n",
    "    plt.xlabel('batch iters')  \n",
    "    plt.legend(['gen_loss', 'disc_loss'], loc='upper left')  \n",
    "\n",
    "    # summarize history for accuracy  \n",
    "    plt.subplot(212)  \n",
    "    plt.plot(history['fake_accuracy'])  \n",
    "    plt.plot(history['real_accuracy'])  \n",
    "    plt.title('discriminator accuracy')  \n",
    "    plt.ylabel('accuracy')  \n",
    "    plt.xlabel('batch iters')  \n",
    "    plt.legend(['fake_accuracy', 'real_accuracy'], loc='upper left')  \n",
    "    \n",
    "    # training_history 디렉토리에 epoch별로 그래프를 이미지 파일로 저장합니다.\n",
    "    plt.savefig('{}/aiffel/dcgan_newimage/fashion/training_history/train_history_{:04d}.png'\n",
    "                    .format(os.getenv('HOME'), epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba96879a",
   "metadata": {},
   "source": [
    "모델을 저장하기 위한 checkpoint를 만들어보자. optimizer와 생성자, 판별자를 모두 넣어 저장하는데 이는 생성자 판별자가 학습한 모델 가중치를 저장하는 것이라 보면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a685af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = os.getenv('HOME')+'/aiffel/dcgan_newimage/fashion/training_checkpoints'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95edb253",
   "metadata": {},
   "source": [
    "# 학습 시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f48c247",
   "metadata": {},
   "source": [
    "위에서 선언한 한 단계 학습하는 `train_step`, 샘플 이미지 저장하는 `generate_and_save_images()`, 학습 과정을 시각화하는 `draw_train_history()`, 모델 저장하기 위한 `checkpoint`을 활용해서 합쳐 학습해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d93900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs, save_every):\n",
    "    start = time.time()\n",
    "    history = {'gen_loss':[], 'disc_loss':[], 'real_accuracy':[], 'fake_accuracy':[]}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        for it, image_batch in enumerate(dataset):\n",
    "            gen_loss, disc_loss, real_accuracy, fake_accuracy = train_step(image_batch)\n",
    "            history['gen_loss'].append(gen_loss)\n",
    "            history['disc_loss'].append(disc_loss)\n",
    "            history['real_accuracy'].append(real_accuracy)\n",
    "            history['fake_accuracy'].append(fake_accuracy)\n",
    "\n",
    "            if it % 50 == 0:\n",
    "                display.clear_output(wait=True)\n",
    "                generate_and_save_images(generator, epoch+1, it+1, seed)\n",
    "                print('Epoch {} | iter {}'.format(epoch+1, it+1))\n",
    "                print('Time for epoch {} : {} sec'.format(epoch+1, int(time.time()-epoch_start)))\n",
    "\n",
    "        if (epoch + 1) % save_every == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator, epochs, it, seed)\n",
    "        print('Time for training : {} sec'.format(int(time.time()-start)))\n",
    "\n",
    "        draw_train_history(history, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660da21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_every = 5\n",
    "EPOCHS = 50\n",
    "\n",
    "# 사용가능한 GPU 디바이스 확인\n",
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd244bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train(train_dataset, EPOCHS, save_every)\n",
    "\n",
    "# 학습과정의 loss, accuracy 그래프 이미지 파일이 ~/aiffel/dcgan_newimage/fashion/training_history 경로에 생성되고 있으니\n",
    "# 진행 과정을 수시로 확인해 보시길 권합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d33736",
   "metadata": {},
   "source": [
    "저장되는 그래프의 이미지들을 보면서 유추해볼 수 있다. 생성자가 만든 fake image에 대해 판별자의 accuracy가 1에 가깝게 유지되면 생성자가 만든 이미지가 판별자를 속이지 못하고 있다는 뜻이다.   \n",
    "더 나은 결과물을 만들기 위해 많은 epoch로 학습을 시키면 이 전에 없던 새로운 디자인의 패션을 만들어 내는 것을 확인해볼 수 있다. 에포크만 늘리기보다 모델 구조나 학습 방법을 바꿔야할 수도 있으므로 그래프를 지속적으로 살펴보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe456b0",
   "metadata": {},
   "source": [
    "### 학습 과정 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a16386",
   "metadata": {},
   "source": [
    "학습이 끝나고 생성한 샘플 이미지들을 합쳐 GIF파일로 만들어보자. `imageio` 라이브러리를 활용해 만들것이다. `imageio.get_writer`로 파일을 열고 `append_data`로 이미지를 붙여나가는 방식이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6081b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_file = os.getenv('HOME')+'/aiffel/dcgan_newimage/fashion/fashion_mnist_dcgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "    filenames = glob.glob('{}/aiffel/dcgan_newimage/fashion/generated_samples/sample*.png'.format(os.getenv('HOME')))\n",
    "    filenames = sorted(filenames)\n",
    "    last = -1\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frame = 2*(i**0.5)\n",
    "        if round(frame) > round(last):\n",
    "            last = frame\n",
    "        else:\n",
    "            continue\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "\n",
    "!ls -l ~/aiffel/dcgan_newimage/fashion/fashion_mnist_dcgan.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f311cb91",
   "metadata": {},
   "source": [
    "# CIFAR-10 이미지 생성 프로젝트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ccddc1",
   "metadata": {},
   "source": [
    "앞에서 배운 FASHION-MNIST 데이터셋과 DCGAN 모델구조를 이용해서 CIFAR-10 데이터를 생성하는 모델을 만들어보자. 몇 가지 달라지는 점이 있다.   \n",
    "- image data shape가 (28, 28, 1) -> (32, 32, 3)으로 변경된다.\n",
    "- 단색의 grayscale이 RGB 3채널의 컬러 이미지로 변경된다.\n",
    "- 입력데이터 차원이 3~4배 증가하면서 학습 진행되는 양상이 달라진다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98a3b8c",
   "metadata": {},
   "source": [
    "1. 작업환경 구성하기\n",
    "2. 데이터셋 구성하기\n",
    "3. 생성자 모델 구현하기\n",
    "4. 판별자 모델 구현하기\n",
    "5. 손실함수와 최적화 함수 구현하기\n",
    "6. 훈련과정 상세 기능 구현하기\n",
    "7. 학습 과정 진행하기\n",
    "8. (optional) GAN 훈련 과정 개선하기\n",
    "\n",
    "참고할만한 페이지들\n",
    "- [How to Train a GAN? Tips and tricks to make GANs work](https://github.com/soumith/ganhacks)\n",
    "- [10 Lessons I Learned Training GANs for one Year](https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628)\n",
    "- [Tips for Training Stable Generative Adversarial Networks](https://machinelearningmastery.com/how-to-train-stable-generative-adversarial-networks/)\n",
    "- [Improved Techniques for Training GANs(paper)](https://proceedings.neurips.cc/paper/2016/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
