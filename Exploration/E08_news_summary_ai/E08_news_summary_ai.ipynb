{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e50301ba",
   "metadata": {},
   "source": [
    "# 8. 뉴스 요약 봇 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b5e6d3",
   "metadata": {},
   "source": [
    "텍스트 요약의 핵심은 정보를 압축한 후에 정보 손실 발생이 최소화되어야한다. 요약 문장을 만들어 내는 방법은 크게 추출적 요약(Extractive Summarization), 추상적 요약(Abstactive Summarization) 두 가지 접근으로 나눌 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c964955",
   "metadata": {},
   "source": [
    "- 추출적 요약(Extractive Summarization)   \n",
    "원문의 문장들을 추출해서 요약하는 방식이다. 예를들어 10개의 문장으로 구성된 문단에서 핵심 문장 3개를 꺼내 요악문을 만드는 방식이다. 하지만 이들이 자연스럽지 않을 수 있다. 딥러닝보다 머신러닝 방식에 속하는 텍스트 랭크같은 알고리즘을 사용해서 이 방법을 사용한다. 예를들어 네이버 뉴스의 요약봇이 이에 해당한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc562acf",
   "metadata": {},
   "source": [
    "- 추상적 요약(Abstacktive Summarization)   \n",
    "원문에서 새로운 문장을 생성해내는 방식이다. 자연어 생성(Natural Language Generation, NLG)의 영역이다. 문단에서 핵심문장을 판단하는 점에서 문장 분류 문제로 볼 수 있다.   \n",
    "RNN을 이용해서 전에 문장 만들기를 해봤는데 앞에 있는 정보보다 뒤로 갈수록 중요도가 올라가는 장기 의존성 문제 때문에 이를 사용하긴 어려워 보인다.   \n",
    "[구글 인공지능이 만든 해드라인 기사](https://zdnet.co.kr/view/?no=20160905114833&from=Mobile)를 보면 이런 문제를 해결했는데 추상적 요약이라 불리는 '시퀀스 투 시퀀스' 방식을 사용했다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1380b040",
   "metadata": {},
   "source": [
    "# seq2seq\n",
    "![](https://miro.medium.com/max/1360/1*Cu49wPEpWJPoI0a5AV9Q1Q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f36b7a",
   "metadata": {},
   "source": [
    "첫번째 RNN인 인코더를 문맥 정보를 갖고있는 컨텍스트 벡터로 변환하고 고정한다. 두번째 RNN 디코더부터 컨텍스트 벡터를 입력받아 한 단어씩 생성해 요약 문장을 완성하는 과정이다.   \n",
    "seq2seq를 구현할 때 바닐라 RNN이 아닌 LSTM을 사용해보자. 특징은 time step의 셀에 hidden state, cell state가 같이 전달한다.   \n",
    "seq2seq 구조에서 디코더는 시작 토큰 SOS가 입력되면 종료 토큰 EOS를 예측하는 순간까지 멈추지 않고 진행한다.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee6f54c",
   "metadata": {},
   "source": [
    "![](https://wikidocs.net/images/page/22893/dotproductattention1_final.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703ac4eb",
   "metadata": {},
   "source": [
    "seq2seq 인코더의 마지막 time step의 hidden state를 컨텍스트 벡터로 사용했다. 하지만 RNN 계열의 한계로 많은 정보가 손실된 상태이다. 그래서 __어텐션 메커니즘을__ 사용해서 인코더의 모든 step이 hidden state의 컨텍스트 벡터에 반영되도록 하자.   \n",
    "주의할점은 디코더의 스텝에 따라 인코더의 hidden state의 가중치 값이 달라진다. 기본적인 seq2seq 모델의 컨텍스트 벡터는 위치와 무관하게 한번 계산되면 고정값을 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00f8012",
   "metadata": {},
   "source": [
    "# 데이터 준비하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e67a90",
   "metadata": {},
   "source": [
    "Kaggle에 제공된 아마존 리뷰 데이터셋을 사용할 예정이다. 또한 NLTK의 stopwords를 사용할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b2244a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /aiffel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import urllib.request\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56f8d99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 100000\n"
     ]
    }
   ],
   "source": [
    "# Reviews.csv의 총 568,454개중 10만개만 사용\n",
    "data = pd.read_csv(os.getenv(\"HOME\")+\"/aiffel/news_summarization/data/Reviews.csv\", nrows=100000)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7489d11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bd23f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32179</th>\n",
       "      <td>Pops well with air popper. Decent flavor (thou...</td>\n",
       "      <td>Solid choice for popcorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10883</th>\n",
       "      <td>So the hottest-hot-off-the-press recommendatio...</td>\n",
       "      <td>wow  really</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44265</th>\n",
       "      <td>When I was opening the boxes I noticed there w...</td>\n",
       "      <td>Not All French Roast KCups</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36653</th>\n",
       "      <td>I am currently on a No Carb diet and bought th...</td>\n",
       "      <td>GREAT AFTER A FEW RINSES!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82951</th>\n",
       "      <td>San Francisco Bay Coffee OneCup for Keurig K-C...</td>\n",
       "      <td>San Francisco Bay Coffee for Keurig Brewer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60228</th>\n",
       "      <td>I enjoy these wintergreen flavored lifesavers....</td>\n",
       "      <td>Minty fresh breath</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48940</th>\n",
       "      <td>I've been using this product for the last thre...</td>\n",
       "      <td>Amazing Seeds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45180</th>\n",
       "      <td>&lt;a href=\"http://www.amazon.com/gp/product/B000...</td>\n",
       "      <td>a hope full wish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1676</th>\n",
       "      <td>Great low cal snack... satisfies the crunchy, ...</td>\n",
       "      <td>Love these!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19362</th>\n",
       "      <td>Bought a small SmartBones chicken dog chew for...</td>\n",
       "      <td>Made in China!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34514</th>\n",
       "      <td>I really enjoyed this coffee. I am new to the ...</td>\n",
       "      <td>Caribou Coffee Is Amazing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90368</th>\n",
       "      <td>TASTES KIND OF CHALKY AND LEAVES A VERY FUNNY ...</td>\n",
       "      <td>HORRIBLE AFTERTASTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74463</th>\n",
       "      <td>I have always loved these chips and buy them o...</td>\n",
       "      <td>Disappointed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35502</th>\n",
       "      <td>This seasoning is great with ground Turkey (or...</td>\n",
       "      <td>Spicy and loaded with Sabor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37652</th>\n",
       "      <td>bought these at costco for a few cents less an...</td>\n",
       "      <td>very tasty</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  \\\n",
       "32179  Pops well with air popper. Decent flavor (thou...   \n",
       "10883  So the hottest-hot-off-the-press recommendatio...   \n",
       "44265  When I was opening the boxes I noticed there w...   \n",
       "36653  I am currently on a No Carb diet and bought th...   \n",
       "82951  San Francisco Bay Coffee OneCup for Keurig K-C...   \n",
       "60228  I enjoy these wintergreen flavored lifesavers....   \n",
       "48940  I've been using this product for the last thre...   \n",
       "45180  <a href=\"http://www.amazon.com/gp/product/B000...   \n",
       "1676   Great low cal snack... satisfies the crunchy, ...   \n",
       "19362  Bought a small SmartBones chicken dog chew for...   \n",
       "34514  I really enjoyed this coffee. I am new to the ...   \n",
       "90368  TASTES KIND OF CHALKY AND LEAVES A VERY FUNNY ...   \n",
       "74463  I have always loved these chips and buy them o...   \n",
       "35502  This seasoning is great with ground Turkey (or...   \n",
       "37652  bought these at costco for a few cents less an...   \n",
       "\n",
       "                                          Summary  \n",
       "32179                    Solid choice for popcorn  \n",
       "10883                                 wow  really  \n",
       "44265                  Not All French Roast KCups  \n",
       "36653                   GREAT AFTER A FEW RINSES!  \n",
       "82951  San Francisco Bay Coffee for Keurig Brewer  \n",
       "60228                          Minty fresh breath  \n",
       "48940                               Amazing Seeds  \n",
       "45180                            a hope full wish  \n",
       "1676                                  Love these!  \n",
       "19362                            Made in China!!!  \n",
       "34514                   Caribou Coffee Is Amazing  \n",
       "90368                         HORRIBLE AFTERTASTE  \n",
       "74463                                Disappointed  \n",
       "35502                 Spicy and loaded with Sabor  \n",
       "37652                                  very tasty  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['Text','Summary']]\n",
    "data.head()\n",
    "\n",
    "#랜덤한 15개 샘플 출력\n",
    "data.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90453ef4",
   "metadata": {},
   "source": [
    "# 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47370b8",
   "metadata": {},
   "source": [
    "## 데이터 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62ec11c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 열에서 중복을 배제한 유일한 샘플의 수 : 88426\n",
      "Summary 열에서 중복을 배제한 유일한 샘플의 수 : 72348\n"
     ]
    }
   ],
   "source": [
    "print('Text 열에서 중복을 배제한 유일한 샘플의 수 :', data['Text'].nunique())\n",
    "print('Summary 열에서 중복을 배제한 유일한 샘플의 수 :', data['Summary'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536954ab",
   "metadata": {},
   "source": [
    "중복을 제외하면 Text는 88426개 Summary는 72348개가 나온다.   \n",
    "`drop_duplicates()`를 사용해서 중복 샘플 제거하기, `dropna()`를 사용해서 Null 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4767b44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88426\n"
     ]
    }
   ],
   "source": [
    "# inplace=True 를 설정하면 DataFrame 타입 값을 return 하지 않고 data 내부를 직접적으로 바꾼다\n",
    "data.drop_duplicates(subset = ['Text'], inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18e619e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text       0\n",
      "Summary    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# null값 확인하기\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8752fc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88425\n"
     ]
    }
   ],
   "source": [
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b77944",
   "metadata": {},
   "source": [
    "## 텍스트 정규화(text normalization)   \n",
    "- 같은 의미인데 다른 단어 정규화   \n",
    "사전은 [정규화 사전 출저](https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python) 참고\n",
    "- 자주 등장하지만 도움이 되지 않는 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "304f96c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화 사전의 수:  120\n"
     ]
    }
   ],
   "source": [
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "print(\"정규화 사전의 수: \", len(contractions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a96cfd8",
   "metadata": {},
   "source": [
    "print('불용어 개수 :', len(stopwords.words('english') ))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dca5ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 함수\n",
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
    "    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
    "    sentence = re.sub(r\"'s\\b\",\"\", sentence) # 소유격 제거. Ex) roland's -> roland\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
    "    \n",
    "    # 불용어 제거 (Text)\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n",
    "    # 불용어 미제거 (Summary)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7acabd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  everything bought great infact ordered twice third ordered wasfor mother father\n",
      "summary: great way to start the day\n"
     ]
    }
   ],
   "source": [
    "# 임의의 예시를 만들어서 확인해보기\n",
    "temp_text = 'Everything I bought was great, infact I ordered twice and the third ordered was<br />for my mother and father.'\n",
    "temp_summary = 'Great way to start (or finish) the day!!!'\n",
    "\n",
    "print(\"text: \", preprocess_sentence(temp_text))\n",
    "print(\"summary:\", preprocess_sentence(temp_summary, False))  # 불용어를 제거하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "125bc7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 전처리 후 결과:  ['bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better', 'product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo', 'confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story lewis lion witch wardrobe treat seduces edmund selling brother sisters witch', 'looking secret ingredient robitussin believe found got addition root beer extract ordered made cherry soda flavor medicinal', 'great taffy great price wide assortment yummy taffy delivery quick taffy lover deal']\n"
     ]
    }
   ],
   "source": [
    "clean_text = []\n",
    "# 전체 Text 데이터에 대한 전처리 : 10분 이상 시간이 걸릴 수 있다\n",
    "for s in data['Text']:\n",
    "    clean_text.append(preprocess_sentence(s))\n",
    "\n",
    "# 전처리 후 출력\n",
    "print(\"Text 전처리 후 결과: \", clean_text[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ef5bf6",
   "metadata": {},
   "source": [
    "Summary에 전처리를 할때는 불용어 제거를 하지 않는다는 뜻에서 두번째 인자로 False 넣어주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7835403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary 전처리 후 결과:  ['good quality dog food', 'not as advertised', 'delight says it all', 'cough medicine', 'great taffy']\n"
     ]
    }
   ],
   "source": [
    "clean_summary = []\n",
    "# 전체 Summary 데이터에 대한 전처리 : 5분 이상 시간이 걸릴 수 있습니다. \n",
    "for s in data['Summary']:\n",
    "    clean_summary.append(preprocess_sentence(s, False))\n",
    "\n",
    "print(\"Summary 전처리 후 결과: \", clean_summary[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74dd2128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "# 전처리하고 empty 샘플들 확인 후 Null 값 넣기\n",
    "data['Text'] = clean_text\n",
    "data['Summary'] = clean_summary\n",
    "\n",
    "# 빈 값을 Null 값으로 변환\n",
    "data.replace('', np.nan, inplace=True)\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa6585f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text        0\n",
       "Summary    70\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94187ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88355\n"
     ]
    }
   ],
   "source": [
    "# 생긴 Null값들 제거\n",
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67ca210",
   "metadata": {},
   "source": [
    "## 샘플의 최대 길이 정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d55ab14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트의 최소 길이 : 2\n",
      "텍스트의 최대 길이 : 1235\n",
      "텍스트의 평균 길이 : 38.792428272310566\n",
      "요약의 최소 길이 : 1\n",
      "요약의 최대 길이 : 28\n",
      "요약의 평균 길이 : 4.010729443721352\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 길이 분포 출력\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_len = [len(s.split()) for s in data['Text']]\n",
    "summary_len = [len(s.split()) for s in data['Summary']]\n",
    "\n",
    "print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "print('요약의 최소 길이 : {}'.format(np.min(summary_len)))\n",
    "print('요약의 최대 길이 : {}'.format(np.max(summary_len)))\n",
    "print('요약의 평균 길이 : {}'.format(np.mean(summary_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(text_len)\n",
    "plt.title('Text')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(summary_len)\n",
    "plt.title('Summary')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Text')\n",
    "plt.hist(text_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Summary')\n",
    "plt.hist(summary_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833f529d",
   "metadata": {},
   "source": [
    "Text의 최소 길이는 2 최대 길이는 1235로 차이가 크다 평균 길이는 38이고 대체적으로 100내외의 길이를 가진다.   \n",
    "Summary의 경우 최소는 1, 최대 28 평균이 4로 상대적으로 많이 짧고 대체적으로 10 이하의 길이를 갖고 있다.   \n",
    "최대 길이를 임의로 정한 후 데이터의 몇 퍼센트에 해당하는지 확인해보고 괜찮으면 정제하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8b47c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "text_max_len = 50\n",
    "summary_max_len = 8\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9c9c49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "  cnt = 0\n",
    "  for s in nested_list:\n",
    "    if(len(s.split()) <= max_len):\n",
    "        cnt = cnt + 1\n",
    "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51b0affd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 50 이하인 샘플의 비율: 0.7745119121724859\n",
      "전체 샘플 중 길이가 8 이하인 샘플의 비율: 0.9424593967517402\n"
     ]
    }
   ],
   "source": [
    "below_threshold_len(text_max_len, data['Text'])\n",
    "below_threshold_len(summary_max_len,  data['Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d52940d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 65818\n"
     ]
    }
   ],
   "source": [
    "data = data[data['Text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
    "data = data[data['Summary'].apply(lambda x: len(x.split()) <= summary_max_len)]\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e0d9b1",
   "metadata": {},
   "source": [
    "## 시작 토큰, 종료 토큰 추가하기\n",
    "앞서 말한것처럼 seq2seq 훈련을 위해 디코더의 시작과 종료 토큰을 추가해줘야한다. 시작 토큰 `SOS, sostoken`, 종료 토큰 `EOS, eostoken`라 명명하고 추가해주자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9404ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "      <th>decoder_input</th>\n",
       "      <th>decoder_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "      <td>good quality dog food</td>\n",
       "      <td>sostoken good quality dog food</td>\n",
       "      <td>good quality dog food eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product arrived labeled jumbo salted peanuts p...</td>\n",
       "      <td>not as advertised</td>\n",
       "      <td>sostoken not as advertised</td>\n",
       "      <td>not as advertised eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>confection around centuries light pillowy citr...</td>\n",
       "      <td>delight says it all</td>\n",
       "      <td>sostoken delight says it all</td>\n",
       "      <td>delight says it all eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "      <td>cough medicine</td>\n",
       "      <td>sostoken cough medicine</td>\n",
       "      <td>cough medicine eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "      <td>great taffy</td>\n",
       "      <td>sostoken great taffy</td>\n",
       "      <td>great taffy eostoken</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                Summary  \\\n",
       "0  bought several vitality canned dog food produc...  good quality dog food   \n",
       "1  product arrived labeled jumbo salted peanuts p...      not as advertised   \n",
       "2  confection around centuries light pillowy citr...    delight says it all   \n",
       "3  looking secret ingredient robitussin believe f...         cough medicine   \n",
       "4  great taffy great price wide assortment yummy ...            great taffy   \n",
       "\n",
       "                    decoder_input                  decoder_target  \n",
       "0  sostoken good quality dog food  good quality dog food eostoken  \n",
       "1      sostoken not as advertised      not as advertised eostoken  \n",
       "2    sostoken delight says it all    delight says it all eostoken  \n",
       "3         sostoken cough medicine         cough medicine eostoken  \n",
       "4            sostoken great taffy            great taffy eostoken  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary에 시작 토큰과 종료 토큰을 추가한다.\n",
    "data['decoder_input'] = data['Summary'].apply(lambda x : 'sostoken '+ x)\n",
    "data['decoder_target'] = data['Summary'].apply(lambda x : x + ' eostoken')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "034a39f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "# Numpy 타입으로 저장\n",
    "encoder_input = np.array(data['Text']) # 인코더의 입력\n",
    "decoder_input = np.array(data['decoder_input']) # 디코더의 입력\n",
    "decoder_target = np.array(data['decoder_target']) # 디코더의 레이블\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354224e6",
   "metadata": {},
   "source": [
    "## 훈련 데이터와 테스트 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d379c58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  418 24742 38672 ... 18953 55607 14944]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad543616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a158f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터의 수 : 13163\n"
     ]
    }
   ],
   "source": [
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "print('테스트 데이터의 수 :', n_of_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a19fda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 개수 : 52655\n",
      "훈련 레이블의 개수 : 52655\n",
      "테스트 데이터의 개수 : 13163\n",
      "테스트 레이블의 개수 : 13163\n"
     ]
    }
   ],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50161a33",
   "metadata": {},
   "source": [
    "## 정수 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578ae027",
   "metadata": {},
   "source": [
    "단어들을 정수로 바꿔주는 맵핑 작업이 필요하다. 이를 단어 집합(vocabulary)을 만든다고 표현한다.   \n",
    "Keras의 tokenizer를 이용하면 단어 집합을 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab32278e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "src_tokenizer = Tokenizer() # 토크나이저 정의\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0b0915",
   "metadata": {},
   "source": [
    "등장 빈도수가 7회 미마인 데이터들의 비중을 확인하기   \n",
    "`src_tokenizer.word_counts.items()`를 확인하면 등장 빈도수가 저장되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e60e9813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 32061\n",
      "등장 빈도가 6번 이하인 희귀 단어의 수: 23823\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 8238\n",
      "단어 집합에서 희귀 단어의 비율: 74.3052306540657\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 3.4066122840085984\n"
     ]
    }
   ],
   "source": [
    "threshold = 7\n",
    "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5691c07d",
   "metadata": {},
   "source": [
    "32086개의 단어로 6회 이하 등장한 단어들은 74%나 된다. 하지만 훈련 데이터에서 등장 빈도로 차지하는 비율은 3.39%밖에 되지 않는다. 그래서 6회 이하의 단어들은 인코딩 과정에서 빼고 훈련 데이터에서 제거하고 진행해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b316d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "src_vocab = 8000 # 단어 집합의 크기를 8,000으로 제한\n",
    "src_tokenizer = Tokenizer(num_words=src_vocab) \n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8312d2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5681, 864, 1248, 111, 3, 219, 6449, 3739, 53, 513, 127, 47, 1248, 111, 163, 50, 1107, 5895, 7, 151, 507, 159, 89, 3910, 111, 14, 163, 3, 9, 185], [407, 194, 26, 38, 165, 2, 891, 356, 440, 168, 4500, 1791, 2649, 99, 165, 91, 203, 2650, 107, 12, 67, 1776], [47, 957, 936, 189, 161, 47, 807, 344, 3541, 1108, 39, 1620, 76, 1207, 47, 12, 189, 957, 47, 2]]\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
    "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
    "\n",
    "# 잘 진행되었는지 샘플 출력\n",
    "print(encoder_input_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5954d4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "# Summary 데이터에도 적용\n",
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c277f191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 10541\n",
      "등장 빈도가 5번 이하인 희귀 단어의 수: 8167\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 2374\n",
      "단어 집합에서 희귀 단어의 비율: 77.47841760743762\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 5.911211229750742\n"
     ]
    }
   ],
   "source": [
    "threshold = 6\n",
    "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tar_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7777a0",
   "metadata": {},
   "source": [
    "5회 이하인 단어들은 8077개인데 실제 훈련 데이터에서 등장 빈도의 비중은 5.87%밖에 되지 않는다. 그래서 어림잡아 2000개만 집합의 크기로 잡아 진행하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "231065a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "input  [[1, 3, 98], [1], [1, 498], [1, 80, 6, 1055, 5, 112], [1, 3, 136, 1962, 75]]\n",
      "target\n",
      "decoder  [[3, 98, 2], [2], [498, 2], [80, 6, 1055, 5, 112, 2], [3, 136, 1962, 75, 2]]\n"
     ]
    }
   ],
   "source": [
    "tar_vocab = 2000\n",
    "tar_tokenizer = Tokenizer(num_words=tar_vocab) \n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train) \n",
    "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
    "\n",
    "# 잘 변환되었는지 확인\n",
    "print('input')\n",
    "print('input ',decoder_input_train[:5])\n",
    "print('target')\n",
    "print('decoder ',decoder_target_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328a3ad7",
   "metadata": {},
   "source": [
    "정수 인코딩 작업이 끝났다. 하지만 체크해야할 점이 있다. 빈도수가 낮은 단어로만 구성된 샘플들은 empty 샘플이 되었을 가능성이 있으므로 확인해보자.   \n",
    "decoder_input에는 sostoken, decoder_target에는 eostoken이 추가된 상태이므로 이를 주의하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90d83b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제할 훈련 데이터의 개수 : 1283\n",
      "삭제할 테스트 데이터의 개수 : 325\n",
      "훈련 데이터의 개수 : 51372\n",
      "훈련 레이블의 개수 : 51372\n",
      "테스트 데이터의 개수 : 12838\n",
      "테스트 레이블의 개수 : 12838\n"
     ]
    }
   ],
   "source": [
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
    "\n",
    "print('삭제할 훈련 데이터의 개수 :', len(drop_train))\n",
    "print('삭제할 테스트 데이터의 개수 :', len(drop_test))\n",
    "\n",
    "encoder_input_train = [sentence for index, sentence in enumerate(encoder_input_train) if index not in drop_train]\n",
    "decoder_input_train = [sentence for index, sentence in enumerate(decoder_input_train) if index not in drop_train]\n",
    "decoder_target_train = [sentence for index, sentence in enumerate(decoder_target_train) if index not in drop_train]\n",
    "\n",
    "encoder_input_test = [sentence for index, sentence in enumerate(encoder_input_test) if index not in drop_test]\n",
    "decoder_input_test = [sentence for index, sentence in enumerate(decoder_input_test) if index not in drop_test]\n",
    "decoder_target_test = [sentence for index, sentence in enumerate(decoder_target_test) if index not in drop_test]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98d23f1",
   "metadata": {},
   "source": [
    "## 패딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31eedf45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "encoder_input_train = pad_sequences(encoder_input_train, maxlen=text_max_len, padding='post')\n",
    "encoder_input_test = pad_sequences(encoder_input_test, maxlen=text_max_len, padding='post')\n",
    "decoder_input_train = pad_sequences(decoder_input_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_train = pad_sequences(decoder_target_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_input_test = pad_sequences(decoder_input_test, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_test = pad_sequences(decoder_target_test, maxlen=summary_max_len, padding='post')\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8719ddc8",
   "metadata": {},
   "source": [
    "# 모델 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "535bfb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# 인코더 설계 시작\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(text_max_len,))\n",
    "\n",
    "# 인코더의 임베딩 층\n",
    "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "# 인코더의 LSTM 1\n",
    "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# 인코더의 LSTM 2\n",
    "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# 인코더의 LSTM 3\n",
    "encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d014288",
   "metadata": {},
   "source": [
    "임베딩 벡터의 차원은 128로 hidden state는 256으로 정했다. hidden state는 LSTM에서 capacity를 정하는 파라미터이다. 3개의 층을 지나서 인코더로부터 나온 벡터는 디코더로 보내줘야한다.   \n",
    "LSTM은 dropout뿐만 아니라 recurrent dropout까지 사용한다. 일반적인 dropout은 레이어의 weight를 랜덤으로 생각하여 overfitting을 저지해주지만 이는 레이어가 아닌 time step마다 해주는 방식이다. 그래서 regularization 효과와 overfitting을 방지할 수 있다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15fff5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "# 디코더 설계\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 디코더의 임베딩 층\n",
    "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 디코더의 LSTM\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c]) # initial_state에 hidden state, cell state값 넣어주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f00902f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 128)      1024000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    256000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 2000)   514000      lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,633,104\n",
      "Trainable params: 3,633,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a0f03c",
   "metadata": {},
   "source": [
    "# 어텐션 메커니즘\n",
    "텐서플로우에 이미 구현된 Bahdanau 스타일 어텐션을 가져와서 적용해보자. [텐서플로우 bahdanau docs](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AdditiveAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37842374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 128)      1024000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    256000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AdditiveAttent (None, None, 256)    256         lstm_3[0][0]                     \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 512)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 2000)   1026000     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 4,145,360\n",
      "Trainable params: 4,145,360\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import AdditiveAttention\n",
    "\n",
    "# 어텐션 층(어텐션 함수)\n",
    "attn_layer = AdditiveAttention(name='attention_layer')\n",
    "\n",
    "# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
    "attn_out = attn_layer([decoder_outputs, encoder_outputs])\n",
    "\n",
    "\n",
    "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f687b",
   "metadata": {},
   "source": [
    "# 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace2cdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "201/201 [==============================] - 138s 643ms/step - loss: 2.7051 - val_loss: 2.4177\n",
      "Epoch 2/50\n",
      "201/201 [==============================] - 126s 629ms/step - loss: 2.3806 - val_loss: 2.2797\n",
      "Epoch 3/50\n",
      "201/201 [==============================] - 128s 636ms/step - loss: 2.2338 - val_loss: 2.1535\n",
      "Epoch 4/50\n",
      "201/201 [==============================] - 129s 642ms/step - loss: 2.1121 - val_loss: 2.0645\n",
      "Epoch 5/50\n",
      "201/201 [==============================] - 130s 645ms/step - loss: 2.0293 - val_loss: 2.0142\n",
      "Epoch 6/50\n",
      "201/201 [==============================] - 127s 633ms/step - loss: 1.9672 - val_loss: 1.9766\n",
      "Epoch 7/50\n",
      "201/201 [==============================] - 128s 637ms/step - loss: 1.9143 - val_loss: 1.9438\n",
      "Epoch 8/50\n",
      "200/201 [============================>.] - ETA: 0s - loss: 1.8675"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test), \\\n",
    "          batch_size=256, callbacks=[es], epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8855cd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac0246d",
   "metadata": {},
   "source": [
    "# 인퍼런스 모델 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8e030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
    "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
    "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722b7f08",
   "metadata": {},
   "source": [
    "seq2seq는 훈련할 때와 실제 동작하는 인퍼런스 단계의 방식이 다르므로 그에 맞게 모델 설계를 별개로 진행해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bada6da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 설계\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cce612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어텐션 함수\n",
    "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
    "attn_out_inf = attn_layer([decoder_outputs2, decoder_hidden_state_input])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
    "\n",
    "# 최종 디코더 모델\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dc51b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인퍼런스 단계에서 단어 시퀀스를 완성하는 함수\n",
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "     # <SOS>에 해당하는 토큰 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n",
    "\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = tar_index_to_word[sampled_token_index]\n",
    "\n",
    "        if (sampled_token!='eostoken'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        #  <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (summary_max_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 상태를 업데이트 합니다.\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c53634",
   "metadata": {},
   "source": [
    "# 모델 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d7837d",
   "metadata": {},
   "source": [
    "주어진 정수 시퀀스를 텍스트 시퀀스로 변환하여 결과를 확인하는 함수 만들기   \n",
    "Text의 정수 시퀀스에서 패딩을 위해 사용한 숫자 0 제외하고 Summary의 정수 시퀀스에서는 숫자 0, 시작, 종료 토큰 인덱스를 출력에서 제외하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3a39f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2text(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if (i!=0):\n",
    "            temp = temp + src_index_to_word[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2summary(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if ((i!=0 and i!=tar_word_to_index['sostoken']) and i!=tar_word_to_index['eostoken']):\n",
    "            temp = temp + tar_index_to_word[i] + ' '\n",
    "    return temp\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2abbbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50, 100):\n",
    "    print(\"원문 :\", seq2text(encoder_input_test[i]))\n",
    "    print(\"실제 요약 :\", seq2summary(decoder_input_test[i]))\n",
    "    print(\"예측 요약 :\", decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e985f4f2",
   "metadata": {},
   "source": [
    "테스트 데이터에서 50개의 샘플의 결과를 확인하자 기존 요약과는 다른 요약을 출력하면서 원문의 내용을 담고 있는 의미 있는 요약들이 보인다. 워드 임베딩과 RNN의 조합으로 이뤄냈다.   \n",
    "그러나 좋지 않은 요약의 예도 있다. 성능 개선하기 위해 어텐션 조합을 수정하거나, 빔 서치(beam search), 사전 훈련된 워드 임베딩, 인코더와 디코더 자체의 구조를 변경하는 트랜스포머같은 개선 방안들이 존재한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9228a396",
   "metadata": {},
   "source": [
    "# 추출적 요약 해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b4bfd8",
   "metadata": {},
   "source": [
    "패키지 `Summa`에선 추출적 요약 모듈인 summarize를 제공해서 이를 이용해보자.   \n",
    "> $ pip list | grep summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fff1fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from summa.summarizer import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c59d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 매트릭스 시놉시스 다운하기\n",
    "text = requests.get('http://rare-technologies.com/the_matrix_synopsis.txt').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cd04dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ae36e7",
   "metadata": {},
   "source": [
    "## summarize 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b03275",
   "metadata": {},
   "source": [
    "summarize()의 인자로 사용되는 값들을 알아보자\n",
    "> text(str): 요약할 테스트\n",
    "ratio(float, optional): 요약문에서 원본으로 선택되는 문장 비율 0~1\n",
    "words(int or None, optional): 출력에 포함할 단어 수, 만약 ratio와 함께 두 파라미터가 모두 제공되면 ratio는 무시한다.\n",
    "split(bool, optional): True면 문장 list/False는 join된 문자열 반환한다\n",
    "\n",
    "문장 토큰화를 별도로 진행하지 않아도 내부적으로 문장 토큰화를 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b17723",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Summary:')\n",
    "print(summarize(text, ratio=0.005)) # 원문의 0.005% 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf8442",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Summary:')\n",
    "print(summarize(text, ratio=0.005, split=True)) # 리스트로 받기위해 split=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dc3c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Summary:')\n",
    "print(summarize(text, words=50)) # 단어의 수로 요약문의 크기 조절하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eced1895",
   "metadata": {},
   "source": [
    "# 뉴스 기사 요약해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84989aa5",
   "metadata": {},
   "source": [
    "[News_Summary 사용하기](https://github.com/sunnysai12345/News_Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7d4fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary_more.csv\", filename=\"news_summary_more.csv\")\n",
    "data = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')\n",
    "\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff0fb9e",
   "metadata": {},
   "source": [
    "text와 headlines 두 가지 열로 구성되어 있다. 추상적 요약을 하는 경우에 text를 본문, headlines를 요약된 데이터로 삼아 모델을 학습할 수 있다. 추출적 요약을 사용하면 text열만 사용하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160abada",
   "metadata": {},
   "source": [
    "- Step 2. 데이터 전처리하기(추상적 요약하기)\n",
    "- Step 3. 어텐션 메커니즘 사용하기(추상적 요약)\n",
    "- Step 4. 실제 결과와 요약문 비교하기(추상적 요약)\n",
    "- Step 5. Summa를 이용한 추출적 요약 해보기"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
