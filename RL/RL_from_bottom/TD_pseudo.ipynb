{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD_pseudo.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "xXkSCgwrZKid",
        "XES5fHqeeZHX"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CHAPTER 5. MDPë¥¼ ëª¨ë¥¼ ë•Œ value í‰ê°€í•˜ê¸°\n",
        "TD(Temporal Difference)\n",
        "\n",
        "MCì˜ ë‹¨ì ìœ¼ë¡œ ì¬ê·€ì ì¸ íŠ¹ì§•ì„ ê°–ê³  ìˆì–´, ì—í”¼ì†Œë“œê°€ ëë‚˜ì•¼ returnê°’ì„ ì–»ì–´ ì—…ë°ì´íŠ¸ë¥¼ í•  ìˆ˜ ìˆë‹¤.   \n",
        "TDëŠ” ìƒíƒœ ê°€ì¹˜ì˜ ê¸°ëŒ€ê°’ì„ ì˜ˆì¸¡í•˜ê³ , ì˜ˆì¸¡í•œ ê¸°ëŒ€ê°’ì„ í† ëŒ€ë¡œ ë‹¤ìŒ ë²ˆ ìƒíƒœ ê°€ì¹˜ì˜ ê¸°ëŒ€ê°’ì„ ì˜ˆì¸¡í•œë‹¤.   \n",
        "ì´ëŸ¬í•œ ë°©ì‹ì€ ê°’ì„ í•œ ê°œ ì”© ì–»ì„ ë•Œë§ˆë‹¤ ì—…ë°ì´íŠ¸ í•  ìˆ˜ ìˆë‹¤.\n",
        "\n",
        "$MC : V(s_t) â† V(s_t) + \\alpha(G_t-V(s_t))$   \n",
        "$TD : V(s_t) â† V(S_t) + \\alpha(r_{t+1}+\\gamma V(s_{t+1})-V(s_t))$\n",
        "\n",
        "---\n",
        "\n",
        "$V(s_0) â† V(s_0)+0.01*(-1+V(s_1)-V(s_0))$   \n",
        "$V(s_1) â† V(s_1)+0.01*(-1+V(s_2)-V(s_1))$"
      ],
      "metadata": {
        "id": "TY_gHuzgRRjV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pseudo-code\n",
        "> 1. í…Œì´ë¸”ì˜ ê°’ì„ ì´ˆê¸°í™” í•œë‹¤.\n",
        "2. agentê°€ policyì— ë”°ë¼ ê²½í—˜ì„ ìŒ“ëŠ”ë‹¤.\n",
        "3. ìƒíƒœ ì „ì´ê°€ ì¼ì–´ë‚˜ë©´ í…Œì´ë¸”ì˜ ê°’ì„ ì—…ë°ì´íŠ¸ í•´ì¤€ë‹¤.\n",
        "4. ì—í”¼ì†Œë“œë¥¼ ë§ˆì¹  ë•Œ ê¹Œì§€ 2~3ë²ˆì„ ë°˜ë³µí•œë‹¤.\n",
        "5. í…Œì´ë¸”ì˜ ê°’ì´ ìˆ˜ë ´í•  ë•Œ ê¹Œì§€ 4ë²ˆì„ ë°˜ë³µí•œë‹¤.\n",
        "6. ìˆ˜ë ´í•œ í…Œì´ë¸”ì˜ ê²°ê³¼ë¥¼ ì¶œë ¥í•´ì¤€ë‹¤."
      ],
      "metadata": {
        "id": "Tdo6q23LVYJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### grid world; env"
      ],
      "metadata": {
        "id": "xXkSCgwrZKid"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x6avG-aLP3ie"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class GridWorld():\n",
        "    def __init__(self):\n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "\n",
        "    def step(self, a):\n",
        "        # 0ë²ˆ ì•¡ì…˜: ì™¼ìª½, 1ë²ˆ ì•¡ì…˜: ìœ„, 2ë²ˆ ì•¡ì…˜: ì˜¤ë¥¸ìª½, 3ë²ˆ ì•¡ì…˜: ì•„ë˜ìª½\n",
        "        if a == 0:\n",
        "            self.move_left()\n",
        "        elif a == 1:\n",
        "            self.move_up()\n",
        "        elif a == 2:\n",
        "            self.move_right()\n",
        "        elif a == 3:\n",
        "            self.move_down()\n",
        "\n",
        "        reward = -1  # ë³´ìƒì€ í•­ìƒ -1ë¡œ ê³ ì •\n",
        "        done = self.is_done()\n",
        "        return (self.x, self.y), reward, done\n",
        "\n",
        "    def move_right(self):\n",
        "        self.y += 1\n",
        "        if self.y > 3:\n",
        "            self.y = 3\n",
        "\n",
        "    def move_left(self):\n",
        "        self.y -= 1\n",
        "        if self.y < 0:\n",
        "            self.y = 0\n",
        "\n",
        "    def move_up(self):\n",
        "        self.x -= 1\n",
        "        if self.x < 0:\n",
        "            self.x = 0\n",
        "\n",
        "    def move_down(self):\n",
        "        self.x += 1\n",
        "        if self.x > 3:\n",
        "            self.x = 3\n",
        "\n",
        "    def is_done(self):\n",
        "        if self.x == 3 and self.y == 3:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def get_state(self):\n",
        "        return (self.x, self.y)\n",
        "\n",
        "    def reset(self):\n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "        return (self.x, self.y)\n",
        "\n",
        "\n",
        "class Agent():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def select_action(self):\n",
        "        coin = random.random()\n",
        "        if coin < 0.25:\n",
        "            action = 0\n",
        "        elif coin < 0.5:\n",
        "            action = 1\n",
        "        elif coin < 0.75:\n",
        "            action = 2\n",
        "        else:\n",
        "            action = 3\n",
        "        return action"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TD"
      ],
      "metadata": {
        "id": "7Vu_b28eZOKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # TD\n",
        "    env = GridWorld()\n",
        "    agent = Agent()\n",
        "    # 1. í…Œì´ë¸”ì˜ ê°’ì„ ì´ˆê¸°í™” í•œë‹¤.\n",
        "    data = [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]] \n",
        "    gamma = 1.0\n",
        "    reward = -1\n",
        "    alpha = 0.01\n",
        "\n",
        "    # 5. í…Œì´ë¸”ì˜ ê°’ì´ ìˆ˜ë ´í•  ë•Œ ê¹Œì§€ 4ë²ˆì„ ë°˜ë³µí•œë‹¤.\n",
        "    for k in range(50000): \n",
        "        done = False\n",
        "        # 4. ì—í”¼ì†Œë“œë¥¼ ë§ˆì¹  ë•Œ ê¹Œì§€ 2~3ë²ˆì„ ë°˜ë³µí•œë‹¤.\n",
        "        while not done:\n",
        "            x, y = env.get_state()\n",
        "            # 2. agentê°€ policyì— ë”°ë¼ ê²½í—˜ì„ ìŒ“ëŠ”ë‹¤.\n",
        "            action = agent.select_action()\n",
        "            (x_prime, y_prime), reward, done = env.step(action)\n",
        "            x_prime, y_prime = env.get_state()\n",
        "            # 3. ìƒíƒœ ì „ì´ê°€ ì¼ì–´ë‚˜ë©´ í…Œì´ë¸”ì˜ ê°’ì„ ì—…ë°ì´íŠ¸ í•´ì¤€ë‹¤.\n",
        "            # ğ‘‡ğ·:ğ‘‰(ğ‘ ğ‘¡)â†ğ‘‰(ğ‘†ğ‘¡)+ğ›¼(ğ‘Ÿğ‘¡+1+ğ›¾ğ‘‰(ğ‘ ğ‘¡+1)âˆ’ğ‘‰(ğ‘ ğ‘¡))\n",
        "            data[x][y] = data[x][y] + alpha * (reward + gamma * data[x_prime][y_prime] - data[x][y])\n",
        "        env.reset()\n",
        "\n",
        "    # 6. ìˆ˜ë ´í•œ í…Œì´ë¸”ì˜ ê²°ê³¼ë¥¼ ì¶œë ¥í•´ì¤€ë‹¤.\n",
        "    for row in data:\n",
        "        print(row)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwxE487AZIWT",
        "outputId": "c63174c1-c885-4d92-b88d-55b560566673"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-56.79481916526043, -55.05562443397609, -51.89876910060371, -49.23394981558453]\n",
            "[-55.102270506271076, -52.656172367695795, -47.35562614796741, -42.69929713435356]\n",
            "[-51.477125961669415, -47.85166781122008, -39.7681151540909, -27.812210252522434]\n",
            "[-48.48950874923934, -42.95023283065192, -30.3256593893496, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CHAPTER 6. MDPë¥¼ ëª¨ë¥¼ ë•Œ optimal policy ì°¾ê¸°\n",
        "MDPë¥¼ ëª¨ë¥¼ ë•Œ optimal policyë¥¼ ì°¾ê¸° ìœ„í•´ TDë¥¼ baseë¡œ SARSAì™€ Q-learningì„ ì‚¬ìš©í•œë‹¤.   \n",
        "\n",
        "### on-policy, off-policy\n",
        "behavior policyëŠ” ë‹¤ìŒ stageë¡œ actionì„ í•˜ëŠ” policy, target policyëŠ” ë‹¤ìŒ sampleì„ ì–»ëŠ” policyë¥¼ ë§í•œë‹¤.   \n",
        "- behavior policy = target policy ì¸ ê²½ìš° on-policyë¼ê³  í•œë‹¤.   \n",
        "- behavior policy != target policy ì¸ ê²½ìš° off-policyë¼ê³  í•œë‹¤.\n",
        "\n",
        "off-policyë¥¼ ì“°ëŠ” ê°€ì¥ í° ì´ìœ ëŠ” ì¬í‰ê°€ê°€ ê°€ëŠ¥í•˜ë‹¤.\n",
        "\n",
        "### SARSA, Q-learning\n",
        "- SARSAëŠ” on-policyë¥¼ ì‚¬ìš©í•œë‹¤. ì´ë¦„ ê·¸ëŒ€ë¡œ Stateì—ì„œ Actionì„ ì„ íƒí•˜ë©´ Rewardë¥¼ ë°›ê³  State'ì— ë„ì°©í•˜ì—¬ ê±°ê¸°ì„œ ë‹¤ì‹œ Action'ì„ ì„ íƒí•œë‹¤.   \n",
        "- Q-learningì€ off-policyë¥¼ ì‚¬ìš©í•œë‹¤. ì´ëŠ” behavior policy(í–‰ë™ ì •ì±…)ê³¼ target policyì´ ë‹¤ë¥¸ ê²½ìš°ë¥¼ ë§í•œë‹¤.   \n",
        "Q_tableì—ì„œ target policyë¡œ expected ê°’ì„ ì–»ê³ , target policyë¡œ ì–»ì€ ì •ë³´ë¥¼ í† ëŒ€ë¡œ behavior policyë¡œ actioní•œë‹¤.\n",
        "\n",
        "TDê¸°ë°˜ SARSA í•™ìŠµ : $q_\\pi(s_t,a_t)=E_\\pi(r_{r+1}+\\gamma q_\\pi(s_{t+1},a_{t+1})]$   \n",
        "TDê¸°ë°˜ Q-learning í•™ìŠµ : $q_*(s,a)=E_{s'}[r+\\gamma \\underset{a'}{max}q_*(s',a')]$\n",
        "\n",
        "\n",
        "### SARSA ì—…ë°ì´íŠ¸ ì‹\n",
        "        self.q_table[x, y, a] = self.q_table[x, y, a] + 0.1 * (\n",
        "                    r + self.q_table[next_x, next_y, a_prime] - self.q_table[x, y, a])\n",
        "\n",
        "### QëŸ¬ë‹ ì—…ë°ì´íŠ¸ ì‹\n",
        "        self.q_table[x, y, a] = self.q_table[x, y, a] + 0.1 * (\n",
        "                    r + np.amax(self.q_table[next_x, next_y, :]) - self.q_table[x, y, a])\n"
      ],
      "metadata": {
        "id": "1_V5tvmFbYIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SARSA pseudo-code\n",
        "> 1. í…Œì´ë¸”, Q-tableì„ ì´ˆê¸°í™” í•œë‹¤.\n",
        "2. agentê°€ policyì— ë”°ë¼ ê²½í—˜ì„ ìŒ“ëŠ”ë‹¤.\n",
        "3. SARSA ì •ì±…ì„ í†µí•´ Q-tableì„ ì—…ë°ì´íŠ¸ í•œë‹¤.\n",
        "4. ì—í”¼ì†Œë“œë¥¼ ë§ˆì¹  ë•Œ ê¹Œì§€ 2~3ë²ˆì„ ë°˜ë³µí•œë‹¤.\n",
        "5. í…Œì´ë¸”ì˜ ê°’ì´ ìˆ˜ë ´í•  ë•Œ ê¹Œì§€ 4ë²ˆì„ ë°˜ë³µí•œë‹¤.\n",
        "6. ìˆ˜ë ´í•œ í…Œì´ë¸”ì˜ ê²°ê³¼ë¥¼ ì¶œë ¥í•´ì¤€ë‹¤."
      ],
      "metadata": {
        "id": "YL7o9deafUcH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### grid world; env"
      ],
      "metadata": {
        "id": "XES5fHqeeZHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class GridWorld():\n",
        "    def __init__(self):\n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "\n",
        "    def step(self, a):\n",
        "        # 0ë²ˆ ì•¡ì…˜: ì™¼ìª½, 1ë²ˆ ì•¡ì…˜: ìœ„, 2ë²ˆ ì•¡ì…˜: ì˜¤ë¥¸ìª½, 3ë²ˆ ì•¡ì…˜: ì•„ë˜ìª½\n",
        "        if a == 0:\n",
        "            self.move_left()\n",
        "        elif a == 1:\n",
        "            self.move_up()\n",
        "        elif a == 2:\n",
        "            self.move_right()\n",
        "        elif a == 3:\n",
        "            self.move_down()\n",
        "\n",
        "        reward = -1  # ë³´ìƒì€ í•­ìƒ -1ë¡œ ê³ ì •\n",
        "        done = self.is_done()\n",
        "        return (self.x, self.y), reward, done\n",
        "\n",
        "    def move_left(self):\n",
        "        if self.y == 0:\n",
        "            pass\n",
        "        elif self.y == 3 and self.x in [0, 1, 2]:\n",
        "            pass\n",
        "        elif self.y == 5 and self.x in [2, 3, 4]:\n",
        "            pass\n",
        "        else:\n",
        "            self.y -= 1\n",
        "\n",
        "    def move_right(self):\n",
        "        if self.y == 1 and self.x in [0, 1, 2]:\n",
        "            pass\n",
        "        elif self.y == 3 and self.x in [2, 3, 4]:\n",
        "            pass\n",
        "        elif self.y == 6:\n",
        "            pass\n",
        "        else:\n",
        "            self.y += 1\n",
        "\n",
        "    def move_up(self):\n",
        "        if self.x == 0:\n",
        "            pass\n",
        "        elif self.x == 3 and self.y == 2:\n",
        "            pass\n",
        "        else:\n",
        "            self.x -= 1\n",
        "\n",
        "    def move_down(self):\n",
        "        if self.x == 4:\n",
        "            pass\n",
        "        elif self.x == 1 and self.y == 4:\n",
        "            pass\n",
        "        else:\n",
        "            self.x += 1\n",
        "\n",
        "    def is_done(self):\n",
        "        if self.x == 4 and self.y == 6:  # ëª©í‘œ ì§€ì ì¸ (4,6)ì— ë„ë‹¬í•˜ë©´ ëë‚œë‹¤\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def reset(self):\n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "        return (self.x, self.y)"
      ],
      "metadata": {
        "id": "AHM5sboceM1Q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SARSA"
      ],
      "metadata": {
        "id": "xcf-XAFoekJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QAgent():\n",
        "    def __init__(self):\n",
        "        # 1. Q-tableì„ ì´ˆê¸°í™” í•œë‹¤.\n",
        "        self.q_table = np.zeros((5, 7, 4))\n",
        "        self.eps = 0.9\n",
        "\n",
        "    def select_action(self, s):\n",
        "        # eps-greedyë¡œ ì•¡ì…˜ì„ ì„ íƒí•´ì¤€ë‹¤\n",
        "        x, y = s\n",
        "        coin = random.random()\n",
        "        if coin < self.eps:\n",
        "            action = random.randint(0, 3)\n",
        "        else:\n",
        "            action_val = self.q_table[x, y, :]\n",
        "            action = np.argmax(action_val)\n",
        "        return action\n",
        "\n",
        "    def update_table(self, transition):\n",
        "        s, a, r, s_prime = transition\n",
        "        x, y = s\n",
        "        next_x, next_y = s_prime\n",
        "        a_prime = self.select_action(s_prime)  # S'ì—ì„œ ì„ íƒí•  ì•¡ì…˜ (ì‹¤ì œë¡œ ì·¨í•œ ì•¡ì…˜ì´ ì•„ë‹˜)\n",
        "        # SARSA ì—…ë°ì´íŠ¸ ì‹ì„ ì´ìš©\n",
        "        self.q_table[x, y, a] = self.q_table[x, y, a] + 0.1 * (\n",
        "                    r + self.q_table[next_x, next_y, a_prime] - self.q_table[x, y, a])\n",
        "\n",
        "    def anneal_eps(self):\n",
        "        self.eps -= 0.03\n",
        "        self.eps = max(self.eps, 0.1)\n",
        "\n",
        "    def show_table(self):\n",
        "        q_lst = self.q_table.tolist()\n",
        "        data = np.zeros((5, 7))\n",
        "        for row_idx in range(len(q_lst)):\n",
        "            row = q_lst[row_idx]\n",
        "            for col_idx in range(len(row)):\n",
        "                col = row[col_idx]\n",
        "                action = np.argmax(col)\n",
        "                data[row_idx, col_idx] = action\n",
        "        print(data)\n",
        "\n",
        "def main():\n",
        "    env = GridWorld()\n",
        "    agent = QAgent()\n",
        "\n",
        "    # 5. í…Œì´ë¸”ì˜ ê°’ì´ ìˆ˜ë ´í•  ë•Œ ê¹Œì§€ 4ë²ˆì„ ë°˜ë³µí•œë‹¤.\n",
        "    for n_epi in range(1000):\n",
        "        done = False\n",
        "\n",
        "        s = env.reset()\n",
        "        # 4. ì—í”¼ì†Œë“œë¥¼ ë§ˆì¹  ë•Œ ê¹Œì§€ 2~3ë²ˆì„ ë°˜ë³µí•œë‹¤.\n",
        "        while not done:\n",
        "            # 2. agentê°€ policyì— ë”°ë¼ ê²½í—˜ì„ ìŒ“ëŠ”ë‹¤.\n",
        "            a = agent.select_action(s)\n",
        "            s_prime, r, done = env.step(a)\n",
        "            # 3. SARSA ì •ì±…ì„ í†µí•´ Q-tableì„ ì—…ë°ì´íŠ¸ í•œë‹¤.\n",
        "            agent.update_table((s, a, r, s_prime))\n",
        "            s = s_prime\n",
        "        agent.anneal_eps()\n",
        "    \n",
        "    # 6. ìˆ˜ë ´í•œ í…Œì´ë¸”ì˜ ê²°ê³¼ë¥¼ ì¶œë ¥í•´ì¤€ë‹¤.\n",
        "    agent.show_table()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2c3WSqVeeTm",
        "outputId": "e32d0256-99c5-4e37-c0ad-612537863bb1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3. 3. 0. 3. 2. 2. 3.]\n",
            " [2. 3. 0. 2. 2. 3. 3.]\n",
            " [2. 3. 0. 1. 0. 3. 3.]\n",
            " [2. 2. 2. 1. 0. 3. 3.]\n",
            " [2. 2. 2. 1. 0. 2. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q-learning pseudo-code\n",
        "> 1. í…Œì´ë¸”, Q-tableì„ ì´ˆê¸°í™” í•œë‹¤.\n",
        "2. agentê°€ policyì— ë”°ë¼ ê²½í—˜ì„ ìŒ“ëŠ”ë‹¤.\n",
        "3. Q-learning ì •ì±…ì„ í†µí•´ Q-tableì„ ì—…ë°ì´íŠ¸ í•œë‹¤.\n",
        "4. ì—í”¼ì†Œë“œë¥¼ ë§ˆì¹  ë•Œ ê¹Œì§€ 2~3ë²ˆì„ ë°˜ë³µí•œë‹¤.\n",
        "5. í…Œì´ë¸”ì˜ ê°’ì´ ìˆ˜ë ´í•  ë•Œ ê¹Œì§€ 4ë²ˆì„ ë°˜ë³µí•œë‹¤.\n",
        "6. ìˆ˜ë ´í•œ í…Œì´ë¸”ì˜ ê²°ê³¼ë¥¼ ì¶œë ¥í•´ì¤€ë‹¤."
      ],
      "metadata": {
        "id": "AuTu0FjK1ZCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q-learning"
      ],
      "metadata": {
        "id": "qh5dMxI-oILx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QAgent():\n",
        "    def __init__(self):\n",
        "        # 1. Q-tableì„ ì´ˆê¸°í™” í•œë‹¤.\n",
        "        self.q_table = np.zeros((5, 7, 4))\n",
        "        self.eps = 0.9\n",
        "\n",
        "    def select_action(self, s):\n",
        "        # eps-greedyë¡œ ì•¡ì…˜ì„ ì„ íƒí•´ì¤€ë‹¤\n",
        "        x, y = s\n",
        "        coin = random.random()\n",
        "        if coin < self.eps:\n",
        "            action = random.randint(0, 3)\n",
        "        else:\n",
        "            action_val = self.q_table[x, y, :]\n",
        "            action = np.argmax(action_val)\n",
        "        return action\n",
        "\n",
        "    def update_table(self, transition):\n",
        "        s, a, r, s_prime = transition\n",
        "        x, y = s\n",
        "        next_x, next_y = s_prime\n",
        "        a_prime = self.select_action(s_prime)  # S'ì—ì„œ ì„ íƒí•  ì•¡ì…˜ (ì‹¤ì œë¡œ ì·¨í•œ ì•¡ì…˜ì´ ì•„ë‹˜)\n",
        "        # QëŸ¬ë‹ ì—…ë°ì´íŠ¸ ì‹ì„ ì´ìš©\n",
        "        self.q_table[x, y, a] = self.q_table[x, y, a] + 0.1 * (\n",
        "                    r + np.amax(self.q_table[next_x, next_y, :]) - self.q_table[x, y, a])\n",
        "\n",
        "    def anneal_eps(self):\n",
        "        self.eps -= 0.01  # QëŸ¬ë‹ì—ì„  epsilon ì´ ì¢€ë” ì²œì²œíˆ ì¤„ì–´ ë“¤ë„ë¡ í•¨.\n",
        "        self.eps = max(self.eps, 0.2)\n",
        "\n",
        "    def show_table(self):\n",
        "        q_lst = self.q_table.tolist()\n",
        "        data = np.zeros((5, 7))\n",
        "        for row_idx in range(len(q_lst)):\n",
        "            row = q_lst[row_idx]\n",
        "            for col_idx in range(len(row)):\n",
        "                col = row[col_idx]\n",
        "                action = np.argmax(col)\n",
        "                data[row_idx, col_idx] = action\n",
        "        print(data)\n",
        "\n",
        "\n",
        "def main():\n",
        "    env = GridWorld()\n",
        "    agent = QAgent()\n",
        "\n",
        "    # 5. í…Œì´ë¸”ì˜ ê°’ì´ ìˆ˜ë ´í•  ë•Œ ê¹Œì§€ 4ë²ˆì„ ë°˜ë³µí•œë‹¤.\n",
        "    for n_epi in range(1000):\n",
        "        done = False\n",
        "\n",
        "        s = env.reset()\n",
        "        # 4. ì—í”¼ì†Œë“œë¥¼ ë§ˆì¹  ë•Œ ê¹Œì§€ 2~3ë²ˆì„ ë°˜ë³µí•œë‹¤.\n",
        "        while not done:\n",
        "            # 2. agentê°€ policyì— ë”°ë¼ ê²½í—˜ì„ ìŒ“ëŠ”ë‹¤.\n",
        "            a = agent.select_action(s) \n",
        "            s_prime, r, done = env.step(a)\n",
        "            # 3. Q-learning ì •ì±…ì„ í†µí•´ Q-tableì„ ì—…ë°ì´íŠ¸ í•œë‹¤.\n",
        "            agent.update_table((s, a, r, s_prime))\n",
        "            s = s_prime\n",
        "        agent.anneal_eps()\n",
        "\n",
        "    agent.show_table()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ta1IceeAofsv",
        "outputId": "4e532364-56e4-4683-a4da-43e69ab463af"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3. 3. 0. 3. 3. 2. 3.]\n",
            " [2. 3. 0. 2. 2. 3. 3.]\n",
            " [3. 3. 0. 1. 0. 3. 3.]\n",
            " [2. 2. 2. 1. 0. 2. 3.]\n",
            " [3. 3. 2. 1. 0. 2. 0.]]\n"
          ]
        }
      ]
    }
  ]
}