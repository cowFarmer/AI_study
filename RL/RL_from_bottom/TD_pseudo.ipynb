{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD_pseudo.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "xXkSCgwrZKid",
        "XES5fHqeeZHX"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CHAPTER 5. MDP를 모를 때 value 평가하기\n",
        "TD(Temporal Difference)\n",
        "\n",
        "MC의 단점으로 재귀적인 특징을 갖고 있어, 에피소드가 끝나야 return값을 얻어 업데이트를 할 수 있다.   \n",
        "TD는 상태 가치의 기대값을 예측하고, 예측한 기대값을 토대로 다음 번 상태 가치의 기대값을 예측한다.   \n",
        "이러한 방식은 값을 한 개 씩 얻을 때마다 업데이트 할 수 있다.\n",
        "\n",
        "$MC : V(s_t) ← V(s_t) + \\alpha(G_t-V(s_t))$   \n",
        "$TD : V(s_t) ← V(S_t) + \\alpha(r_{t+1}+\\gamma V(s_{t+1})-V(s_t))$\n",
        "\n",
        "---\n",
        "\n",
        "$V(s_0) ← V(s_0)+0.01*(-1+V(s_1)-V(s_0))$   \n",
        "$V(s_1) ← V(s_1)+0.01*(-1+V(s_2)-V(s_1))$"
      ],
      "metadata": {
        "id": "TY_gHuzgRRjV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pseudo-code\n",
        "> 1. 테이블의 값을 초기화 한다.\n",
        "2. agent가 policy에 따라 경험을 쌓는다.\n",
        "3. 상태 전이가 일어나면 테이블의 값을 업데이트 해준다.\n",
        "4. 에피소드를 마칠 때 까지 2~3번을 반복한다.\n",
        "5. 테이블의 값이 수렴할 때 까지 4번을 반복한다.\n",
        "6. 수렴한 테이블의 결과를 출력해준다."
      ],
      "metadata": {
        "id": "Tdo6q23LVYJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### grid world; env"
      ],
      "metadata": {
        "id": "xXkSCgwrZKid"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x6avG-aLP3ie"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class GridWorld():\n",
        "    def __init__(self):\n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "\n",
        "    def step(self, a):\n",
        "        # 0번 액션: 왼쪽, 1번 액션: 위, 2번 액션: 오른쪽, 3번 액션: 아래쪽\n",
        "        if a == 0:\n",
        "            self.move_left()\n",
        "        elif a == 1:\n",
        "            self.move_up()\n",
        "        elif a == 2:\n",
        "            self.move_right()\n",
        "        elif a == 3:\n",
        "            self.move_down()\n",
        "\n",
        "        reward = -1  # 보상은 항상 -1로 고정\n",
        "        done = self.is_done()\n",
        "        return (self.x, self.y), reward, done\n",
        "\n",
        "    def move_right(self):\n",
        "        self.y += 1\n",
        "        if self.y > 3:\n",
        "            self.y = 3\n",
        "\n",
        "    def move_left(self):\n",
        "        self.y -= 1\n",
        "        if self.y < 0:\n",
        "            self.y = 0\n",
        "\n",
        "    def move_up(self):\n",
        "        self.x -= 1\n",
        "        if self.x < 0:\n",
        "            self.x = 0\n",
        "\n",
        "    def move_down(self):\n",
        "        self.x += 1\n",
        "        if self.x > 3:\n",
        "            self.x = 3\n",
        "\n",
        "    def is_done(self):\n",
        "        if self.x == 3 and self.y == 3:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def get_state(self):\n",
        "        return (self.x, self.y)\n",
        "\n",
        "    def reset(self):\n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "        return (self.x, self.y)\n",
        "\n",
        "\n",
        "class Agent():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def select_action(self):\n",
        "        coin = random.random()\n",
        "        if coin < 0.25:\n",
        "            action = 0\n",
        "        elif coin < 0.5:\n",
        "            action = 1\n",
        "        elif coin < 0.75:\n",
        "            action = 2\n",
        "        else:\n",
        "            action = 3\n",
        "        return action"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TD"
      ],
      "metadata": {
        "id": "7Vu_b28eZOKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # TD\n",
        "    env = GridWorld()\n",
        "    agent = Agent()\n",
        "    # 1. 테이블의 값을 초기화 한다.\n",
        "    data = [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]] \n",
        "    gamma = 1.0\n",
        "    reward = -1\n",
        "    alpha = 0.01\n",
        "\n",
        "    # 5. 테이블의 값이 수렴할 때 까지 4번을 반복한다.\n",
        "    for k in range(50000): \n",
        "        done = False\n",
        "        # 4. 에피소드를 마칠 때 까지 2~3번을 반복한다.\n",
        "        while not done:\n",
        "            x, y = env.get_state()\n",
        "            # 2. agent가 policy에 따라 경험을 쌓는다.\n",
        "            action = agent.select_action()\n",
        "            (x_prime, y_prime), reward, done = env.step(action)\n",
        "            x_prime, y_prime = env.get_state()\n",
        "            # 3. 상태 전이가 일어나면 테이블의 값을 업데이트 해준다.\n",
        "            # 𝑇𝐷:𝑉(𝑠𝑡)←𝑉(𝑆𝑡)+𝛼(𝑟𝑡+1+𝛾𝑉(𝑠𝑡+1)−𝑉(𝑠𝑡))\n",
        "            data[x][y] = data[x][y] + alpha * (reward + gamma * data[x_prime][y_prime] - data[x][y])\n",
        "        env.reset()\n",
        "\n",
        "    # 6. 수렴한 테이블의 결과를 출력해준다.\n",
        "    for row in data:\n",
        "        print(row)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwxE487AZIWT",
        "outputId": "c63174c1-c885-4d92-b88d-55b560566673"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-56.79481916526043, -55.05562443397609, -51.89876910060371, -49.23394981558453]\n",
            "[-55.102270506271076, -52.656172367695795, -47.35562614796741, -42.69929713435356]\n",
            "[-51.477125961669415, -47.85166781122008, -39.7681151540909, -27.812210252522434]\n",
            "[-48.48950874923934, -42.95023283065192, -30.3256593893496, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CHAPTER 6. MDP를 모를 때 optimal policy 찾기\n",
        "MDP를 모를 때 optimal policy를 찾기 위해 TD를 base로 SARSA와 Q-learning을 사용한다.   \n",
        "\n",
        "### on-policy, off-policy\n",
        "behavior policy는 다음 stage로 action을 하는 policy, target policy는 다음 sample을 얻는 policy를 말한다.   \n",
        "- behavior policy = target policy 인 경우 on-policy라고 한다.   \n",
        "- behavior policy != target policy 인 경우 off-policy라고 한다.\n",
        "\n",
        "off-policy를 쓰는 가장 큰 이유는 재평가가 가능하다.\n",
        "\n",
        "### SARSA, Q-learning\n",
        "- SARSA는 on-policy를 사용한다. 이름 그대로 State에서 Action을 선택하면 Reward를 받고 State'에 도착하여 거기서 다시 Action'을 선택한다.   \n",
        "- Q-learning은 off-policy를 사용한다. 이는 behavior policy(행동 정책)과 target policy이 다른 경우를 말한다.   \n",
        "Q_table에서 target policy로 expected 값을 얻고, target policy로 얻은 정보를 토대로 behavior policy로 action한다.\n",
        "\n",
        "TD기반 SARSA 학습 : $q_\\pi(s_t,a_t)=E_\\pi(r_{r+1}+\\gamma q_\\pi(s_{t+1},a_{t+1})]$   \n",
        "TD기반 Q-learning 학습 : $q_*(s,a)=E_{s'}[r+\\gamma \\underset{a'}{max}q_*(s',a')]$\n",
        "\n",
        "\n",
        "### SARSA 업데이트 식\n",
        "        self.q_table[x, y, a] = self.q_table[x, y, a] + 0.1 * (\n",
        "                    r + self.q_table[next_x, next_y, a_prime] - self.q_table[x, y, a])\n",
        "\n",
        "### Q러닝 업데이트 식\n",
        "        self.q_table[x, y, a] = self.q_table[x, y, a] + 0.1 * (\n",
        "                    r + np.amax(self.q_table[next_x, next_y, :]) - self.q_table[x, y, a])\n"
      ],
      "metadata": {
        "id": "1_V5tvmFbYIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SARSA pseudo-code\n",
        "> 1. 테이블, Q-table을 초기화 한다.\n",
        "2. agent가 policy에 따라 경험을 쌓는다.\n",
        "3. SARSA 정책을 통해 Q-table을 업데이트 한다.\n",
        "4. 에피소드를 마칠 때 까지 2~3번을 반복한다.\n",
        "5. 테이블의 값이 수렴할 때 까지 4번을 반복한다.\n",
        "6. 수렴한 테이블의 결과를 출력해준다."
      ],
      "metadata": {
        "id": "YL7o9deafUcH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### grid world; env"
      ],
      "metadata": {
        "id": "XES5fHqeeZHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class GridWorld():\n",
        "    def __init__(self):\n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "\n",
        "    def step(self, a):\n",
        "        # 0번 액션: 왼쪽, 1번 액션: 위, 2번 액션: 오른쪽, 3번 액션: 아래쪽\n",
        "        if a == 0:\n",
        "            self.move_left()\n",
        "        elif a == 1:\n",
        "            self.move_up()\n",
        "        elif a == 2:\n",
        "            self.move_right()\n",
        "        elif a == 3:\n",
        "            self.move_down()\n",
        "\n",
        "        reward = -1  # 보상은 항상 -1로 고정\n",
        "        done = self.is_done()\n",
        "        return (self.x, self.y), reward, done\n",
        "\n",
        "    def move_left(self):\n",
        "        if self.y == 0:\n",
        "            pass\n",
        "        elif self.y == 3 and self.x in [0, 1, 2]:\n",
        "            pass\n",
        "        elif self.y == 5 and self.x in [2, 3, 4]:\n",
        "            pass\n",
        "        else:\n",
        "            self.y -= 1\n",
        "\n",
        "    def move_right(self):\n",
        "        if self.y == 1 and self.x in [0, 1, 2]:\n",
        "            pass\n",
        "        elif self.y == 3 and self.x in [2, 3, 4]:\n",
        "            pass\n",
        "        elif self.y == 6:\n",
        "            pass\n",
        "        else:\n",
        "            self.y += 1\n",
        "\n",
        "    def move_up(self):\n",
        "        if self.x == 0:\n",
        "            pass\n",
        "        elif self.x == 3 and self.y == 2:\n",
        "            pass\n",
        "        else:\n",
        "            self.x -= 1\n",
        "\n",
        "    def move_down(self):\n",
        "        if self.x == 4:\n",
        "            pass\n",
        "        elif self.x == 1 and self.y == 4:\n",
        "            pass\n",
        "        else:\n",
        "            self.x += 1\n",
        "\n",
        "    def is_done(self):\n",
        "        if self.x == 4 and self.y == 6:  # 목표 지점인 (4,6)에 도달하면 끝난다\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def reset(self):\n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "        return (self.x, self.y)"
      ],
      "metadata": {
        "id": "AHM5sboceM1Q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SARSA"
      ],
      "metadata": {
        "id": "xcf-XAFoekJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QAgent():\n",
        "    def __init__(self):\n",
        "        # 1. Q-table을 초기화 한다.\n",
        "        self.q_table = np.zeros((5, 7, 4))\n",
        "        self.eps = 0.9\n",
        "\n",
        "    def select_action(self, s):\n",
        "        # eps-greedy로 액션을 선택해준다\n",
        "        x, y = s\n",
        "        coin = random.random()\n",
        "        if coin < self.eps:\n",
        "            action = random.randint(0, 3)\n",
        "        else:\n",
        "            action_val = self.q_table[x, y, :]\n",
        "            action = np.argmax(action_val)\n",
        "        return action\n",
        "\n",
        "    def update_table(self, transition):\n",
        "        s, a, r, s_prime = transition\n",
        "        x, y = s\n",
        "        next_x, next_y = s_prime\n",
        "        a_prime = self.select_action(s_prime)  # S'에서 선택할 액션 (실제로 취한 액션이 아님)\n",
        "        # SARSA 업데이트 식을 이용\n",
        "        self.q_table[x, y, a] = self.q_table[x, y, a] + 0.1 * (\n",
        "                    r + self.q_table[next_x, next_y, a_prime] - self.q_table[x, y, a])\n",
        "\n",
        "    def anneal_eps(self):\n",
        "        self.eps -= 0.03\n",
        "        self.eps = max(self.eps, 0.1)\n",
        "\n",
        "    def show_table(self):\n",
        "        q_lst = self.q_table.tolist()\n",
        "        data = np.zeros((5, 7))\n",
        "        for row_idx in range(len(q_lst)):\n",
        "            row = q_lst[row_idx]\n",
        "            for col_idx in range(len(row)):\n",
        "                col = row[col_idx]\n",
        "                action = np.argmax(col)\n",
        "                data[row_idx, col_idx] = action\n",
        "        print(data)\n",
        "\n",
        "def main():\n",
        "    env = GridWorld()\n",
        "    agent = QAgent()\n",
        "\n",
        "    # 5. 테이블의 값이 수렴할 때 까지 4번을 반복한다.\n",
        "    for n_epi in range(1000):\n",
        "        done = False\n",
        "\n",
        "        s = env.reset()\n",
        "        # 4. 에피소드를 마칠 때 까지 2~3번을 반복한다.\n",
        "        while not done:\n",
        "            # 2. agent가 policy에 따라 경험을 쌓는다.\n",
        "            a = agent.select_action(s)\n",
        "            s_prime, r, done = env.step(a)\n",
        "            # 3. SARSA 정책을 통해 Q-table을 업데이트 한다.\n",
        "            agent.update_table((s, a, r, s_prime))\n",
        "            s = s_prime\n",
        "        agent.anneal_eps()\n",
        "    \n",
        "    # 6. 수렴한 테이블의 결과를 출력해준다.\n",
        "    agent.show_table()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2c3WSqVeeTm",
        "outputId": "e32d0256-99c5-4e37-c0ad-612537863bb1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3. 3. 0. 3. 2. 2. 3.]\n",
            " [2. 3. 0. 2. 2. 3. 3.]\n",
            " [2. 3. 0. 1. 0. 3. 3.]\n",
            " [2. 2. 2. 1. 0. 3. 3.]\n",
            " [2. 2. 2. 1. 0. 2. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q-learning pseudo-code\n",
        "> 1. 테이블, Q-table을 초기화 한다.\n",
        "2. agent가 policy에 따라 경험을 쌓는다.\n",
        "3. Q-learning 정책을 통해 Q-table을 업데이트 한다.\n",
        "4. 에피소드를 마칠 때 까지 2~3번을 반복한다.\n",
        "5. 테이블의 값이 수렴할 때 까지 4번을 반복한다.\n",
        "6. 수렴한 테이블의 결과를 출력해준다."
      ],
      "metadata": {
        "id": "AuTu0FjK1ZCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q-learning"
      ],
      "metadata": {
        "id": "qh5dMxI-oILx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QAgent():\n",
        "    def __init__(self):\n",
        "        # 1. Q-table을 초기화 한다.\n",
        "        self.q_table = np.zeros((5, 7, 4))\n",
        "        self.eps = 0.9\n",
        "\n",
        "    def select_action(self, s):\n",
        "        # eps-greedy로 액션을 선택해준다\n",
        "        x, y = s\n",
        "        coin = random.random()\n",
        "        if coin < self.eps:\n",
        "            action = random.randint(0, 3)\n",
        "        else:\n",
        "            action_val = self.q_table[x, y, :]\n",
        "            action = np.argmax(action_val)\n",
        "        return action\n",
        "\n",
        "    def update_table(self, transition):\n",
        "        s, a, r, s_prime = transition\n",
        "        x, y = s\n",
        "        next_x, next_y = s_prime\n",
        "        a_prime = self.select_action(s_prime)  # S'에서 선택할 액션 (실제로 취한 액션이 아님)\n",
        "        # Q러닝 업데이트 식을 이용\n",
        "        self.q_table[x, y, a] = self.q_table[x, y, a] + 0.1 * (\n",
        "                    r + np.amax(self.q_table[next_x, next_y, :]) - self.q_table[x, y, a])\n",
        "\n",
        "    def anneal_eps(self):\n",
        "        self.eps -= 0.01  # Q러닝에선 epsilon 이 좀더 천천히 줄어 들도록 함.\n",
        "        self.eps = max(self.eps, 0.2)\n",
        "\n",
        "    def show_table(self):\n",
        "        q_lst = self.q_table.tolist()\n",
        "        data = np.zeros((5, 7))\n",
        "        for row_idx in range(len(q_lst)):\n",
        "            row = q_lst[row_idx]\n",
        "            for col_idx in range(len(row)):\n",
        "                col = row[col_idx]\n",
        "                action = np.argmax(col)\n",
        "                data[row_idx, col_idx] = action\n",
        "        print(data)\n",
        "\n",
        "\n",
        "def main():\n",
        "    env = GridWorld()\n",
        "    agent = QAgent()\n",
        "\n",
        "    # 5. 테이블의 값이 수렴할 때 까지 4번을 반복한다.\n",
        "    for n_epi in range(1000):\n",
        "        done = False\n",
        "\n",
        "        s = env.reset()\n",
        "        # 4. 에피소드를 마칠 때 까지 2~3번을 반복한다.\n",
        "        while not done:\n",
        "            # 2. agent가 policy에 따라 경험을 쌓는다.\n",
        "            a = agent.select_action(s) \n",
        "            s_prime, r, done = env.step(a)\n",
        "            # 3. Q-learning 정책을 통해 Q-table을 업데이트 한다.\n",
        "            agent.update_table((s, a, r, s_prime))\n",
        "            s = s_prime\n",
        "        agent.anneal_eps()\n",
        "\n",
        "    agent.show_table()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ta1IceeAofsv",
        "outputId": "4e532364-56e4-4683-a4da-43e69ab463af"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3. 3. 0. 3. 3. 2. 3.]\n",
            " [2. 3. 0. 2. 2. 3. 3.]\n",
            " [3. 3. 0. 1. 0. 3. 3.]\n",
            " [2. 2. 2. 1. 0. 2. 3.]\n",
            " [3. 3. 2. 1. 0. 2. 0.]]\n"
          ]
        }
      ]
    }
  ]
}