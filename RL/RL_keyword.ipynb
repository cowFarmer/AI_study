{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_keyword.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 강화학습 키워드 정리\n",
        "\n",
        "- [Crystal Clear Reinforcement Learning](https://towardsdatascience.com/crystal-clear-reinforcement-learning-7e6c1541365e)\n",
        "\n",
        "강화학습은 environment와 interacting하며 학습한다. 행동을 가르치는게 아닌, 행동의 결과를 통해 배운다. 과거 경험(exploitation)과 새로운 선택(exploration)을 기반으로 선택한다. 강화학습의 agent는 수치적인 reward를 받는데, 시간이 지날수록 누적된 reward를 maximize하는 방향으로 행동을 학습한다.\n",
        "\n",
        "Reinforcement Learning은 다양한 분야에서 사용되고 있다."
      ],
      "metadata": {
        "id": "3PNbT0XY4Tey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "W4HED1jx2Ad4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 용어 정리\n",
        "먼저 자주 사용되는 용어를 정리해보자."
      ],
      "metadata": {
        "id": "JnEH1rIf13en"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent   \n",
        "agent는 env과 상호작용을 하며 action을 통해 상태를 obseravation하고 reward를 받는다. \n",
        "\n",
        "### Environment   \n",
        "agent와 상호작용하는 모든것을 말한다. agent의 action을 보고 observation을 바꿔주고(= state를 바꿔준다.) 그에 맞는 reward를 agent한테 보내준다.\n",
        "\n",
        "### State\n",
        "agent의 action을 보고 state를 바꿔준다. state에 따라 agent의 view를 달리 해주기 위해 state를 변경하는 observation을 한다.\n",
        "\n",
        "### Reward   \n",
        "agent가 state에서 취할 수 있는 action을 하면 positive 혹은 negative한 reward를 받는다. reward가 positive면 보상, negative면 처벌이라 부를 수 있다.   \n",
        "reward는 agent가 매 time step마다 environment에서 받는 feedback이다. 그리고 reward를 보고 다음 행동을 결정할 수 있다.   \n",
        "일반적으로 positive reward를 최대화 하는 방식으로 문제를 해결한다. 그리고 많은 알고리즘들은 가능한 빠르게 해결하기 위해 약간의 negative를 취하는 방법도 있다. reward는 RL agent의 최적화된 동작을 설계할 때 중요한 지표이다. (예를들어, 텅 빈 거리에서 자율주행 자동차의 속도를 늘려 도착지에 빨리 도착하는 방법과 막힌 도로에서 느리게 동작하는 방법을 선택하기에 중요하다.) reward는 즉각적으로 알려준다.\n",
        "\n",
        "### Discount Factor   \n",
        "미래의 보상보다 현재의 보상이 더 중요하다. discount factor로 표현되는 $\\gamma$는 미래 보상의 현재 가치를 결정하는 데 사용된다.   \n",
        "예를들어 현재 받을 reward가 1, 미래에 받을 reward가 1인 상황에서 미래에 받을 reward에 대한 의존도를 낮추고 현재받을 수 있는 reward에 집중하기 위해 사용한다. 미래의 reward에 1 보다 작은 값인 $\\gamma$를 곱해주어 미래 reward의 현재 가치를 낮춘다.\n",
        "\n",
        "### State Value   \n",
        "상태 값인 V(S)로 해당 state에서 시작하여 행동의 총 보상이 누적된다. 장기적으로 무엇이 좋은지 볼 수 있다.\n",
        "'State Value function = Expected return = Discounted sum of all rewards'\n",
        "\n",
        "### Action   \n",
        "agent가 state에서 할 수 있는 action들이다. 예를들어 미로찾기에서 agent는 상, 하, 좌, 우 4가지로 action할 수 있다.\n",
        "\n",
        "### Action Value   \n",
        "action value 혹은 state-action value(Q function)는 policy $\\pi$가 있는 상태에서 agent가 특정 행동을 수행하는게 얼마나 좋은지 볼 수 있다.   \n",
        "Q function은 $Q(S, A)$로 표현된다. Q는 행동의 quality를 의미한다.\n",
        "\n",
        "### Policy   \n",
        "state에서 agent의 action을 결정하는 정책이다. action이 deterministic이거나 stochasic인 상황에서 action mapping을 policy할 수 있다. $\\pi$ 혹은 policy, P라고 표현한다."
      ],
      "metadata": {
        "id": "doaObwwI2ap3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "3dbOScRA19Vv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 필수 개념\n",
        "Reinforcement Learning 알고리즘을 알아보기 전에 몇 가지 필수 개념들을 다루는데, 그것들을 짚고 가자."
      ],
      "metadata": {
        "id": "MO7pWZNU1487"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploitation and Exploration   \n",
        "Exploit은 Greedy action이라고도 불리며, state에서 best value를 주는 action을 그대로 수행하는 것이다.\n",
        "Explore는 현재 좋은 reward를 주는 방향을 알아도, 더 좋은 reward를 찾기 위해 새로운 시도를 하는 것이다.   \n",
        "예를 들어 바로 앞에 5, 조금 뒤에 10의 reward를 주는 환경이 있다고 가정해보자. exploit은 바로 앞에 5를 받기 위해 agent의 action을 결정할 것이고, explore를 통해 10의 reward를 주는 state를 발견하면 앞으로 10을 받게끔 agent의 action을 결정할 것이다. 그래서 장기적으로 더 나은 수익을 창출하기 위해 explore가 필요하다.\n",
        "\n",
        "### On-policy, Off-policy   \n",
        "off-policy의 개념을 만들기 위해 policy를 behavior policy, target policy로 나눴다. behavior policy는 action으로 다음 state를 가는 policy를 말한다. target policy는 다음target(sample)을 얻기 위해 하는 policy이다.   \n",
        "behavior policy = target policy 인 경우 on-policy라고 한다.   \n",
        "behavior policy != target policy 인 경우 off-policy라고 한다.   \n",
        "TD의 SARSA가 on-policy, TD의 Q-learning이 off-policy이다.   \n",
        "Off-policy를 사용하는 가장 큰 이유는 재평가가 가능하다.\n",
        "\n",
        "### Planning, Learning   \n",
        "RL 알고리즘은 planning, learnnig으로 분류할 수 있다. planning 단계에서는 agent가 model의 environment를 알고 있어, model의 optimal policy를 따른다.   \n",
        "learning 단계에서는 environment에 대해 모르는 상태로, trial and error 과정을 통해 optimal policy 혹은 optimal state value를 학습한다.\n",
        "\n",
        "### Model-based, Model-free   \n",
        "model-based에서 agent가 model의 environment를 알고 있는 상태를 말한다. 이는 상태 변화의 확률을 알고 있어, optimal action을 계획하는게 주요 목적이다.   \n",
        "model-free는 상태 변화에 따른 전이 확률을 알고 있지 않아, 최적의 결과를 주는 optimal을 얻기 위해 environment와 상호 작용하여 학습해야 한다."
      ],
      "metadata": {
        "id": "cRMYz7CX2Ykx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "5JEpOMF12RAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning 'Planning' Algorithm"
      ],
      "metadata": {
        "id": "tW2gbnK4gSm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Markov Decision Processes(MDP)   \n",
        "시간에 따라 환경이 변하지 않음을 전제로 두고 있다.   \n",
        "MDP의 최종 목표는 모든 상태에 대해 어떤 조치를 취해야 하는지 알려줄 수 있는 optimal policy를 찾는 것이다. \n",
        "RL는 임의의 policy에 대해 V(S)를 계산하는 policy evaluation(정책 평가 또는 문제 prediction) 방법을 사용한다.\n",
        "\n",
        "### Generalized Policy Iteration   \n",
        "optimal policy, optimal value function을 찾는게 목표이다. policy의 evaluated를 greedy action을 통해 수렴이 일어날 때 까지 반복하여 optimal policy 혹은 optimal value를 찾는다.   \n",
        "-> Q-function과 동일한거 같다.\n",
        "\n",
        "### Bellman Equation   \n",
        "특정 state의 value를 구하는 방법은 현재 state $S$에서 미래 state $S'$의 reward에 discount를 더하면 된다.\n",
        "\n",
        "### Value Iteration   \n",
        "벨만 방정식을 풀기 위해 일반적으로 모든 state에 임의의 value를 주고 neighbors를 기준으로 update한다. 그리고 수렴할 때 까지 반복한다. 이를 value iteration이라고 한다.   \n",
        "value iteration은 optimal value function을 찾거나 하나의 policy를 추출하는 것을 포함하고 있다. \n",
        "\n",
        "### Policy Iteration   \n",
        "optimal policy를 찾기 위해 policy iteration을 한다.  MDP는 행동의 가짓수가 한계가 있기 때문에 optimal policy, optimal value가 유한한 반복으로 수렴된다.   \n",
        "policy iteration는 policy evaluation과 policy improvement가 포함되어 수렴될 때 까지 서로 반복한다.\n",
        "\n",
        "### RL 'Learning' algorithm\n",
        "RL 문제를 해결하기 위해 모든 상태에 대한 optimal policy, optimal values를 찾는게 목표이다.\n",
        "알고리즘의 특징을 기준으로 크게 세 가지 범주로 그룹화된다.\n",
        "\n",
        "![](https://miro.medium.com/max/700/1*uW79NHhW4vC7NELLYLVhzQ.png)"
      ],
      "metadata": {
        "id": "gh0GIooh2VWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Value Based Algorithm\n",
        "![](https://miro.medium.com/max/700/1*6Sgfw0jYqvnyxncVnHl69w.png)"
      ],
      "metadata": {
        "id": "dA74cYz_hQ8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Monte Carlo Learning\n",
        "Monte Carlo는 agent가 state에 대해 배우고 environment와 상호작용 할 때 reward를 준다.   \n",
        "agent가 experienced samples를 generate하고 평균 value를 기반으로 state, state-action에 대한 value를 계산한다. MC의 중요한 특징을 알아보자.\n",
        "- model이 없어 agent가 MDP 상태를 모른다.\n",
        "- sampling된 experience로 부터 학습한다.\n",
        "- $v\\pi(s)$는 policy에 따라 experience의 average를 return한다. \n",
        "- episode가 끝나고 업데이트 된다.\n",
        "외에 episode 문제에만 적용할 수 있다, bootstrapping이 없다.\n",
        "\n",
        "expected return(기대 수익률) 대신 policy에 따라 sampled를 기반으로 empirical return(경험적 수익)을 얻는다.   \n",
        "MC는 방문 횟수의 영향을 받는 first visit MC, every visit MC 두 가지가 있다.\n",
        "\n",
        "### TD(0)\n",
        "TD 학습의 가장 간단한 형태이다. 매 step마다 다음 state로 update되면서 value function이 업데이트 된다. state가 바뀌는 observe reward를 기반으로 학습하고, 충분한 샘플링 과정을 거친 후에 수렴된다. TD의 특징을 알아보자.\n",
        "- model이 없어 agent가 MDP 상태를 모른다.\n",
        "- sampling된 experience로 부터 학습한다.\n",
        "- TD는 결과를 기다리지 않고 다른 학습된 추정치를 기반으로 추정치를 업데이트 한다.\n",
        "- 불완전한 episode에서 배울 수 있다. 따라서 연속된 문제에서 사용할 수 있다.\n",
        "- 추측에 대한 추측을 업데이트 하고 real experience를 기반으로 수정한다.\n",
        "\n",
        "![](https://miro.medium.com/max/700/1*7rxsa0-hk2e9w7S7rFgC-A.png)\n",
        "￼\n",
        "learning rate $a$는 현재 state를 업데이트 하기 위해 new state와 현재 state 차이의 오차를 말한다. 그리고 이를 TD error라고 말한다.\n",
        "\n",
        "### SARSA\n",
        "control or improvement를 위한 TD 알고리즘 중 하나이다. 이름에서 볼 수 있듯이 SARSA는 $S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}$이다. 한 쌍의 S, A 값에서 다른 S, A로 이동하는 과정에서 reward를 수집한다.   \n",
        "SARSA는 on-policy이다. action value function인 $Q$를 사용하고, policy $\\pi$를 따른다. GPI(Generalized Policy Iteration)는 policy(e-greedy)에 따라 action한다.\n",
        "\n",
        "![](https://miro.medium.com/max/700/1*10RCbzCXFuF2wu5J-Dj-vw.png)\n",
        "\n",
        "### Q-Learning\n",
        "off-policy 알고리즘이다. DQN(Deep Q-Learning)은 Q-learning 기반 알고리즘이다. Q-learning에서 target policy는 greedy policy이고 action policy는 e-greedy policy이다.\n",
        "\n",
        "![](https://miro.medium.com/max/700/1*nCK6v1OA0x12d0VC7a-9Mw.png)\n",
        "\n",
        "### Expected SARSA\n",
        "각 action의 현재 policy에 따른 가능성을 고려하여 expected value를 사용한다는 점을 제외하고 Q-learning과 같다.   \n",
        "expected 값에 따르므로 expected SARSA라고 말한다.\n",
        "\n",
        "![](https://miro.medium.com/max/700/1*TOjSjw6KONQGYG5fkoUbqA.png)"
      ],
      "metadata": {
        "id": "XMrUqx-J2nV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TD 개념을 기반으로 갖고 있지만 값을 즉시 update하지 않고 시간이 지난 후에 update하는 알고리즘들을 알아보자.\n",
        "\n",
        "### n-step TD/ SARSA\n",
        "특정 time step 뒤에 update되는 action value 값을 말한다. 이러한 경우 agent는 value를 update 하기 전에 더 많은 rewards를 수집한다.\n",
        "\n",
        "![](https://miro.medium.com/max/700/1*D6tHGkJC6m8ysQPHBYgOlg.png)\n",
        "\n",
        "### TD(λ)\n",
        "n-step TD의 확장 단계이다. 가능한 n-step의 return을 평균을 내어 single return을 내는 것이다. 시간이 지나면서 기하급수적으로 감소하는 weight를 사용한다. 이를 $\\lambda$라고 표현하고 0과 1사이의 값을 갖고 있다.   \n",
        "모든 weight의 합을 1으로 만들기 위해 n번째 수익을 $(n-1)^{\\lambda}$로 계산하여 normalize한다.\n",
        "\n",
        "$\\lambda=1$일 때 본질적으로 state, action이 끝까지 가는 방식이 MC와 유사하다. $\\lambda=0$일 때 $TD(0)$로 감소시킨다. $0<\\lambda<1$ 인 경우 MC와 TD를 혼합한 방법이 된다.\n",
        "\n",
        "eligibility traces(자격 추적)를 사용하여 더욱 개선할 수 있다. 과거에 발생한 일을 기억하고 현재 정보를 사용하여 지금까지 본 모든 state-value를 update 하는 아이디어다.\n",
        "eligibility traces는 how frequent(자주), how recent(최근)인지 두 가지를 결합한다. RL는 learning을 빠르게 하기 위해 next state에 learning한걸 useful하게 extend 하는게 필요하다. 이를 하기 위해 마지막 steps의 states를 단기적으로 저장하는 메커니즘이 필요하다.   \n",
        "$\\lambda ∈ [0,1]$은 trace-decay라고 하는 accumulating trace이다. 이는 드문 state에 samll weight를 주고 최근에 방문한 state에 큰 weight 값을 준다.   \n",
        "$\\lambda=0$의 경우 $TD(0)$로 바로 앞의 예측만 update 된다. MC 방식에서는 state를 update 하기 위해 에피소드가 끝날 때 까지 기다려야한다. $TD(1)$은 에피소드가 끝나지 않아도, 이전 state를 online으로 update 할 수 있다.\n",
        "\n",
        "![](https://miro.medium.com/max/700/1*5txo8ElbI2jL4q7-By8oIg.png)\n",
        "\n",
        "### Deep Q Network(DQN)\n",
        "딥러닝과 강화학습을 통합했다. deep neural network는 Q value에 근사치를 준다. DQN은 neural network를 통해 Q function값의 근사치를 주고 방대한 양의 계산을 절약하여 연속 시간 action space로 확장할 수 있다.\n",
        "- Experience replay   \n",
        "agent를 실행하고 마지막 100,000개의 정보를 buffer에 저장하고 버퍼에서 mini batch를 통해 deep network를 train한다.\n",
        "- Target network   \n",
        "두 개의 deep network인 $\\theta$, $\\theta-$를 생성한다. 첫 번째 값은 Q value(target)를 retrieve 하는데 사용하고, 두 번째 값은 training의 update에 사용한다. 50,000번의 update 후에 $\\theta-$ = $\\theta$ 하여 동기화 한다.\n",
        "\n",
        "![](https://miro.medium.com/max/700/1*sBQZOI2-R8ZjrtS6r4ZZ6A.png)\n",
        "￼\n",
        "1. state (S)를 preprocess를 거치고 state의 Q-value를 DQN에 공급한다.\n",
        "2. e-greedy policy(behaviour policy, exploration)를 통해 action을 선택한다. 1-eplison 확률로 $A = argmax(Q(S, A, \\theta))$ 작업을 하여 action을 고른다.\n",
        "3. S에서 A을 수행하고 S'로 이동하여 R를 받는다. 이 과정을 $S, A, R, S'$로 buffer에 저장한다. 게임으로 예를 들면 S'는 전처리 과정을 거친 다음 게임 화면이다.\n",
        "4. buffer에서 batch만큼 sampling하고 loss를 계산한다. 이는 Q(reward + $\\gamma$ maxQ'(greedy target policy)와 predicted Q의 제곱 차이이다.\n",
        "5. 위 계산의 loss를 최소화 하기 위해 gradient descent한다.\n",
        "6. N 번째 반복마다 $\\theta-$ = $\\theta$ 한다.\n",
        "7. M 개의 에피소드에 대해 반복하여 $\\theta$를 수렴한다."
      ],
      "metadata": {
        "id": "FyOzAG88hUJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Double Q-Learning\n",
        "Q-learning은 일부 stochasic environmnet에서 저조한 퍼포먼스를 보인다. 왜냐하면 $MaxQ(S', A')$으로 action에 대한 과대평가 떄문이다. 그래서 과대평가를 줄이기 위해 action selection, action evaluation을 decomposing(분해)한다.   \n",
        "Double Q-Learning은 두 개의 Q-value function과 Q-A, Q-B(Target network와 Deep Q Network의 근사치)를 갖고, 각각 next stage에 서로 update를 받는다.   \n",
        "Q-A는 다음 stage의 best action value을 얻고, Q-B는 Q-A가 선택한 action을 사용하여 state action의 expected value를 결정한다.\n",
        "\n",
        "![](https://miro.medium.com/max/700/1*YkcVQRPZU9Xzq8FLqwAcbQ.png)\n",
        "\n",
        "### Dueling DQN\n",
        "Q value는 state value function인 V, advantage value A 두 가지로 분해할 수 있다. Advantage function은 평균 action과 비교하여 더 나은 action이 무엇인지 capture한다. 동시에 value function은 현재 state에서 얼마나 좋은지 capture한다.   \n",
        "Dueling Q Networks는 Q function을 value와 advantage function의 합에 의존한다.   \n",
        "각 부분을 학습하기 위해 두 개의 네트워크가 있고, 그들의 output을 집계(aggregate)한다.   \n",
        "\n",
        "![](https://miro.medium.com/max/700/1*qP_5Ci2bJuv-ZIvXUp6TdA.png)\n",
        "￼\n",
        "이러한 유형의 네트워크에 추가하면, 네트워크가 서로의 작업을 작 구별할 수 있고, learning이 크게 향상된다. 많은 state에서 다른 action의 value는 매우 유사하며 action to take는 덜 중요하다. 이 작업은 많은 action을 선택하는게 중요한 환경에서 중요하다.\n",
        "\n",
        "\n",
        "### Policy Based Algorithm\n",
        "policy based에서 state와 action을 알려주었을 때 expected되는 reward의 합을 배우는 대신 state를 action으로 mappling하는 policy를 직접 학습한다. \n",
        "이 아이디어의 핵심은 높은 return의 확률을 높이고 낮은 return 확률을 낮추는 것이다.\n",
        "\n",
        "policy gradient 방법은 target model에 맞눈 policy를 직접 optimizing 하는 것이다. policy $P(A|S)$는 $\\theta$로 모델에 파라미터화 된다. reward function의 합은 policy에 따라 달라지며, best reward를 위해 다양한 알고리즘을 적용하여 $\\theta$를 optimize한다. policy based에서는 정책 표현($\\pi:S -> A)$을 구축하고 학습하는 동안 메모리에 보관한다.\n",
        "\n",
        "policy-based 방법이 continuous space에서 useful하다. 왜냐하면 action, state에 무한한 경우의 수가 있는 경우 value-based는 계산 비용이 너무 많이 든다. \n",
        "\n",
        "### REINFORCE\n",
        "REINFORCE(Monte-Carlo policy gradient)는 policy parameter $\\theta$를 update하기 위해 Monte-Calro 방법의 estimate return에  의존한다.   \n",
        "REINFORCE는 actual gradient와 sample gradient가 같게 작동한다. agent는 현재 policy를 사용하여 한 에피소드의 궤적을 수집하고 이를 사용하여 policy의 parameter를 update한다.\n",
        "1. 무작위로 policy의 parameter $\\theta$를 초기화한다.\n",
        "2. policy에 대한 궤적을 하나 생성한다.\n",
        "3. 각 time step에 따른 estimate return을 추정한다.\n",
        "4. policy parameter $\\theta$를 update한다.(value function의 gradient는 policy gradient의 log를 곱한 값으로 expect 가능하다.)\n",
        "5. 2~4번을 수렴할 때 까지 반복한다.\n",
        "\n",
        "![](https://miro.medium.com/max/700/1*WbcY2LGj0fVsHCoauTYYfw.png)\n",
        "\n",
        "REINFORCE는 on-policy 방법으로 stochastic policy를 학습한다. 이는 최신 stochastic policy에 맞춰 sampling하여 explore함을 의미한다. action selection의 랜덤성 amount는 초기 조건과 훈련 절차에 따라 달라진다.   \n",
        "업데이트 규칙은 시간이 지날수록 찾은 reward를 활용하는 exploit하기 때문에 policy가 local optima에 갇힐 수 있다.\n",
        "\n",
        "중요한 점은 Markov 속성을 사용하지 않으며, 알고리즘 변경 없이 MDP에서 사용할 수도 있다.\n",
        "\n",
        "큰 단점은 높은 variance와 느린 수렴이다.\n",
        "\n",
        "#### REINFORCE with Baseline\n",
        "variance를 줄이기 위해, baseline B(S)가 action A에 의존하지 않는 return G(S)에서 baseline B(S)를 빼는 것이다. 이는 수학적으로 policy gradient가 unbiased한 결과를 기대해볼 수 있다.\n",
        "\n",
        "문제는 B(S)를 선택하는 방법이다. basline의 선택 사항중 하나는 state value V(S, w)의 추정치를 계산하는 것이다."
      ],
      "metadata": {
        "id": "w0MCgC3CiRfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actor-Critic Algorithm\n",
        "policy gradient의 두 가지 중요한 구성 요소는 policy model, value function이다. policy를 추가하여 value function을 배우는게 매우 의미가 있다. value function을 알면 policy update에 도움이 될 수 있다. (예를들어 vanila policy gradients의 gradient variance 감소) 이것이 Actor-Critic이 하는 일이다. 여기서 Actor는 policy고 Critic은 value function이다.\n",
        "\n",
        "actor-critic은 파라미터를 공유하거나 공유하지 않는 두 가지 model로 나뉠 수 있다.\n",
        "- critic은 value function의 파라미터 w를 업데이트하고 알고리즘에 따라 action-value Q, state-value V가 될 수 있다.\n",
        "- actor는 critic이 제안한 방법으로 policy의 파라미터 $\\theta$를 업데이트 한다.\n",
        "\n",
        "위에서 말한 policy gradient의 방법으로 variance를 줄이고, 안정성을 늘리는 한 가지 방법은 baseline에서 누적 reward를 빼는 것이다. 이 baseline을 빼는 것은 기대치에서 unbiased 하다. 그래서 baseline을 통해 return을 조정하여 variance를 줄인다. REINFORCE 알고리즘을 개선하는 방법은 많이 있다.\n",
        "\n",
        "\n",
        "-> 이후에 정리할 내용\n",
        "A3C\n",
        "A2C\n",
        "TRPO\n",
        "PPO\n",
        "DDPG\n",
        "SAC(Soft Actor-Critic)\n",
        "IMPALA"
      ],
      "metadata": {
        "id": "hA17qUv3i7D6"
      }
    }
  ]
}